<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Ze&#39;s Blog</title>
    <link>https://zeqiang-lai.github.io/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Ze&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 25 Apr 2022 14:32:05 +0000</lastBuildDate><atom:link href="https://zeqiang-lai.github.io/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Steepest Descent &amp;&amp; Conjugate Gradient</title>
      <link>https://zeqiang-lai.github.io/blog/posts/ai/sd_cg/</link>
      <pubDate>Mon, 25 Apr 2022 14:32:05 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/posts/ai/sd_cg/</guid>
      <description>&lt;p&gt;这是一篇主要介绍Conjugate Gradient (CG)的笔记，当然为了引入CG，也会一并介绍其“前身” Steepest Descent。&lt;/p&gt;
&lt;p&gt;本文主要参考这篇论文：

  &lt;a href=&#34;https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf&#34;&gt;An Introduction to the Conjugate Gradient Method Without the Agonizing Pain&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is definitely the best paper to understand the steepest descent conjugate gradient method, which save me a lot of time, thanks god. And it is written in 1994, pretty cool, right? 🧐&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;类似的知乎： 
  &lt;a href=&#34;https://zhuanlan.zhihu.com/p/64227658&#34;&gt;https://zhuanlan.zhihu.com/p/64227658&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>蒙特卡洛法</title>
      <link>https://zeqiang-lai.github.io/blog/posts/ai/monte_carlo/</link>
      <pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/posts/ai/monte_carlo/</guid>
      <description>&lt;p&gt;蒙特卡洛(MonteCarlo)是一大类随机算法(RandomizedAlgorithms)的总称，它们通过随机样本来估算真实值。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Convolution with Image Filter &amp;&amp; Convolution with fft/ifft</title>
      <link>https://zeqiang-lai.github.io/blog/posts/ai/conv-fft/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/posts/ai/conv-fft/</guid>
      <description>我们可以使用傅立叶变换实现卷积，具体做法大概就是先对数据和卷积核进行傅立叶变换将数据变换到频域，然后卷积就是频域上的乘积, 最后做逆傅立叶变换转化回原来的空域。
r = ifft(fft(x).*fft(h)) 但是需要注意的是傅立叶变换做的卷积对应的是Circular Convolution。
g = conv2(f,h,&amp;#39;same&amp;#39;); g_fft2 = ifft2(fft2(circshift(f,[-1,-1])).*fft2(rot90(h,2),2,4)); 使用傅立叶变换如果要获得和Circular Convolution一模一样的结果，需要对数据做一个shift，shift的大小和卷积核大小有关，二维情况就是[-w/2, -h/2]；然后卷积核也要反转一下，因为conv2卷积是倒着的。</description>
    </item>
    
    <item>
      <title>More about Variational Autoencoder</title>
      <link>https://zeqiang-lai.github.io/blog/posts/ai/vae-more/</link>
      <pubDate>Fri, 04 Dec 2020 21:32:04 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/posts/ai/vae-more/</guid>
      <description>&lt;p&gt;一些关于VAE的扩展知识。&lt;/p&gt;
&lt;p&gt;不断更新中&amp;hellip;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How genralized linear model work?</title>
      <link>https://zeqiang-lai.github.io/blog/posts/ai/Generalized-Linear-Model/</link>
      <pubDate>Sat, 12 May 2018 08:24:04 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/posts/ai/Generalized-Linear-Model/</guid>
      <description>&lt;p&gt;本文将简单的讲述：GLM是如何工作的？&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Principal component analysis</title>
      <link>https://zeqiang-lai.github.io/blog/posts/ai/Principal-component-analysis/</link>
      <pubDate>Mon, 08 Jan 2018 12:45:04 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/posts/ai/Principal-component-analysis/</guid>
      <description>&lt;p&gt;PCA (Principal component analysis) 是一种给数据降维的方法。&lt;/p&gt;
&lt;p&gt;利用PCA，能将一堆高维空间的数据映射到一个低维空间，并最大限度保持它们之间的可区分性。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
