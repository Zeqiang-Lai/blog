[{"id":0,"href":"/blog/docs/notes/concept/","title":"Concept","section":"Notes","content":"Concepts #  Deep Generative Models #   VAE #   å†çœ‹å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨VAE: [ Note], [ Slide]  Flow-based #   Invertible Neural Network Normalizing Flow (Flow Neural Network) [Note] Denoising Normalizing Flow  Illustration   EM #    æ¨å¯¼é«˜æ–¯æ··åˆæ¨¡å‹çš„EMç®—æ³•ï¼š[ Note] ä»æœ€å°åŒ–è‡ªç”±èƒ½çš„è§’åº¦æ¨å¯¼é«˜æ–¯æ··åˆæ¨¡å‹çš„EMç®—æ³•ï¼š[ Note]  ADMM #    ADMM Tutorial: [ Note] ADMMå„ä¸ªä»»åŠ¡æ¨å¯¼: [ Note]  Others #    Denoising Autoencoder  æœ´ç´ è´å¶æ–¯ #  æœ´ç´ è´å¶æ–¯ ç”¨äºæ±‚è§£åˆ†ç±»é—®é¢˜ï¼Œç»™å®šæ•°æ®ï¼Œæ¯ä¸ªæ•°æ®ç‚¹åŒ…å«å¤šåˆ—ç‰¹å¾å’Œä¸€ä¸ªç±»åˆ«ã€‚ ä»»åŠ¡æ˜¯ä¼°è®¡ç»™å®šç‰¹å¾ï¼Œå„ä¸ªç±»åˆ«å‡ºç°çš„æ¦‚ç‡ï¼Œå³æ±‚ï¼š $$ p\\left(C \\mid F_{1}, \\ldots, F_{n}\\right) $$\næˆ‘ä»¬å¯ä»¥ç”¨é¢‘ç‡ä¼°è®¡æ¦‚ç‡çš„æ–¹æ³•ç›´æ¥ä»æ•°æ®ä¸­ä¼°è®¡è¿™ä¸ªæ¦‚ç‡ï¼Œä½†æ˜¯å½“ç‰¹å¾Fçš„ç§ç±»å¾ˆå¤šï¼Œå–å€¼ä¹Ÿå¾ˆå¤šçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å¾ˆå¤§é‡çš„æ•°æ®æ‰èƒ½åšå‡ºå‡†ç¡®ä¼°è®¡ã€‚è¿™åœ¨ç°å®ç”Ÿæ´»ä¸­å¾€å¾€æ˜¯åšä¸åˆ°çš„ã€‚\nå› æ­¤ï¼Œæœ´ç´ è´å¶æ–¯é¦–å…ˆç”¨è´å¶æ–¯å…¬å¼æ”¹å†™æ±‚è§£ç›®æ ‡ï¼š $$ p\\left(C \\mid F_{1}, \\ldots, F_{n}\\right)=\\frac{p(C) p\\left(F_{1}, \\ldots, F_{n} \\mid C\\right)}{p\\left(F_{1}, \\ldots, F_{n}\\right)} $$\nåˆ†æ¯æ˜¯æ¯ä¸ªç‰¹å¾å‡ºç°çš„è”åˆæ¦‚ç‡ï¼Œä¸ç±»åˆ«æ— å…³ï¼Œå¯ä»¥çœ‹ä½œæŸä¸ªæœªçŸ¥çš„å®šå€¼ã€‚å¯¹äºåˆ†å­ï¼Œ$p(C)$æ˜¯æ¯”è¾ƒå®¹æ˜“é€šè¿‡é¢‘ç‡ä¼°è®¡çš„ï¼Œè€Œ$p\\left(F_{1}, \\ldots, F_{n} \\mid C\\right)$åˆ™ä¸å¥½ä¼°è®¡ï¼Œå› ä¸ºå½“ç‰¹å¾Få¾ˆå¤šçš„æ—¶å€™ï¼Œæ•°æ®ä¸€èˆ¬æ¯”è¾ƒç¨€ç–ï¼Œç»™å®š$C$ï¼Œ$F_{1}, \\ldots, F_{n}$å‡ºç°çš„æ¦‚ç‡å¯èƒ½ä¸º0ï¼Œå› ä¸ºæ•°æ®é›†ä¸­ä¸å­˜åœ¨è¿™ç§æƒ…å†µï¼ˆä½†ä¸ä»£è¡¨çœŸçš„å‡ºç°æ¦‚ç‡ä¸º0ï¼‰ã€‚\nå› æ­¤ï¼Œæœ´ç´ è´å¶æ–¯åšäº†ä¸ªæœ´ç´ çš„å‡è®¾ï¼Œå³å„ä¸ªç‰¹å¾ç›¸äº’ç‹¬ç«‹ã€‚åŸºäºæ­¤ï¼Œåˆ™æœ‰ï¼š\n $$ \\begin{equation} \\begin{aligned} p\\left(F_{1}, \\ldots, F_{n} \\mid C\\right) \u0026 \\propto p\\left(F_{1} \\mid C\\right) p\\left(F_{2} \\mid C\\right) p\\left(F_{3} \\mid C\\right) \\ldots \\\\ \u0026 \\propto \\prod_{i=1}^{n} p\\left(F_{i} \\mid C\\right) \\end{aligned} \\end{equation} $$  æ˜¾ç„¶ï¼ŒåŸºäºé¢‘ç‡ç»Ÿè®¡æ¦‚ç‡$p\\left(F_{i} \\mid C\\right)$ä¼šæ›´åŠ å®¹æ˜“å¾—å¤šã€‚\n  "},{"id":1,"href":"/blog/docs/notes/paper/","title":"Paper","section":"Notes","content":"Paper Notes #  ä¸€äº›è®ºæ–‡é˜…è¯»ç¬”è®°\n  [PDF] Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems  [PDF] Effective Snapshot Compressive-spectral Imaging via Deep Denoising and Total Varia-tion Priors  [Draft] RAFT: Recurrent All-Pairs Field Transforms for Optical Flow  [Draft] PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing  Simple Review #   Vision Transformer for Small-Size Datasets #   æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å°æ•°æ®é›†çš„Transformerã€‚ ä¸»è¦æ¨¡å—ï¼šShifted Patch Tokenization, Locality Self-Attentionã€‚ç»†èŠ‚æ²¡çœ‹ã€‚  Functional Neural Networks for Parametric Image Restoration Problems #   å¯¹äºè¶…åˆ†ï¼Œå»å™ªï¼ŒJPEG Decompressionï¼Œæœ‰ä¸åŒçš„parameterï¼ˆscale factorï¼Œnoise levelç­‰ï¼‰ï¼Œç°æœ‰æ–¹æ³•é’ˆå¯¹ä¸åŒparameterå•ç‹¬è®­ç»ƒæ¨¡å‹ï¼Œæˆ–ä¸è€ƒè™‘ä¸åŒparameterçš„å·®å¼‚ï¼Œä¸€èµ·è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ã€‚ ä½œè€…æå‡ºå­¦ä¹ ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥paramterï¼Œè¾“å‡ºé’ˆå¯¹è¿™ä¸ªparameterçš„æ¨¡å‹å‚æ•°ã€‚ä¸»è¦ç”¨ç‚¹æ˜¯ä¸ç”¨å•ç‹¬è®­ç»ƒæ¯ä¸ªparameterçš„æ¨¡å‹ï¼Œè€Œæ˜¯è®­ç»ƒä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥parameterï¼Œè¾“å‡ºæ¨¡å‹å‚æ•°ã€‚  Variational Image Restoration Network #   ç”¨äº†Generative Modelå’ŒVariational inferenceåšImage Restorationã€‚å¤ªé•¿å…¬å¼å¤ªå¤šï¼Œæ²¡ç»†çœ‹ã€‚  Designing a Practical Degradation Model for Deep Blind Image Super-Resolution #   Github\n æå‡ºäº†ä¸€ä¸ªè¶…åˆ†çš„Degradation Modelç”¨äºçœŸå®å›¾åƒè¶…åˆ†ã€‚ å…¶å®å°±æ˜¯æ€»ç»“äº†å¸¸è§çš„Blurï¼ŒDownsampleï¼ŒNoiseçš„ç§ç±»ï¼Œç„¶ådegradationå°±æ˜¯è¿™äº›çš„æ’åˆ—ç»„åˆã€‚  Revisiting RCAN: Improved Training for Image Super-Resolution #   æå‡ºäº†ä¸€äº›è®­ç»ƒç­–ç•¥ï¼Œè®©RCANçš„æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚ ç­–ç•¥ç®€å•è¯´ï¼šå¤šGPUå¤§Batch Sizeï¼ŒSiLuæ›¿æ¢ReLuï¼Œè®­ç»ƒæ›´é•¿æ—¶é—´ï¼ŒLamb Optimizerï¼Œ48*48 patchè®­ç»ƒï¼Œ64*64 finetuneï¼ŒFP16åŠ é€Ÿè®­ç»ƒï¼Œä¸è¦regularizationï¼Œå…ˆè®­ç»ƒx2ï¼Œç„¶åç”¨x2å‚æ•°è®­ç»ƒx3ï¼Œä»¥æ­¤ç±»æ¨ã€‚  Enhancing Low-Light Images in Real World via Cross-Image Disentanglement #  arxiv 2022 | CIDN\n ä¸éœ€è¦é…å¯¹æ•°æ®è®­ç»ƒçš„æš—å…‰å¢å¼ºç½‘ç»œ æ ¸å¿ƒæ€æƒ³ï¼Œå°†ä¸€å¼ å›¾åˆ†æˆ content feature å’Œ brightness featureã€‚ä½¿ç”¨ä½å…‰å›¾åƒçš„content featureï¼Œå’Œæ­£å¸¸å…‰å›¾åƒçš„brightness featureï¼Œç„¶åä½¿ç”¨normal-light decoderé‡æ„å‡ºæ¥ã€‚æ­£å¸¸å…‰å‚è€ƒå›¾åƒä¸å¿…ä¸ä½å…‰å›¾åƒå¯¹é½ã€‚  ç»“æ„å›¾   è®­ç»ƒæ•°æ®æ˜¯å‚è€ƒå›¾åƒç²—å¯¹é½çš„æ­£å¸¸å…‰å›¾ï¼Œæµ‹è¯•æ•°æ®å‚è€ƒå›¾åƒå¯ä»¥å®Œå…¨ä¸ä¸€æ ·ã€‚ å‡ ä¸ªLossï¼šï¼ˆ1ï¼‰ä½å…‰å’Œæ­£å¸¸å…‰çš„content featureåº”è¯¥ç±»ä¼¼ã€‚(2) ä½å…‰çš„content featureå’Œbrightness featureåº”è¯¥èƒ½å¤Ÿé‡æ„å‡ºä½å…‰å›¾åƒã€‚ï¼ˆ3ï¼‰æ­£å¸¸å…‰å›¾åƒæ²¡æœ‰gtã€‚ï¼ˆ4ï¼‰ä»¤å…‰ç…§ç‰¹å¾ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œä¸ç»“æ„æ— å…³ã€‚ï¼ˆ5ï¼‰VGG perceptual lossã€‚    Disentangling Noise from Images: A Flow-Based Image Denoising Neural Network #  arxiv 2021 | Github\n Invertible Denoising Network: A Light Solution for Real Noise Removalçš„æœŸåˆŠæ‰©å±•ã€‚  Denoising Normalizing Flow #  NeurIPS 2021\n æœ¬æ–‡å¹¶ä¸æ˜¯ç”¨æ¥å»å™ªçš„Normalizing Flowã€‚ç±»ä¼¼Denoising Autoencoderã€‚  DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows #  CVPR 2021 | Github | Blog:Normalizing Flow\n æå‡ºäº†ä¸€ä¸ªä¸éœ€è¦é…å¯¹æ•°æ®è®­ç»ƒçš„conditional flowç½‘ç»œç”¨äºåˆæˆè®­ç»ƒæ•°æ®ã€‚ å…·ä½“åšæ³•ç®€ä»‹ï¼šéœ€è¦å…ˆäº†è§£Normalizing Flowã€‚TODO  Invertible Denoising Network: A Light Solution for Real Noise Removal #   Github | Blog:Invertible Network\n æå‡ºä½¿ç”¨Invertible NetworkåšçœŸå®å™ªå£°å»å™ªã€‚ ç½‘ç»œå°†noisy imageç¼–ç æˆlow resolution clean imageå’Œä¸€ä¸ªhigh frequencyå’Œnoiseä¿¡æ¯çš„latent vectorã€‚ é€†è¿‡ç¨‹æŠŠlatent vectoræ¢æˆä¸€ä¸ªä»æ ‡å‡†æ­£æ€åˆ†å¸ƒå–æ ·çš„vectorï¼Œç„¶åé€†å›å»ï¼Œå¾—åˆ°clean imageã€‚  é—®é¢˜ï¼ˆæ¯”è¾ƒç–‘æƒ‘çš„åœ°æ–¹ï¼‰ï¼šä¸ºä»€ä¹ˆæ˜¯éšæœºå–æ ·ï¼Œå–å‡ºæ¥çš„latent vectorèµ·çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Œä¸åŒçš„vectorï¼Œæ¢å¤å‡ºæ¥çš„clean imageä¸æ˜¯æ˜¯ä¸ä¸€æ ·çš„ï¼Ÿ\nAdaDM: Enabling Normalization for Image Super-Resolution #   ä¼ ç»Ÿè¶…åˆ†ç½‘ç»œä¸€èˆ¬ä¸åŠ Normalizationï¼Œå› ä¸ºè¿™ä¼šä½¿å¾—ç‰¹å¾æ–¹å·®å˜å°ï¼Œå¯¹æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ã€‚ è¿™ç¯‡æ–‡ç« æ–‡ç« æå‡ºäº† Adaptive Deviation Modulator (AdaDM)ï¼Œå¯ä»¥æ”¾å¤§æ–¹å·®ï¼Œä½¿å¾—Normalizationåˆèƒ½åŠ äº†ã€‚å¹¶ä¸”æœ‰æ€§èƒ½æå‡ã€‚  IDR: Self-Supervised Image Denoising via Iterative Data Refinement #   Github\n æå‡ºäº†ä¸€ä¸ªunsupervised denoising methodï¼Œä¸ç”¨é…å¯¹æ•°æ®è®­ç»ƒã€‚ åŸºäºåœ¨å¸¦å™ªå›¾åƒä¸ŠåŠ noise modelçš„åˆæˆå™ªå£°è®­ç»ƒçš„ç½‘ç»œï¼Œèƒ½å¤Ÿä¸€å®šç¨‹åº¦åœ°å¯¹å¸¦å™ªå›¾åƒé™å™ªã€‚noiser-noisyæ•°æ®é›†å’Œnoisy-cleançš„domain gapè¶Šå°ï¼Œç½‘ç»œé™å™ªæ€§èƒ½è¶Šå¥½ã€‚ æœ¬æ–‡æå‡ºç”¨noiser-noisyæ•°æ®é›†è®­ç»ƒä¸€ä¸ªç½‘ç»œï¼Œç„¶åç”¨ç½‘ç»œå»å™ªå¾—åˆ°æ›´å¹²å‡€çš„noisyå›¾åƒï¼Œå†åˆæˆnoiser-nosiyæ•°æ®é›†ã€‚è¿­ä»£è¿›è¡Œè®­ç»ƒï¼Œä¸æ–­ç¼©å°noiser-noisyæ•°æ®é›†å’Œnoisy-cleançš„domain gapã€‚  Rethinking Noise Synthesis and Modeling in Raw Denoising #   Github\n ç‰©ç†å™ªå£°æ¨¡å‹å¯¹äºçœŸå®åœºæ™¯å»å™ªæ¯”DNNçš„å¥½ä½¿ï¼Œä½†æ˜¯éœ€è¦å¯¹ä¸åŒç›¸æœºï¼Œå…‰ç…§æ¡ä»¶å•ç‹¬å»ºæ¨¡æˆ–è€…æ ‡å®šï¼Œæ¯•ç«Ÿéº»çƒ¦ã€‚ æœ¬æ–‡æå‡ºä»ç›¸æœºå™ªå£°ä¸­éšæœºå–æ ·ä½œä¸ºåˆæˆå™ªå£°ï¼Œå› æ­¤ä¸éœ€è¦å™ªå£°æ¨¡å‹ï¼Œåªéœ€è¦æ‹æ‘„å¤šå¼ å¸¦å™ªå›¾åƒä½œä¸ºdatabaseå³å¯ã€‚ ä½œè€…å°†å™ªå£°åˆ†ä¸ºä¿¡å·ç›¸å…³å’Œä¿¡å·æ— å…³çš„ï¼Œä¿¡å·æ— å…³é‡‡ç”¨éšæœºå–æ ·ï¼Œä¿¡å·ç›¸å…³éƒ¨åˆ†åªè€ƒè™‘photo shot noiseï¼Œä»æ³Šæ¾åˆ†å¸ƒé‡‡æ ·ä½¿ç”¨ï¼Œä»ç„¶éœ€è¦æ ‡å®šã€‚  On Efficient Transformer and Image Pre-training for Low-level Vision #   Github\n æå‡ºäº†ä¸€ä¸ªEncoder-Decoderç»“æ„çš„Transformerã€‚æ¯”IPTå°10å€ï¼Œåªç”¨200kï¼ˆ15.6%çš„ImageNetï¼‰æ•°æ®é¢„è®­ç»ƒã€‚GFLOPSåªæœ‰SwinIRçš„8.4%, 38 vs 451ã€‚ ç ”ç©¶äº†é¢„è®­ç»ƒç­–ç•¥ï¼Œå‘ç°å¤šä»»åŠ¡é¢„è®­ç»ƒæ¯”å•ä»»åŠ¡å’Œå•çº¯å¢åŠ æ•°æ®é‡å¥½ã€‚simply increasing the data scale may not be the op- timal option, and in contrast, multi-related-task pre-training is more effective and data-efficient.  TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions #   Github\n Transformerç»“æ„ï¼Œä¸åŒä»»åŠ¡ç›¸åŒencoderï¼Œdecoderï¼Œä½†æ˜¯decoderï¼ŒæŠŠä»»åŠ¡ç±»å‹ä½œä¸ºqueryä¼ è¿›decoderã€‚  Quick Review   All in One Bad Weather Removal using Architectural Search #   ç”¨å¤šä¸ªEncoderï¼Œä¸€ä¸ªDecoderåšImage Restorationï¼Œå…·ä½“æ˜¯Bad Weather Removalã€‚ Encoderåé¢æ¥äº†ä¸€ä¸ªNASæœçš„ç½‘ç»œï¼Œå…¶ä»–æ²¡æœ‰ç‰¹åˆ«çš„è®¾è®¡ã€‚Feature Spaceä¹Ÿæ²¡ä»€ä¹ˆçº¦æŸã€‚åœ¨å»å™ªç­‰ä¼ ç»Ÿé¢†åŸŸæ•ˆæœæ„Ÿè§‰ä¸ä¸€å®šå¥½ï¼Œå¾ˆå¯èƒ½ä¸å¥½ã€‚  Quick Review   PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing #   Sample Impl ï½œ çŸ¥ä¹è§£è¯» | æœ¬ç«™ ğŸŒŸ\n å›¾åƒåŒ¹é…çš„å…¸ä¸­å…¸ï¼ŒåŸºäºåŒ¹é…å—å‘¨å›´çš„åŒ¹é…ä¹Ÿè¿‘ä¼¼åŒ¹é…çš„å…ˆéªŒçŸ¥è¯†çš„éšæœºåŒ–ç®—æ³•ã€‚ åŸè®ºæ–‡æŒºå¥½è¯»çš„ï¼Œä½†æ˜¯å»ºè®®å…ˆçœ‹è§£è¯»æœ‰ä¸€ä¸ªinsightä¹‹åå†è¯»ã€‚  Quick Review  éšæœºåˆå§‹åŒ–åŒ¹é…å…³ç³»ã€‚ åŸºäºæ­£ç¡®çš„åŒ¹é…å…³ç³»è¿›è¡Œä¿®æ­£ï¼Œå³æ­£ç¡®åŒ¹é…çš„Patch Pairå‘¨å›´çš„patchåº”è¯¥æ˜¯åŒ¹é…çš„ã€‚ åŠ å…¥æ‰°åŠ¨ï¼Œåœ¨ä¿®æ­£ååŒ¹é…çš„åŒ¹é…å¯¹å‘¨å›´æœç´¢æ˜¯å¦æœ‰æ›´åŠ åŒ¹é…çš„patchã€‚     A ConvNet for the 2020s #   Offical Impl\n CNNçš„å¤§å‹Ablation Studyï¼Œç ”ç©¶å„ç§å·²æœ‰æ¨¡å—å’Œè®­ç»ƒæŠ€å·§çš„æœ‰æ•ˆæ€§ï¼ŒåŸºäºæ­¤æ„å»ºäº†ä¸€ä¸ªè¶…è¶Šç°æœ‰Transformerçš„çº¯CNNæ¨¡å‹ã€‚  Masked Autoencoders Are Scalable Vision Learners #   Offical Impl\n åŸºäºTransformerçš„ï¼ˆé«˜å±‚ï¼‰è§†è§‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªç”¨é‡æ„ä»»åŠ¡ã€‚  "},{"id":2,"href":"/blog/docs/gen/","title":"AIGC","section":"Docs","content":"Generated Space #  Denoising is all we need.  Getting Started\n  Introduction  Diffusion Models\nTBD\nScore-based Generative Modelling\n  Reverse Time Stochastic Differential Equations for Generative Modelling  Stochastic Differential Equations  "},{"id":3,"href":"/blog/docs/matrix/","title":"Matrix","section":"Docs","content":"Matrix #  æ­£åœ¨åŠªåŠ›ç¼–å†™ä¸­âœï¸  çŸ©é˜µåˆ†æé€ŸæŸ¥è¡¨: [ PDF]\n  å¦‚ä½•å°†å„ä¸ªçŸ¥è¯†ç‚¹ä¸²èµ·æ¥ï¼Ÿ  æ‚ä¸ƒæ‚å…«: ä¸€äº›æ— æ³•åˆ†ç±»çš„å°çŸ¥è¯†  çº¿æ€§ç©ºé—´  å„ç§çŸ©é˜µ\n  å¯¹ç§°çŸ©é˜µ  æ­£å®šçŸ©é˜µ  æ­£äº¤çŸ©é˜µ  ç›¸ä¼¼çŸ©é˜µ : âš ï¸ ç›¸ä¼¼çŸ©é˜µæ˜¯æŒ‡ä¸€ç³»åˆ—äº’ç›¸ç›¸ä¼¼çš„çŸ©é˜µã€‚  ç‰¹å¾å€¼/ç‰¹å¾å‘é‡\n  åŸºç¡€çŸ¥è¯†  çŸ©é˜µå¯¹è§’åŒ– : åˆ©ç”¨ç‰¹å¾å€¼ï¼Œç‰¹å¾å‘é‡è¿›è¡Œã€‚  Jordan Form  çŸ©é˜µåˆ†è§£\n  ç‰¹å¾å€¼åˆ†è§£(çŸ©é˜µå¯¹è§’åŒ–) : å¯¹è§’åŒ–ä¹Ÿç®—æ˜¯ä¸€ç§åˆ†è§£ã€‚  å¥‡å¼‚å€¼åˆ†è§£ : ç‰¹å¾å€¼åˆ†è§£(eigen decomposition)çš„æ‰©å±•  è°±åˆ†è§£ : ç‰¹å¾å€¼åˆ†è§£çš„å¦ä¸€ç§è¡¨è¾¾å½¢å¼ã€‚  "},{"id":4,"href":"/blog/posts/ai/drag_gan/","title":"DragGAN æŠ¢å…ˆä½“éªŒä¸æœ¬åœ°éƒ¨ç½²æ•™ç¨‹","section":"Posts","content":"æœ€è¿‘é£é¡å…¨ç½‘çš„DragGAN, å®˜æ–¹ä»£ç å°šæœªæ”¾å‡ºã€‚ä¸è¿‡ç°åœ¨å·²ç»å¯ä»¥æŠ¢å…ˆä½“éªŒå•¦ã€‚\né¡¹ç›®åœ°å€\n  Zeqiang-Lai/DragGAN: ç›¸å…³ä»£ç æ¨¡å‹ï¼Œæ”¯æŒæœ¬åœ°éƒ¨ç½²ï¼ŒColabåœ¨çº¿ä½“éªŒã€‚  OpenGVLab/InternGPT: å¯ä»¥å…è´¹åœ¨çº¿ä½“éªŒ  åœ¨çº¿ä½“éªŒ #   InternGPT  Google Colab  CodewithGPU   âš ï¸ æ³¨æ„ Google Colab è®°å¾—é€šè¿‡ä»£ç æ‰§è¡Œç¨‹åº/æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ é€‰æ‹©ä¸€å—GPUã€‚\n Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!  Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!  æœ¬åœ°éƒ¨ç½² - Pip Install æ–¹å¼ #  æ¥æ¥ä¸‹çš„å›¾ç‰‡å±•ç¤ºä»¥Windowsä¸‹çš„éƒ¨ç½²ä¸ºä¾‹ï¼ŒLinuxä¸‹çš„éƒ¨ç½²ä¹Ÿæ˜¯ç›¸åŒçš„\nç›®å‰ï¼Œ Zeqiang-Lai/DragGAN çš„å®ç°å·²ç»ä¸Šä¼ åˆ° PyPI æºä¸Šäº†ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æ— éœ€ä¸‹è½½ä»£ç ï¼Œåªéœ€è¦ä½¿ç”¨ pip install å³å¯è¿›è¡Œå®‰è£…ã€‚\nå®‰è£… Conda #  ä¸ºäº†é¿å…ä¾èµ–å†²çªï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨Condaåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…Condaï¼Œå¯ä»¥åœ¨ è¿™é‡Œä¸‹è½½ä¸€ä¸ªMinicondaã€‚\n ä¸‹è½½å®Œæˆåï¼Œç‚¹å‡»å®‰è£…åŒ…ä¸€ç›´ä¸‹ä¸€æ­¥å°±å¯ä»¥äº†ã€‚\n åˆ›å»º Conda è™šæ‹Ÿç¯å¢ƒ #  æ¥ä¸‹æ¥ä» Windows èœå•æ é€‰æ‹© Anaconda Powershell Prompt (miniconda3) è¿›å…¥Conda çš„å‘½ä»¤è¡Œã€‚\n è¿›å…¥ä¹‹åï¼Œè¾“å…¥ä»¥ä¸‹æŒ‡ä»¤åˆ›å»ºä¸€ä¸ªåä¸º draggan çš„ç¯å¢ƒï¼Œpython ç‰ˆæœ¬ä¸º3.7ã€‚æç¤ºæ˜¯å¦ç»§ç»­çš„æ—¶å€™è¾“å…¥ y å³å¯ç»§ç»­ã€‚\nconda create -n draggan python=3.7  å› ä¸ºæˆ‘è¿™æŠŠå·²ç»æœ‰ä¸€ä¸ªç¯å¢ƒå«dragganäº†ï¼Œæ‰€ä»¥å›¾ç‰‡é‡Œç”¨çš„æ˜¯draggan2\n  å®‰è£… PyTorch #  æˆ‘ä»¬é¦–å…ˆæ¿€æ´»ä¸€ä¸‹åˆšåˆšåˆ›å»ºçš„ç¯å¢ƒï¼Œè¾“å…¥ä»¥ä¸‹æŒ‡ä»¤å³å¯\nconda activate draggan  æ¥ç€ï¼Œå‚è€ƒPyTorchçš„ å®˜æ–¹å®‰è£…æ•™ç¨‹ï¼Œ\n æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£…PyTorchï¼ŒäºŒé€‰ä¸€å³å¯ï¼Œå…·ä½“é€‰å“ªä¸ªæŒ‰ä¸‹è½½é€Ÿåº¦è‡ªè¡Œé€‰æ‹©ï¼Œ\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia æ²¡æœ‰GPUçš„ç”¨æˆ·ç”¨è¿™ä¸ªæŒ‡ä»¤å®‰è£…\npip3 install torch torchvision torchaudio  å½“å‡ºç° Successfully installed å°±è¯´æ˜å®‰è£…æˆåŠŸå•¦ï¼Œå…¶ä»– WARNING éƒ½ä¸ç”¨ç®¡ã€‚\nå®‰è£… DragGAN #  å®‰è£…å®Œæˆä¹‹åï¼Œæˆ‘ä»¬å®‰è£…DragGANï¼Œè¿™å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œ\npip install draggan å› ä¸ºä¸€äº›æˆ‘ä¹Ÿä¸çŸ¥é“çš„åŸå› ï¼Œæ¸…åpipæºæ²¡æœ‰åŒæ­¥draggan è¿™ä¸ªåŒ…ï¼Œå¦‚æœä½ çš„ pip é…ç½®è¿‡æ¸…åæˆ–å›½å†…çš„pipæºï¼Œä½ å¯èƒ½ä¼šé‡åˆ°åŒ…æ‰¾ä¸åˆ°çš„é—®é¢˜\n è¿™æ—¶å€™ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªæŒ‡ä»¤ï¼Œä¸´æ—¶ä½¿ç”¨å®˜æ–¹æºè¿›è¡Œå®‰è£…\npip install draggan -i https://pypi.org/simple/  ä¸PyTorchå®‰è£…ç±»ä¼¼ï¼Œå½“å‡ºç° Successfully installed å°±è¯´æ˜å®‰è£…æˆåŠŸäº†ï¼Œå…¶ä»– WARNING éƒ½ä¸ç”¨ç®¡ã€‚\n è‡³æ­¤ï¼Œæ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆï¼Œæ¥ä¸‹æ¥å¯ä»¥å¼€å§‹è¿è¡Œäº†ã€‚\nè¿è¡Œ DragGAN Demo #  ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤è¿è¡Œ DragGAN çš„ Demo\npython -m draggan.web å¦‚æœä½ ä¸å°å¿ƒå…³æ‰äº†å‘½ä»¤è¡Œï¼Œä¹Ÿä¸ç”¨é‡æ–°å®‰è£…ï¼Œé€šè¿‡ Anaconda Powershell Prompt (miniconda3) é‡æ–°è¿›å…¥Conda çš„å‘½ä»¤è¡Œï¼Œæ¿€æ´»ç¯å¢ƒï¼Œè¿è¡Œå³å¯ã€‚\nconda activate draggan python -m draggan.web æ²¡æœ‰GPUçš„ç”¨æˆ·ï¼Œä½¿ç”¨\npython -m draggan.web --device cpu å½“å‡ºç°è¿™ä¸ªç½‘å€çš„æ—¶å€™ http://127.0.0.1:7860 ï¼Œè¯´æ˜ç¨‹åºå·²ç»æˆåŠŸè¿è¡Œ\n å°†è¿™ä¸ªç½‘å€è¾“å…¥åˆ°æµè§ˆå™¨é‡Œå°±å¯ä»¥è®¿é—®åˆ° DragGAN çš„ Demo äº†\n åŠŸèƒ½ä»‹ç» #  ç•Œé¢åŠŸèƒ½ä»‹ç»å¦‚ä¸‹\n  é€‰æ‹©æ¨¡å‹ï¼šç›®å‰æˆ‘ä»¬æä¾›äº†10ä¸ªæ¨¡å‹ï¼ˆåœ¨webç•Œé¢é€‰æ‹©åä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰ï¼Œä¸åŒæ¨¡å‹è¾“å‡ºå›¾ç‰‡åˆ†è¾¨ç‡ï¼Œå’Œå¯¹æ˜¾å­˜è¦æ±‚ä¸ä¸€æ ·ï¼Œå…·ä½“å¦‚ä¸‹  æ¨¡å‹ä¿¡æ¯æ±‡æ€»    åç§° åˆ†è¾¨ç‡ æ˜¾å­˜å ç”¨ (MB)     stylegan2-ffhq-config-f.pt 1024 7987   stylegan2-cat-config-f.pt 256 4085   stylegan2-church-config-f.pt 256 4085   stylegan2-horse-config-f.pt 256 4085   ada/ffhq.pt 1024 7987   ada/afhqcat.pt 512 4473   ada/afhqdog.pt 512 4473   ada/afhqwild.pt 512 4473   ada/brecahad.pt 512 4473   ada/metfaces.pt 512 4473         æœ€å¤§è¿­ä»£æ­¥æ•°ï¼šæœ‰äº›æ¯”è¾ƒå›°éš¾çš„æ‹–æ‹½ï¼Œéœ€è¦å¢å¤§è¿­ä»£æ¬¡æ•°ï¼Œå½“ç„¶ç®€å•çš„ä¹Ÿå¯ä»¥å‡å°‘ã€‚\n  è®¾ç½®æ‹–æ‹½ç‚¹å¯¹ï¼Œæ¨¡å‹ä¼šå°†è“è‰²çš„ç‚¹æ‹–æ‹½åˆ°çº¢è‰²ç‚¹ä½ç½®ã€‚è®°ä½éœ€è¦åœ¨ Setup handle points è®¾ç½®æ‹–æ‹½ç‚¹å¯¹ã€‚\n  è®¾ç½®å¯å˜åŒ–åŒºåŸŸï¼ˆå¯é€‰ï¼‰ï¼šè¿™éƒ¨åˆ†æ˜¯å¯é€‰çš„ï¼Œä½ åªéœ€è¦è®¾ç½®æ‹–æ‹½ç‚¹å¯¹å°±å¯ä»¥æ­£å¸¸å…è®¸ã€‚å¦‚æœä½ æƒ³çš„è¯ï¼Œ ä½ å¯ä»¥åœ¨ Draw a mask è¿™ä¸ªé¢æ¿ç”»å‡ºä½ å…è®¸æ¨¡å‹æ”¹å˜çš„åŒºåŸŸã€‚æ³¨æ„è¿™æ˜¯ä¸€ä¸ªè½¯çº¦æŸï¼Œå³ä½¿ä½ åŠ äº†è¿™ä¸ªmaskï¼Œæ¨¡å‹è¿˜æ˜¯æœ‰å¯èƒ½ä¼šæ”¹å˜è¶…å‡ºè®¸å¯èŒƒå›´çš„åŒºåŸŸã€‚\n  è§†é¢‘æ•™ç¨‹ #  æ•¬è¯·æœŸå¾…\n"},{"id":5,"href":"/blog/posts/ai/detr/","title":"Detr Family for End-to-End Detection","section":"Posts","content":"Detrï¼ˆDetection Transformerï¼‰æ˜¯ facebook åœ¨ 2020 å¹´æå‡ºçš„ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œ å®ƒæ”¹å˜äº†ç°æœ‰åŸºäº Fast-RCNN å’Œ YOLO çš„ç›®æ ‡æ£€æµ‹èŒƒå¼ï¼Œåç»­æœ‰è®¸å¤šå·¥ä½œï¼ŒåŸºäº Detr æå‡ºäº†å„ç§ä¸ªæ ·çš„æ”¹è¿›ã€‚\nTimeline #   Detr (ECCV 2020): End-to-End Object Detection with Transformers Deformable Detr (ICLR 2021): Deformable DETR: Deformable Transformers for End-to-End Object Detection Conditional Detr (ICCV 2021): Conditional DETR for Fast Training Convergence Anchor Detr (AAAI 2021): Anchor DETR: Query Design for Transformer-Based Object Detection DAB Detr (ICLR 2022): DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR DN Detr (CVPR 2022): DN-DETR: Accelerate DETR Training by Introducing Query DeNoising  1. Detr #  Detr (Citation: Carion,\u0026#32;Massa \u0026amp; al.,\u0026#32;2020Carion,\u0026#32; N.,\u0026#32; Massa,\u0026#32; F.,\u0026#32; Synnaeve,\u0026#32; G.,\u0026#32; Usunier,\u0026#32; N.,\u0026#32; Kirillov,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Zagoruyko,\u0026#32; S. \u0026#32; (2020). \u0026#32;End-to-End Object Detection with Transformers.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2005.12872 ) æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œå®ƒç›´æ¥å®šä¹‰äº† N1 ä¸ªç›®æ ‡æ¡†çš„ slotï¼Œè¿™é‡Œ Detr ç§°å…¶ä¸º object queryã€‚\nåœ¨ä¼˜åŒ–çš„æ—¶å€™ï¼Œè¿™ N ä¸ª slot å°†ä¼šé€šè¿‡ transformer è¿›è¡Œ self attention ï¼Œä»¥åŠå’Œè¾“å…¥å›¾åƒè¿›è¡Œ cross attentionã€‚ é€šè¿‡è¿™ä¸ªæ–¹å¼ï¼Œæ¯ä¸ª slot å°†ä¼šé€æ­¥è¿›åŒ–æˆå¯ä»¥é¢„æµ‹ç›®æ ‡æ¡†åŠå…¶ç±»åˆ«çš„ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚æœ€ç»ˆé€šè¿‡ä¸¤ä¸ªFFNï¼Œå³å¯å›å½’å‡ºç›®æ ‡æ¡†çš„ä½ç½®å’Œç±»åˆ«ã€‚\n Detræ€»ä½“ç»“æ„\n  Encoder #  Transformer çš„ Encoder å¦‚æœå»æ‰çš„è¯ï¼Œä¼šæ‰ç‚¹ï¼š\n overall AP drops by 3.9 points, with a more signiï¬cant drop of 6.0 AP on large objects.\n ä¸€ä¸ªè§£é‡Šæ˜¯ï¼Œtransformer çš„ self attention å¯ä»¥æå–å…¨å±€ç‰¹å¾ã€‚\n ä»å¯è§†åŒ–çš„ attention map æ¥çœ‹ï¼Œä¸€ä¸ªç‰©ä½“ä¸Šçš„æŸä¸ªç‚¹çš„ attention map éƒ½èšé›†åœ¨è¿™ä¸ªç‰©ä½“çš„å…¶ä»–éƒ¨åˆ†ã€‚ ä½†è¿™å…¶å®å¾ˆ trivialï¼Œattention map æœ¬èº«æ˜¯ä¸¤ä¸ª token çš„ç›¸ä¼¼åº¦ï¼ŒåŒä¸€ä¸ªç‰©ä½“ä¸åŒåŒºåŸŸçš„ç›¸ä¼¼åº¦è‡ªç„¶å¤§ï¼Œå¯¹äºè¿™ä¸ªç¤ºæ„å›¾æ›´æ˜¯ï¼ŒåŒä¸€ä¸ªç‰©ä½“ï¼Œå…·æœ‰æ˜æ˜¾ç›¸ä¼¼çš„é¢œè‰²å’Œçº¹ç†ç‰¹å¾ã€‚ æ‰€ä»¥ï¼Œè¿™ä¸ª attention map èƒ½è¿™ä¹ˆæ˜æ˜¾çš„åŒºåˆ†ç‰©ä½“ï¼Œæœ‰æ²¡æœ‰å¯èƒ½åªæ˜¯å› ä¸ºé¢œè‰²ç›¸è¿‘å‘¢ï¼Ÿå¦‚æœæœ‰ä¸¤åªå¾ˆåƒçš„ç‰›ï¼Œè·ç¦»ä¹Ÿæ¯”è¾ƒè¿‘ï¼Œé‚£ä¹ˆæ­¤æ—¶çš„ attention map ä¼šæ˜¯ä»€ä¹ˆæ ·å‘¢ï¼Ÿ  è¿›ä¸€æ­¥çš„ï¼Œæ—¢ç„¶ encoder çš„ç‰¹å¾å°±å·²ç»å¾ˆæœ‰åŒºåˆ†åº¦äº†ï¼ŒåŒä¸€ä¸ªç‰©ä½“çš„ç‰¹å¾æ¯”è¾ƒåƒäº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬èƒ½ä¸èƒ½é€šè¿‡èšç±»çš„æ–¹å¼ï¼Œæå–å‡ºæ¯ä¸ªç‰©ä½“çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡ä¸¤ä¸ª FFN é¢„æµ‹ bbox å’Œ class å‘¢ï¼Ÿ\n Position Embedding #    ä¸éš¾æƒ³åˆ°ï¼Œdetr ä½¿ç”¨ transformerï¼Œå¦‚æœä¸åŠ  positional encoding çš„è¯ï¼Œfeature æ˜¯ä¸å¸¦ä½ç½®ä¿¡æ¯çš„ï¼Œè¿™æ ·åº”è¯¥æ˜¯å¾ˆéš¾é¢„æµ‹ bbox çš„ä½ç½®çš„ã€‚ è®ºæ–‡å®éªŒç»“æœä¹Ÿä½“ç°äº†è¿™ä¸€ç‚¹ï¼Œå»æ‰ encoder å’Œ decoder çš„ positional encoding ç›´æ¥æ‰äº† 7 ä¸ªç‚¹ã€‚\n  ä½†æ˜¯ï¼Œå³ä¾¿å¦‚æ­¤ï¼Œè¿˜æ˜¯èƒ½å–å¾— 30 å¤šçš„ APï¼Œ è¿™è¯´æ˜åªç”¨ object query æˆ‘ä»¬ä¹Ÿèƒ½å­¦åˆ°ä½ç½®ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥æƒ³è±¡ object query å­¦åˆ°äº†å…³æ³¨æŸäº›ç‰¹å®šåŒºåŸŸï¼Œ å³æŸäº›åæ ‡åŒºåŸŸï¼Œç„¶åé€šè¿‡ cross attention æ¥åˆ¤æ–­ï¼Œè¿™ä¸ª slot é‡Œæœ‰æ²¡æœ‰ç‰©ä½“ã€‚ç„¶è€Œï¼Œencoder feature å¹¶æ²¡æœ‰ä»»ä½•ä½ç½®ä¿¡æ¯ï¼Œcross attention æ€ä¹ˆçŸ¥é“ï¼Œè¿™ä¸ª slot å¯¹åº”çš„ feature åœ¨å“ªå‘¢ï¼Ÿ å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ç†ç”±æ€€ç–‘ï¼Œè¿™é‡Œå¯èƒ½æœ‰éšè—çš„ä½ç½®ä¿¡æ¯æ³„æ¼ï¼Œæ¯ä¸ª slot å¯èƒ½ä¼šå…³æ³¨å›ºå®šä½ç½®çš„ encoder feature tokenã€‚\n   Object Query #  ç†è§£ Object Query æ˜¯ç†è§£ Detr è®¾è®¡çš„æ ¸å¿ƒã€‚Object query å…¶å®å°±æ˜¯é¢„è®¾çš„æ£€æµ‹æ¡†çš„ä¸€ç³»åˆ— slotï¼Œå¯¹äºæŸä¸ª slotï¼Œtransformer decode çš„è¿‡ç¨‹ï¼Œå…¶å®å°±ç›¸å½“äºï¼Œæˆ‘ä»¬ç”¨è¿™ä¸ª slot å»æŸ¥è¯¢ encoder featureï¼Œç„¶åæŠŠä¸è¿™ä¸ª slot ç›¸å…³çš„ slot æå–å‡ºæ¥ï¼Œå­˜åˆ° decoder embedding é‡Œã€‚\nDetr çš„åšæ³•å’Œè¿™ä¸ª idea ç±»ä¼¼ï¼Œä½†æ˜¯å®ƒæ¯ä¸€æ­¥åœ¨ä¼˜åŒ–çš„å…¶å®æ˜¯ transformer çš„ queryï¼Œæ¯æ¬¡çš„ output value ä»ç„¶æ˜¯ encoder feature çš„åŠ æƒã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½ç›´æ¥ä¼˜åŒ– valueï¼Œæ¯æ¬¡çš„ output value ç›¸å½“äºæ˜¯ value çš„ä¸€ä¸ª residual refinement å‘¢ï¼Ÿ\nå…³äº Object queryï¼Œè®ºæ–‡è¿˜åšäº†ä¸€äº›å¯è§†åŒ–çš„åˆ†æï¼Œ\n é¦–å…ˆï¼Œæ¯ä¸ª object query éƒ½ä¼šå€¾å‘äºå…³æ³¨è¿™ä¸ª query è´Ÿè´£ç‰©ä½“çš„è¾¹ç•ŒåŒºåŸŸã€‚ å…¶æ¬¡ï¼Œobject query å¯¹äºç‰©ä½“ç±»åˆ«å¹¶æ²¡æœ‰ç‰¹åˆ«çš„å€¾å‘æ€§ã€‚ æœ€åï¼Œobject query ä¼¼ä¹å¯¹äºä¸åŒä½ç½®çš„ç‰©ä½“å…·æœ‰ä¸€å®šå€¾å‘æ€§ï¼Œè¿™ä¹Ÿæ˜¯åç»­å¾ˆå¤šæ–‡ç« æ”¹è¿›çš„åŸºçŸ³ã€‚  ä¸ºäº†è¯æ˜ã€2ã€‘è¿™ä¸€ç‚¹ï¼Œä½œè€…åšäº†ä¸€ä¸ªå®éªŒï¼Œä½œè€…åˆæˆäº†ä¸€å¼ æœ‰ 24 ä¸ªé•¿é¢ˆé¹¿çš„å›¾ï¼Œåœ¨æ•°æ®é›†é‡Œï¼Œ æ²¡æœ‰ä¸€å¼ å›¾æ˜¯æœ‰è¶…è¿‡ 24 åªé•¿é¢ˆé¹¿çš„ï¼Œå¦‚æœ object query å¯¹ç‰©ä½“ç±»åˆ«æœ‰å€¾å‘æ€§çš„è¯ï¼Œé‚£ä¹ˆå°±åˆ†ä¸å‡ºæ¥ 24 ä¸ªé•¿é¢ˆé¹¿ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸è¶³ä»¥è®­ç»ƒå‡º 24 ä¸ªå¯¹é•¿é¢ˆé¹¿æœ‰åå¥½çš„ queryã€‚\n Object query çš„ attention, å€¾å‘äºå…³æ³¨ç‰©ä½“è¾¹ç•Œ\n    Out of distribution Results\n    ä¸åŒ object query é¢„æµ‹æ£€æµ‹æ¡†çš„åˆ†å¸ƒ\n  Panoptic Segmentation #  Detr è¿™ä¸ªæ¶æ„å¯ä»¥æ‰©å±•åˆ° Panoptic Segmentationï¼Œæ€è·¯æ˜¯\n detr ä¼šè¾“å‡ºæ¯ä¸ªæ£€æµ‹ç‰©ä½“çš„ embeddingï¼Œæ¯ä¸ª object query æœ‰ä¸€ä¸ªã€‚ å›å¿†ï¼Œencoder é‚£ä¸€èŠ‚æˆ‘ä»¬ä¹Ÿæåˆ°äº†ï¼Œå…¶å® encoder çš„ç»“æœå°±å·²ç»æœ‰ä¸€ä¸ªèšç±»çš„æ•ˆæœäº†ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ª attentionï¼ŒæŠŠæ¯ä¸ªæ£€æµ‹ç‰©ä½“çš„è¦†ç›–åŒºåŸŸï¼Œé€šè¿‡ attention map å¤§æ¦‚ä¼°è®¡å‡ºæ¥ã€‚ å¯¹äºæ¯ä¸ªæ£€æµ‹ç‰©ä½“ï¼Œéƒ½æœ‰ä¸€ä¸ª attention mapï¼Œç„¶åæˆ‘ä»¬ç»“åˆ Resnet çš„ç‰¹å¾ï¼Œè¿‡ä¸€ä¸ª CNNï¼Œç„¶åè¿‡ä¸€ä¸ªäºŒåˆ†ç±» FFNï¼Œå¾—åˆ°æ¯ä¸ªæ£€æµ‹ç‰©ä½“ç»†åŒ–çš„äºŒåˆ†ç±» segmentation ç»“æœã€‚ æœ€åï¼Œæ¯ä¸ªç‰©ä½“çš„äºŒåˆ†ç±»ç»“æœï¼Œä¼šé€šè¿‡ argmax æ¦‚ç‡å¾—åˆ°æœ€ç»ˆçš„ panoptic segmentation çš„ç»“æœã€‚  PSï¼šè¿™ä¹ˆåšçš„è¯ï¼Œåº”è¯¥æ˜¯åªèƒ½åº”ç”¨åˆ° Panoptic Segmentationï¼Œå› ä¸ºæ¯ä¸ªåƒç´  segmentation çš„ç»“æœå¿…é¡»å±äºæ£€æµ‹é‚£ä¸€æ­¥çš„æŸä¸ªç±»åˆ«æ‰è¡Œã€‚\n Illustration of the panoptic head. A binary mask is generated in parallel for each detected object, then the masks are merged using pixel-wise argmax.\n  ä¸€äº›ç»†èŠ‚ #   Transformer Encoder çš„ Qï¼ŒK æ˜¯å¸¦ Pos çš„ï¼Œä½†æ˜¯ V ä¸å¸¦ã€‚ Transformer Decoder é‡Œ V è¿˜æ˜¯ä¸å¸¦ Pos ã€1ã€‘ï¼ŒQ å’Œ V åˆ†åˆ«å¸¦ä¸åŒçš„ Posã€‚ å¾…è¡¥å……  ã€1ã€‘: é‚£FFNçš„æ—¶å€™,ä½ç½®ä¿¡æ¯åˆ°åº•ä»å“ªæ¥çš„ ğŸ¤¨ï¼Ÿ   2. Deformable Detr #  Deformable Detr (Citation: Zhu,\u0026#32;Su \u0026amp; al.,\u0026#32;2021Zhu,\u0026#32; X.,\u0026#32; Su,\u0026#32; W.,\u0026#32; Lu,\u0026#32; L.,\u0026#32; Li,\u0026#32; B.,\u0026#32; Wang,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Dai,\u0026#32; J. \u0026#32; (2021). \u0026#32;Deformable DETR: Deformable Transformers for End-to-End Object Detection.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2010.04159 ) ä¸»è¦æ˜¯æå‡ºäº† Deformable Attention Module ç”¨äºæ”¹è¿› Detr çš„ä¸¤ä¸ªé—®é¢˜ï¼š\n æ”¶æ•›æ…¢ï¼šä½œè€… claim è¿™ä¸ªç‚¹æ˜¯æ¥è‡ªäº Detr æ‰€ç”¨çš„ attention æ˜¯ä¸ªå…¨å±€ attentionï¼Œä¸€å¼€å§‹éƒ½æ˜¯å¹³å‡çš„æƒé‡ï¼Œä½†æœ€åä¸€èˆ¬ä¼šæ”¶æ•›åˆ°æ¯”è¾ƒ sparse çš„æƒé‡ã€‚ è¿™ä¸ªå­¦ä¹ çš„è¿‡ç¨‹ä¼šæ¯”è¾ƒæ…¢ã€‚ è®¡ç®—å¤æ‚åº¦å¤§ï¼šè¿˜æ˜¯å› ä¸ºæ˜¯å…¨å±€ attentionï¼Œæ—¶é—´å¤æ‚åº¦éšå›¾åƒå¤§å°å¹³æ–¹å¢é•¿ã€‚  Deformable Attention #  Deformable Attention è§£å†³ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜çš„æ–¹å¼ï¼Œå°±æ˜¯é€šè¿‡ Deformable å¾—åˆ°ä¸€ä¸ªå±€éƒ¨çš„ token listï¼Œç„¶ååªåœ¨è¿™ä¸ª token list å†…åš attentionã€‚\nä¸‹é¢æ˜¯ Deformable Attention çš„ç¤ºæ„å›¾ï¼Œä¸€äº›è¦ç‚¹\n Deformable Attention éœ€è¦ä¸€ä¸ª reference pointï¼Œç„¶åå±€éƒ¨ token list æ˜¯é€šè¿‡è®¡ç®—ä»¥ reference point ä¸ºä¸­å¿ƒçš„ offset å¾—åˆ°çš„ã€‚ å¯¹äº encoder æ¥è¯´ï¼ŒQï¼ŒKï¼ŒV éƒ½æ˜¯ 2D å›¾ç‰‡ï¼Œæ‰€ä»¥ reference point å°±æ˜¯ Q é‡Œæ¯ä¸ª token çš„è‡ªèº«åæ ‡ï¼ˆå½’ä¸€åŒ–åˆ° 0ï½1ï¼‰ã€‚ å¯¹äº decodedr çš„ cross attentionï¼ŒQ æ˜¯ object queryï¼Œæ²¡æœ‰åæ ‡ï¼Œè¿™é‡Œçš„ reference point æ˜¯é€šè¿‡ä¸€ä¸ª learnable linear projection followed by a sigmoid function å¾—åˆ°çš„ã€‚ æœ€åçš„ bbox prediction head é¢„æµ‹çš„æ˜¯ reference point çš„ offset å’Œ bbox çš„é•¿å’Œå®½ã€‚   Deformable Attention ç¤ºæ„å›¾\n  Multi-scale #  ä½œè€…å®é™…åœ¨ç”¨çš„æ—¶å€™ï¼Œæ˜¯ä½¿ç”¨çš„ Multi-scale Deformable Attention ã€‚\n è¿™ä¸ªå¤šå±‚çº§çš„ç‰ˆæœ¬ï¼Œä¸»è¦åŒºåˆ«å°±æ˜¯ deformable çš„å±€éƒ¨ token list æ‰©å±•åˆ°å¤šä¸ªå±‚çº§çš„ feature map ä¸Šã€‚ ä¸ºäº†åŒºåˆ†ä¸åŒ scale çš„ queryï¼Œä½œè€…è¿˜åŠ å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„ scale embedding åˆ° query é‡Œé¢ã€‚  Additional Improvement #  Iterative Bounding Box Refinement #  æ¯ä¸€å±‚ Decoder layer é¢„æµ‹çš„ output embedding éƒ½è¿‡ä¸€ä¸ª detection head å¾—åˆ°ä¸€ä¸ª bbox é¢„æµ‹ç»“æœã€‚\n ç„¶åå°†è¿™ä¸ªé¢„æµ‹ç»“æœçš„ä¸­å¿ƒç‚¹ï¼Œä½œä¸ºä¸‹ä¸€å±‚çš„ decoder layer çš„ reference point ä¸‹ä¸€å±‚é¢„æµ‹çš„ deformable offset ä¹Ÿç”¨é¢„æµ‹ç»“æœçš„ wï¼Œh è°ƒåˆ¶ä¸€ä¸‹ã€‚  Two-Stage Deformable DETR #  ç”¨ Transformer Encoder è¾“å‡ºçš„ feature mapï¼Œå¯¹æ¯ä¸ªåƒç´ çš„ç‰¹å¾éƒ½è¿‡ä¸€ä¸ª prediction head å¾—åˆ°ä¸€ä¸ªç²—çš„ bboxï¼Œç±»åˆ«åªæœ‰å‰æ™¯å’ŒèƒŒæ™¯ã€‚ç”¨è¿™äº› proposed region çš„ä¸­å¿ƒç‚¹ï¼Œä½œä¸ºä¸‹ä¸€æ­¥ object query çš„ reference pointã€‚\n3. Conditional Detr #   å®˜æ–¹çŸ¥ä¹ä¸“æ è§£è¯»\nConditional Detr (Citation: Meng,\u0026#32;Chen \u0026amp; al.,\u0026#32;2021Meng,\u0026#32; D.,\u0026#32; Chen,\u0026#32; X.,\u0026#32; Fan,\u0026#32; Z.,\u0026#32; Zeng,\u0026#32; G.,\u0026#32; Li,\u0026#32; H.,\u0026#32; Yuan,\u0026#32; Y.,\u0026#32; Sun,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; J. \u0026#32; (2021). \u0026#32;Conditional DETR for Fast Training Convergence. https://doi.org/10.48550/arXiv.2108.06152 ) çš„æ ¸å¿ƒæ€æƒ³çœŸçš„éå¸¸ç®€å•ï¼Œå°±æ˜¯æŠŠåŸå§‹ Detr ä¸­ cross attention çš„ Q å’Œ K æ¢äº†ä¸€ä¸‹ã€‚\nSeperate Embedding #  å…·ä½“æ¥è¯´ï¼ŒQ å’Œ K éƒ½æ˜¯ç”±ä¸€ä¸ª content embedding å’Œ position (ä½œè€…è¿™é‡Œç§°ä¸ºspatial) embeddingï¼Œé€šè¿‡ add å¾—åˆ°ï¼ŒConditional Detr å°±æ˜¯æŠŠ add å˜æˆäº† concateã€‚\nèƒŒåçš„åŸç†ï¼Œä½œè€…æ˜¯è¿™æ ·è§£é‡Šçš„ï¼Œä½¿ç”¨ add çš„æ–¹å¼ï¼Œç®— attention weight æ˜¯è¿™æ ·çš„ï¼š\n $$ \\begin{aligned} \u0026\\left(\\mathbf{c}_{q}+\\mathbf{p}_{q}\\right)^{\\top}\\left(\\mathbf{c}_{k}+\\mathbf{p}_{k}\\right) \\\\ =\u0026 \\mathbf{c}_{q}^{\\top} \\mathbf{c}_{k}+\\mathbf{c}_{q}^{\\top} \\mathbf{p}_{k}+\\mathbf{p}_{q}^{\\top} \\mathbf{c}_{k}+\\mathbf{p}_{q}^{\\top} \\mathbf{p}_{k} \\end{aligned} $$  å¯ä»¥çœ‹åˆ° content embedding ä¸ä»…è¦å’Œ content embedding åšç›¸ä¼¼åº¦åŒ¹é…ï¼Œè¿˜è¦å’Œ spatial embedding åšç›¸ä¼¼åº¦åŒ¹é…ã€‚ è¿™æ„å‘³ç€ content embedding ä¸ä»…è¦åŒ…å« content çš„ä¿¡æ¯ï¼Œè¿˜éœ€è¦åŒ…å«ä¸€å®šçš„ spatial çš„ä¿¡æ¯ã€‚è¿™ä¼šä½¿å¾—è®­ç»ƒéš¾åº¦åŠ å¤§ã€‚\nå¦‚æœæ”¹æˆ concate å½¢å¼ï¼Œé‚£ä¹ˆ attention weight çš„è®¡ç®—å°±å˜æˆäº†ï¼š\n $$ \\mathbf{c}_{q}^{\\top} \\mathbf{c}_{k}+\\mathbf{p}_{q}^{\\top} \\mathbf{p}_{k} $$  è¿™æ—¶å€™ content embedding åªå’Œ content embedding åšåŒ¹é…ï¼Œspatial embedding åªå’Œ spatial embedding åšåŒ¹é…ã€‚ è¿™æ ·å°±é™ä½äº†è®­ç»ƒéš¾åº¦ã€‚\näº‹å®ä¸Šï¼Œå•çº¯æ”¹æˆ concate å¹¶ä¸èƒ½æç‚¹ï¼Œä½œè€…åœ¨çŸ¥ä¹ä¸“æ å›å¤äº†ï¼ŒDetr æ”¹æˆ concate è¿˜æ‰ç‚¹äº†ã€‚\nReference Point #  ç±»ä¼¼ Deformable Detr (Citation: Zhu,\u0026#32;Su \u0026amp; al.,\u0026#32;2021Zhu,\u0026#32; X.,\u0026#32; Su,\u0026#32; W.,\u0026#32; Lu,\u0026#32; L.,\u0026#32; Li,\u0026#32; B.,\u0026#32; Wang,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Dai,\u0026#32; J. \u0026#32; (2021). \u0026#32;Deformable DETR: Deformable Transformers for End-to-End Object Detection.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2010.04159 ) ï¼Œä½œè€…æå‡ºè®© decoder é¢„æµ‹ä¸€ä¸ª reference point çš„ offsetï¼Œä»¥åŠ bbox çš„å®½é«˜ã€‚\n $$ \\mathbf{b}=\\operatorname{sigmoid}\\left(\\operatorname{FFN}(\\mathbf{f})+\\left[\\mathbf{s}^{\\top} 000\\right]^{\\top}\\right) $$  ä½œè€…å°è¯•äº†ä¸¤ç§ä¸åŒçš„è®¡ç®— reference point çš„æ–¹æ³•ï¼Œablation å¦‚ä¸‹ã€‚\n   Method AP     ä¸ç”¨ reference point, s=(0,0) 36.8   å°† s ä½œä¸ºç½‘ç»œå‚æ•°ï¼Œè¿›è¡Œä¼˜åŒ– 40.7   æ ¹æ®å¯¹åº”çš„ object query é¢„æµ‹è€Œæ¥ 40.9    Conditional Spatial Query #  ä½œè€…æå‡ºç”¨ Conditional Spatial Query æ›¿ä»£ object query ã€‚\nIntuition ä¸»è¦æ˜¯ï¼Œencoder çš„ feature é‡Œé¢åŒ…å«äº†æˆ‘ä»¬è¦é¢„æµ‹çš„ bbox çš„ä½ç½®å’Œç±»åˆ«ä¿¡æ¯ï¼Œæˆ‘ä»¬åªéœ€è¦é€šè¿‡ cross attention æŠŠå®ƒæœ‰æ•ˆçš„æå–å‡ºæ¥ã€‚ è¿™é‡Œçš„ cross attention å…¶å®å°±æ˜¯é€šè¿‡ decoder embedding å’Œ encoder embedding ä»¥åŠå¯¹åº”çš„ spatial embedding ç›´æ¥çš„ç›¸ä¼¼åº¦è¿›è¡Œçš„ã€‚ è¿™é‡Œçš„ç›¸ä¼¼åº¦ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ object query è¿™ä¸ª slot å’Œ encoder çš„ feature çš„ç›¸ä¼¼åº¦ã€‚\nåœ¨å¤šå±‚çš„ decoder layer é‡Œï¼Œdecoder embedding ä¸æ–­åœ¨å‘ç›®æ ‡æ¡†çš„ embedding å¯¹é½ï¼Œè¿™æ · cross attention å°±èƒ½å¾—åˆ°æ›´å‡†çš„ encoder featureã€‚\nä½†æ˜¯åŸå§‹çš„ Detr çš„ spatial embedding æ˜¯å•çº¯çš„ object queryï¼Œå’Œ encoder çš„ spatial embedding å¯èƒ½ä¸åœ¨ä¸€ä¸ª spaceï¼Œè¿™æ ·åšç›¸ä¼¼åº¦è®¡ç®—å¯èƒ½ä¸å‡†ã€‚ ä½œè€…ä¼¼ä¹æ²¡æ¯”ç”¨åŸå§‹ Detr çš„ object query åšspatial embeddingçš„ç»“æœã€‚\nä½œè€…æå‡ºä½¿ç”¨ reference point çš„ sin embedding åš spatial embeddingï¼Œ\n $$ \\mathbf{p}_{s}=\\operatorname{sinusoidal}(\\operatorname{sigmoid}(\\mathbf{s})) $$  ç„¶åè®¡ç®—ä¸€ä¸ª transformationï¼Œå°†å…¶å’Œ decoder embedding è”ç³»èµ·æ¥ï¼Œåšä¸€ä¸ª offsetï¼Œ\n $$ \\mathbf{p}_{q}=\\mathbf{T} \\mathbf{p}_{s}=\\boldsymbol{\\lambda}_{q} \\odot \\mathbf{p}_{s} $$  å…¶ä¸­ $\\mathbf{T}=\\operatorname{FFN}(\\mathbf{f})$, $\\boldsymbol{\\lambda}_{q}$ æ˜¯ä¸ªå¯¹è§’é˜µã€‚\nå®éªŒç»“æœ\n CSQ-P: ç›´æ¥ç”¨ sin è¿‡åçš„ positional embedding $p_s$ CSQ-T: ç›´æ¥ç”¨ transformation $\\boldsymbol{\\lambda}_{q}$ CSQ-C: ç”¨ decoder content embedding $f$ CSQ-I: ç”¨ f ç»è¿‡ self attention ä¹‹åçš„ $c_q$ é¢„æµ‹çš„ transformation ä¹˜ä¸Š $p_s$ çš„ç»“æœã€‚   ç†è®ºä¸Š Concate è¿™ç§åšæ³•ï¼Œå¯¹äºåŸå§‹ Detr ä¹Ÿæ˜¯å¯ä»¥ç›´æ¥ç”¨çš„ï¼Œreference point ä» object query å¾—åˆ°ï¼Œä½†æ˜¯ positional embedding ç›´æ¥å°±ç”¨ object queryã€‚ è¿™æ ·åšæ•ˆæœå¦‚ä½•ï¼Ÿè€Œä¸”ï¼Œæ—¢ç„¶ reference point æ˜¯ä» object query å¾—åˆ°çš„ï¼Œé‚£ä¹ˆ object query å®é™…ä¸Šå°±åŒ…å«äº† reference point çš„ä¿¡æ¯ã€‚  4. Anchor Detr #  Anchor Detr (Citation: Wang,\u0026#32;Zhang \u0026amp; al.,\u0026#32;2022Wang,\u0026#32; Y.,\u0026#32; Zhang,\u0026#32; X.,\u0026#32; Yang,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Sun,\u0026#32; J. \u0026#32; (2022). \u0026#32;Anchor DETR: Query Design for Transformer-Based Object Detection.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2109.07107 ) å’Œ Conditional Detr (Citation: Meng,\u0026#32;Chen \u0026amp; al.,\u0026#32;2021Meng,\u0026#32; D.,\u0026#32; Chen,\u0026#32; X.,\u0026#32; Fan,\u0026#32; Z.,\u0026#32; Zeng,\u0026#32; G.,\u0026#32; Li,\u0026#32; H.,\u0026#32; Yuan,\u0026#32; Y.,\u0026#32; Sun,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; J. \u0026#32; (2021). \u0026#32;Conditional DETR for Fast Training Convergence. https://doi.org/10.48550/arXiv.2108.06152 ) æ˜¯åŒæœŸå·¥ä½œï¼Œåè€…ç¨å¾®æ—©ä¸€ç‚¹ã€‚\nAnchor as Object Query #  å…¶å®å’Œ Conditional Detr å¾ˆåƒï¼Œéƒ½æ˜¯ reference point åš anchor å’Œ object queryï¼Œç„¶å decoder é¢„æµ‹ offsetã€‚\n Row-Column Decoupled Attention #  ä¸å¤ªé‡è¦ï¼Œæš‚æ—¶æ²¡å†™ã€‚\nå¯¹æ¯” #  å¯¹æ¯” Deformable Detr (Citation: Zhu,\u0026#32;Su \u0026amp; al.,\u0026#32;2021Zhu,\u0026#32; X.,\u0026#32; Su,\u0026#32; W.,\u0026#32; Lu,\u0026#32; L.,\u0026#32; Li,\u0026#32; B.,\u0026#32; Wang,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Dai,\u0026#32; J. \u0026#32; (2021). \u0026#32;Deformable DETR: Deformable Transformers for End-to-End Object Detection.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2010.04159 ) \nå¯¹æ¯” Conditional Detr (Citation: Meng,\u0026#32;Chen \u0026amp; al.,\u0026#32;2021Meng,\u0026#32; D.,\u0026#32; Chen,\u0026#32; X.,\u0026#32; Fan,\u0026#32; Z.,\u0026#32; Zeng,\u0026#32; G.,\u0026#32; Li,\u0026#32; H.,\u0026#32; Yuan,\u0026#32; Y.,\u0026#32; Sun,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; J. \u0026#32; (2021). \u0026#32;Conditional DETR for Fast Training Convergence. https://doi.org/10.48550/arXiv.2108.06152 ) \n éƒ½ç”¨ reference point è½¬ embedding ä½œä¸º object queryã€‚  Conditional Detr çš„ reference point æ˜¯ç”± object query é€šè¿‡ FFN å¾—åˆ°çš„ã€‚ Anchor Detr çš„ reference point æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼ˆå…¶å®æ˜¯ Conditional Detr çš„å¦ä¸€ç§æ–¹æ¡ˆï¼Œä»–ä»¬çš„å®éªŒç»“æœæ˜¯è¿™ç§ä¸ºä½ 0.2ï¼‰   Anchor é€šè¿‡ Pattern embedding å¯ä»¥ä¸€ä¸ª anchor point é¢„æµ‹å¤šä¸ªç‰©ä½“ã€‚ è½¬ Position embedding çš„æ–¹å¼ä¸ä¸€æ ·ï¼šconditional detr ç”¨çš„æ˜¯ sinï¼Œanchor detr é¢å¤–åŠ äº†ä¸€ä¸ª FFNã€‚  5. DAB Detr #  DAB Detr (Citation: Liu,\u0026#32;Li \u0026amp; al.,\u0026#32;2022Liu,\u0026#32; S.,\u0026#32; Li,\u0026#32; F.,\u0026#32; Zhang,\u0026#32; H.,\u0026#32; Yang,\u0026#32; X.,\u0026#32; Qi,\u0026#32; X.,\u0026#32; Su,\u0026#32; H.,\u0026#32; Zhu,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Zhang,\u0026#32; L. \u0026#32; (2022). \u0026#32;DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2201.12329 ) , å‰é¢çš„å·¥ä½œéƒ½æ˜¯ç”¨ anchor point åš object queryï¼ŒDAB Detr ä¹Ÿ follow è¿™ä¸ªåšæ³•ï¼Œç„¶åé€šè¿‡ä¸€ä¸ª modulated attention å¼•å…¥äº† Width å’Œ Heightã€‚æœ€å output embedding é¢„æµ‹çš„ä¸æ­¢æ˜¯ xï¼Œy çš„ offset è¿˜æœ‰ hï¼Œw çš„ offsetã€‚\n 6. DN Detr #  Denoising Detr (Citation: Li,\u0026#32;Zhang \u0026amp; al.,\u0026#32;2022Li,\u0026#32; F.,\u0026#32; Zhang,\u0026#32; H.,\u0026#32; Liu,\u0026#32; S.,\u0026#32; Guo,\u0026#32; J.,\u0026#32; Ni,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Zhang,\u0026#32; L. \u0026#32; (2022). \u0026#32;DN-DETR: Accelerate DETR Training by Introducing Query DeNoising.\u0026#32;11. ) å¼•å…¥ä¸€ä¸ªé¢å¤–çš„ Denoising task ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œå…·ä½“çš„ï¼Œå°† GT object çš„ noisy bbox ä½œä¸º DAB çš„ object query è¾“å…¥ï¼Œç„¶åç›®æ ‡æ˜¯å»å™ªå›æ¥ã€‚\nå‚è€ƒæ–‡çŒ® #    Wang,\u0026#32; Zhang,\u0026#32; Yang\u0026#32;\u0026amp;\u0026#32;Sun (2022)  Wang,\u0026#32; Y.,\u0026#32; Zhang,\u0026#32; X.,\u0026#32; Yang,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Sun,\u0026#32; J. \u0026#32; (2022). \u0026#32;Anchor DETR: Query Design for Transformer-Based Object Detection.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2109.07107    Meng,\u0026#32; Chen,\u0026#32; Fan,\u0026#32; Zeng,\u0026#32; Li,\u0026#32; Yuan,\u0026#32; Sun\u0026#32;\u0026amp;\u0026#32;Wang (2021)  Meng,\u0026#32; D.,\u0026#32; Chen,\u0026#32; X.,\u0026#32; Fan,\u0026#32; Z.,\u0026#32; Zeng,\u0026#32; G.,\u0026#32; Li,\u0026#32; H.,\u0026#32; Yuan,\u0026#32; Y.,\u0026#32; Sun,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; J. \u0026#32; (2021). \u0026#32;Conditional DETR for Fast Training Convergence. https://doi.org/10.48550/arXiv.2108.06152    Liu,\u0026#32; Li,\u0026#32; Zhang,\u0026#32; Yang,\u0026#32; Qi,\u0026#32; Su,\u0026#32; Zhu\u0026#32;\u0026amp;\u0026#32;Zhang (2022)  Liu,\u0026#32; S.,\u0026#32; Li,\u0026#32; F.,\u0026#32; Zhang,\u0026#32; H.,\u0026#32; Yang,\u0026#32; X.,\u0026#32; Qi,\u0026#32; X.,\u0026#32; Su,\u0026#32; H.,\u0026#32; Zhu,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Zhang,\u0026#32; L. \u0026#32; (2022). \u0026#32;DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2201.12329    Zhu,\u0026#32; Su,\u0026#32; Lu,\u0026#32; Li,\u0026#32; Wang\u0026#32;\u0026amp;\u0026#32;Dai (2021)  Zhu,\u0026#32; X.,\u0026#32; Su,\u0026#32; W.,\u0026#32; Lu,\u0026#32; L.,\u0026#32; Li,\u0026#32; B.,\u0026#32; Wang,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Dai,\u0026#32; J. \u0026#32; (2021). \u0026#32;Deformable DETR: Deformable Transformers for End-to-End Object Detection.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2010.04159    Carion,\u0026#32; Massa,\u0026#32; Synnaeve,\u0026#32; Usunier,\u0026#32; Kirillov\u0026#32;\u0026amp;\u0026#32;Zagoruyko (2020)  Carion,\u0026#32; N.,\u0026#32; Massa,\u0026#32; F.,\u0026#32; Synnaeve,\u0026#32; G.,\u0026#32; Usunier,\u0026#32; N.,\u0026#32; Kirillov,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Zagoruyko,\u0026#32; S. \u0026#32; (2020). \u0026#32;End-to-End Object Detection with Transformers.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2005.12872    Li,\u0026#32; Zhang,\u0026#32; Liu,\u0026#32; Guo,\u0026#32; Ni\u0026#32;\u0026amp;\u0026#32;Zhang (2022)  Li,\u0026#32; F.,\u0026#32; Zhang,\u0026#32; H.,\u0026#32; Liu,\u0026#32; S.,\u0026#32; Guo,\u0026#32; J.,\u0026#32; Ni,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Zhang,\u0026#32; L. \u0026#32; (2022). \u0026#32;DN-DETR: Accelerate DETR Training by Introducing Query DeNoising.\u0026#32;11.       ç•¥å¤§äºä¸€å¼ å›¾é‡Œæœ€å¤šèƒ½æ£€æµ‹åˆ°çš„ç‰©ä½“æ•°\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  "},{"id":6,"href":"/blog/posts/ai/sd_cg/","title":"Steepest Descent \u0026\u0026 Conjugate Gradient","section":"Posts","content":"è¿™æ˜¯ä¸€ç¯‡ä¸»è¦ä»‹ç»Conjugate Gradient (CG)çš„ç¬”è®°ï¼Œå½“ç„¶ä¸ºäº†å¼•å…¥CGï¼Œä¹Ÿä¼šä¸€å¹¶ä»‹ç»å…¶â€œå‰èº«â€ Steepest Descentã€‚\næœ¬æ–‡ä¸»è¦å‚è€ƒè¿™ç¯‡è®ºæ–‡ï¼š An Introduction to the Conjugate Gradient Method Without the Agonizing Pain\n It is definitely the best paper to understand the steepest descent conjugate gradient method, which save me a lot of time, thanks god. And it is written in 1994, pretty cool, right? ğŸ§\n ç±»ä¼¼çš„çŸ¥ä¹ï¼š https://zhuanlan.zhihu.com/p/64227658\n Introduction #  Well, let\u0026rsquo;s begin with a short outline of this paper:\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£CGè¦æ±‚è§£çš„é—®é¢˜ï¼ŒCGæ˜¯ä¸€ä¸ªè¿­ä»£æ±‚è§£å¤§è§„æ¨¡çº¿æ€§æ–¹ç¨‹ç»„çš„æ–¹æ³•ï¼Œå³æˆ‘ä»¬è¦æ±‚:\n$$ Ax = b $$\nå…¶ä¸­xæ˜¯æˆ‘ä»¬è¦æ±‚çš„ä¸€ä¸ªå‘é‡ï¼Œbæ˜¯ä¸€ä¸ªå·²çŸ¥å‘é‡ï¼ŒAæ˜¯ä¸€ä¸ªå·²çŸ¥çš„ square, symmetric, positive-definite çš„çŸ©é˜µã€‚\nThe Quadratic Form #  äºŒæ¬¡å‹æ˜¯ä¸€ä¸ªæ ‡é‡äºŒæ¬¡å‡½æ•°ï¼Œå³è¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡xï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡ï¼Œå®ƒçš„å½¢å¼å¦‚ä¸‹ï¼š\n$$ f(x) = \\frac{1}{2}x^T A x - b^T x + c $$\nå¯¹$f(x)$æ±‚å¯¼ï¼Œæˆ‘ä»¬æœ‰ï¼š\n$$ f'(x) = \\frac{1}{2}A^Tx + \\frac{1}{2}Ax -b $$\nå½“Aæ˜¯å¯¹ç§°çŸ©é˜µæ—¶ï¼Œæˆ‘ä»¬æœ‰:\n$$ f'(x) = Ax - b $$\nä¸éš¾å¾—å‡ºï¼Œå½“Aæ˜¯å¯¹ç§°çŸ©é˜µæ—¶ï¼ŒåŸé—®é¢˜$Ax=b$çš„è§£å°±æ˜¯äºŒæ¬¡å‹å¯¼æ•°ä¸º0çš„ç‚¹ï¼Œä¹Ÿå³æå€¼ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚è§£äºŒæ¬¡å‹çš„æå€¼ç‚¹æ‰¾åˆ°$Ax=b$çš„è§£ã€‚\nè¿›ä¸€æ­¥çš„ï¼Œå¦‚æœAæ˜¯æ­£å®šï¼ˆpostive definiteï¼‰çŸ©é˜µï¼ŒäºŒæ¬¡å‹çš„æå€¼ç‚¹æ˜¯å®ƒçš„æœ€å°å€¼ç‚¹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å°åŒ– $f(x)$ æ‰¾åˆ° $Ax=b$ çš„è§£ã€‚\nå½“AçŸ©é˜µæ˜¯æ­£å®šçŸ©é˜µæ—¶ï¼ŒäºŒæ¬¡å‹æœ‰æœ€å°å€¼ï¼Œå¦‚aæ‰€ç¤ºï¼Œå¦‚æœæ˜¯è´Ÿå®šçŸ©é˜µï¼Œåˆ™å¦‚bæ‰€ç¤ºã€‚ åŸè®ºæ–‡ Figure 5ã€‚ Steepest Descent #  å›é¡¾ï¼Œæˆ‘ä»¬ç°åœ¨éœ€è¦åšçš„æ˜¯æ‰¾å‡ºäºŒæ¬¡å‹çš„æœ€å°å€¼ã€‚\né¦–å…ˆï¼ŒåŸºäºè¿­ä»£çš„æ±‚æœ€å°å€¼çš„æ–¹æ³•æ€è·¯éƒ½æ˜¯ä»ä¸€ä¸ªéšæœºè§£ $x_0$å¼€å§‹ï¼Œä¸æ–­æ›´æ–°ï¼Œå¾—åˆ° $x_1, x_2, \u0026hellip;$ï¼Œç›´åˆ°ä¸çœŸå®å€¼è¶³å¤Ÿæ¥è¿‘ã€‚ï¼ˆç”¨$r=Ax-b$åˆ¤æ–­ï¼‰\nSteepest Descent ä¹Ÿæ˜¯åŸºäºè¿­ä»£çš„æ–¹æ³•ï¼Œå®ƒæ˜¯æ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªæ”¹è¿›ï¼Œå³ä»åˆå§‹ç‚¹å‡ºå‘ï¼Œæ¯æ¬¡éƒ½æœè´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°ã€‚\n$$ -f'(x_i) = b - Ax_i $$\næˆ‘ä»¬å®šä¹‰ residual å’Œ error $$ r_i = b - Ax_i $$ $$ e_i = x_i -x $$\nåˆ™æ¯æ¬¡æ›´æ–°å…¬å¼ä¸ºï¼š\n$$ x_{i+1} = x_i + \\alpha r_i $$\nç°åœ¨ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬çŸ¥é“æ›´æ–°çš„æ–¹å‘ï¼Œä½†æˆ‘ä»¬è¦èµ°å¤šè¿œå‘¢ï¼Ÿå³ $\\alpha$ è¦å¦‚ä½•é€‰å–ã€‚\nSteepest Descentçš„æ€è·¯æ˜¯ï¼Œæ¯æ¬¡æ›´æ–°éƒ½å–ä»è´Ÿæ¢¯åº¦æ–¹å‘å‡ºå‘èƒ½å¤Ÿçš„åˆ°è¾¾çš„æœ€ä½ç‚¹ã€‚å¦‚ä¸‹å›¾a,bæ‰€ç¤ºã€‚\n ä»æ•°å­¦ä¸Šï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°ä¸€ä¸ª $\\alpha$ ä½¿å¾— $f(x_{i+1})$ æœ€å°ã€‚å›å¿†ï¼Œå› ä¸ºAæ˜¯æ­£å®šçš„ï¼Œæ‰€ä»¥ $f$ çš„æå€¼ç‚¹å°±æ˜¯æœ€å€¼ç‚¹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¼æ•° $\\frac{\\partial f(x_{i+1})}{ \\partial \\alpha}$ ä¸º0ï¼Œæ±‚å¾— $\\alpha$ã€‚\nåº”ç”¨å¤åˆå‡½æ•°æ±‚å¯¼ï¼š $$ \\frac{\\partial f(x_{i+1})}{ \\partial \\alpha} = \\frac{\\partial f(x_{i+1})}{ \\partial x_{i+1}} \\frac{\\partial x_{i+1}}{ \\partial \\alpha} $$\nå…¶ä¸­ $\\frac{\\partial f(x_{i+1})}{ \\partial x_{i+1}} = f'(x_{i+1}) = - r_{i+1}$ï¼Œ $ \\frac{\\partial x_{i+1}}{ \\partial \\alpha} = r_i$\nä»¤å¯¼æ•°ä¸º0ï¼Œæˆ‘ä»¬æœ‰ï¼ˆä¸ºäº†ç®€åŒ–è¡¨è¾¾å¼ï¼Œè¿™é‡Œç”¨1è¡¨ç¤ºi+1ï¼Œ0è¡¨ç¤ºiï¼‰ï¼š\n $$ \\begin{aligned} r_{(1)}^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A x_{(1)}\\right)^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A\\left(x_{(0)}+\\alpha r_{(0)}\\right)\\right)^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A x_{(0)}\\right)^{T} r_{(0)}-\\alpha\\left(A r_{(0)}\\right)^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A x_{(0)}\\right)^{T} r_{(0)} \u0026=\\alpha\\left(A r_{(0)}\\right)^{T} r_{(0)} \\\\ r_{(0)}^{T} r_{(0)} \u0026=\\alpha r_{(0)}^{T}\\left(A r_{(0)}\\right) \\\\ \\alpha \u0026=\\frac{r_{(0)}^{T} r_{(0)}}{r_{(0)}^{T} A r_{(0)}} . \\end{aligned} $$  äº‹å®ä¸Š Steepest Descent æ¯æ¬¡æ›´æ–°çš„æ–¹å‘éƒ½ä¸ä¸Šä¸€æ­¥çš„æ–¹å‘æ­£äº¤çš„ã€‚\nè¿™æ˜¯å› ä¸ºæ¯ä¸€æ­¥æ›´æ–°çš„æ—¶å€™éƒ½æ˜¯ä»è´Ÿæ¢¯åº¦æ–¹å‘èµ°åˆ°æœ€ä½ç‚¹ï¼Œå¦‚æœåœä¸‹æ¥çš„ä½ç½®æ¢¯åº¦ä¸ä¸æ›´æ–°æ¢¯åº¦æ­£äº¤ï¼Œé‚£ä¹ˆè¿™ä¸ªä½ç½®å°±ä¸æ˜¯æœ€ä½ç‚¹ï¼Œå› ä¸ºè¿™ä¸ªä½ç½®åœ¨æ›´æ–°æ¢¯åº¦ä¸Šè¿˜æœ‰ä¸€ä¸ªä¸ä¸º0çš„åˆ†é‡ã€‚\n  "},{"id":7,"href":"/blog/posts/algorithms/%E6%9C%80%E7%9F%AD%E8%B7%AF/","title":"æœ€çŸ­è·¯ç®—æ³•","section":"Posts","content":"Dijkstraç”¨äºæ±‚æ²¡æœ‰è´Ÿæƒè¾¹çš„å•æºæœ€çŸ­è·¯ã€‚\nDijkstra #  ä»æºç‚¹å‡ºå‘ï¼Œæ¯æ¬¡é€‰æ‹©ä¸è®¿é—®è¿‡çš„ç‚¹è·ç¦»æœ€è¿‘çš„ç‚¹ï¼Œæ›´æ–°è·ç¦»ã€‚è¿™ä¸ªè·ç¦»å°±æ˜¯ä»æºç‚¹åˆ°è¯¥ç‚¹çš„æœ€çŸ­è·ç¦»ã€‚\nWhyï¼Ÿ\nBellman-Ford #  SPFA # "},{"id":8,"href":"/blog/posts/algorithms/gcd/","title":"å¿«é€Ÿæ±‚æœ€å¤§å…¬çº¦æ•°gcd","section":"Posts","content":"å¿«é€Ÿæ±‚ä¸¤ä¸ªæ•°çš„æœ€å¤§å…¬çº¦æ•°(å…¬å› æ•°)æœ‰ä¸¤ä¸ªåŠæ³•ï¼š\n æ›´ç›¸å‡æŸæ³• è¾—è½¬ç›¸é™¤æ³•  æ›´ç›¸å‡æŸæ³• #  åŸç†ï¼šä¸¤ä¸ªæ­£æ•´æ•°aå’Œbï¼ˆa\u0026gt;bï¼‰ï¼Œå®ƒä»¬çš„æœ€å¤§å…¬çº¦æ•°ç­‰äºa-bï¼ˆå¤§å‡å°ï¼‰å’Œbï¼ˆå°ï¼‰çš„æœ€å¤§å…¬çº¦æ•°ã€‚\nint gcd(int a,int b) { if(a==b) return a; if(a\u0026gt;b) return gcd(a-b,b); if(a\u0026lt;b) return gcd(b-a,a); } è¾—è½¬ç›¸é™¤æ³• #  åŸç†ï¼šä¸¤ä¸ªæ­£æ•´æ•°aå’Œbï¼ˆa\u0026gt;bï¼‰ï¼Œå®ƒä»¬çš„æœ€å¤§å…¬çº¦æ•°ç­‰äºa%bï¼ˆå¤§æ¨¡å°ï¼‰å’Œbï¼ˆå°ï¼‰ä¹‹é—´çš„æœ€å¤§å…¬çº¦æ•°ã€‚\nwhyï¼Ÿ\n å‡è®¾æœ€å¤§å…¬çº¦æ•°æ˜¯x, æ˜¾ç„¶ï¼Œxä¹Ÿæ˜¯a%bå› æ•°ã€‚ ç°åœ¨è¯æ˜xæ˜¯a%bå’Œbçš„æœ€å¤§å…¬å› æ•°ï¼šå‡è®¾å­˜åœ¨æ›´å¤§çš„å…¬å› æ•°yï¼Œå› ä¸ºa=kb+a%b, åˆ™yè‚¯å®šä¹Ÿæ˜¯açš„å…¬å› æ•°ã€‚å› æ­¤x=yï¼Œå¾—è¯ã€‚  int gcd(int a,int b) { if(b==0) return a; else return gcd(b,a%b); } // ä¸€è¡Œå†™æ³• int gcd(int a,int b) { return b ? gcd(b,a%b):a; } æ¯”è¾ƒ #  æ›´ç›¸å‡æŸæ³•é¿å…äº†å¤§æ•´æ•°å–æ¨¡å¯¼è‡´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä½†æ˜¯è¿ç®—æ¬¡æ•°è¦æ¯”è¾—è½¬ç›¸é™¤å¤šå¾—å¤šã€‚\nReference #    https://www.cnblogs.com/fusiwei/p/11301436.html "},{"id":9,"href":"/blog/posts/ai/monte_carlo/","title":"è’™ç‰¹å¡æ´›æ³•","section":"Posts","content":"è’™ç‰¹å¡æ´›(MonteCarlo)æ˜¯ä¸€å¤§ç±»éšæœºç®—æ³•(RandomizedAlgorithms)çš„æ€»ç§°ï¼Œå®ƒä»¬é€šè¿‡éšæœºæ ·æœ¬æ¥ä¼°ç®—çœŸå®å€¼ã€‚\nä¼°è®¡åœ†å‘¨ç‡ #  å‡è®¾æœ‰ä¸€ä¸ªåŠå¾„ä¸º1ï¼Œåœ†å¿ƒåœ¨åŸç‚¹çš„åœ†ï¼Œæˆ‘ä»¬çŸ¥é“å®ƒçš„é¢ç§¯æ˜¯$\\pi$ã€‚ç°åœ¨æˆ‘ä»¬éšæœºåœ¨$x\\in[-1,1], y\\in[-1,1]$ä¸Šéšæœºå–ä¸€ä¸ªç‚¹ï¼Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªç‚¹è½åœ¨åœ†å†…çš„æ¦‚ç‡ä¸ºåœ†ä¸æ­£æ–¹å½¢é¢ç§¯ä¹‹æ¯”ï¼š\n$$ p = \\frac{\\pi}{4} $$\nå‡è®¾æˆ‘ä»¬éšæœºé€‰å–äº†nä¸ªç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å…¬å¼åˆ¤æ–­å“ªäº›ç‚¹ä½äºåœ†å†…ï¼Œå‡è®¾æœ‰mä¸ªã€‚\n$$ x^2 + y^2 \u0026lt; 1 $$\næ˜¾ç„¶ï¼Œæˆ‘ä»¬çŸ¥é“mçš„æ•°å­¦æœŸæœ›æ˜¯\n$$ E[M] = \\frac{\\pi n}{4} $$\nå½“nå¾ˆå¤§æ—¶, mè¿‘ä¼¼äºæ•°å­¦æœŸæœ›ï¼Œå› æ­¤æœ‰\n $$ \\begin{aligned} m \\approx \\frac{\\pi n}{4} \\\\ \\pi \\approx \\frac{4m}{n} \\end{aligned} $$  è¿‘ä¼¼æ±‚å®šç§¯åˆ† #  è’™ç‰¹å¡æ´›æ±‚ç§¯åˆ†çš„æœ¬è´¨æ˜¯åˆ©ç”¨éšæœºæ¨¡æ‹Ÿä¼°è®¡ä¸€ä¸ªéšæœºå˜é‡çš„æœŸæœ›ã€‚\nå‡è®¾æˆ‘ä»¬æƒ³æ±‚å®šç§¯åˆ†:\n$$ \\int_{a}^{b} f(x) \\mathrm{d} x $$\næˆ‘ä»¬å¯ä»¥æŠŠå®ƒè½¬æ¢æˆæŸä¸ªéšæœºå˜é‡çš„æ•°å­¦æœŸæœ›ï¼Œå…·ä½“å¦‚ä¸‹ï¼š\nè®¾éšæœºå˜é‡$X \\sim U[a, b]$ï¼Œå³æœä»å‡åŒ€åˆ†å¸ƒï¼ŒXå…·æœ‰æ¦‚ç‡å¯†åº¦$p(x)=\\frac{1}{b-a}$ï¼Œé‚£ä¹ˆå°±æœ‰:\n$$ E[f(X)] =\\int_{a}^{b} p(x)f(x) \\mathrm{d} x = \\frac{1}{b-a}\\int_{a}^{b} f(x) \\mathrm{d} x $$\nåœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“æœŸæœ›æ˜¯å¯ä»¥ä¼°è®¡çš„ï¼Œæˆ‘ä»¬éšæœºå–Nä¸ª$f(x)$çš„æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬çš„å¹³å‡å€¼å¯ä»¥ä½œä¸ºæ‰€æ±‚æœŸæœ›çš„è¿‘ä¼¼ï¼Œå³ï¼š\n$$ E[f(x)] = \\frac{\\sum_1^Nf(x_i)}{N} = \\frac{1}{b-a}\\int_{a}^{b} f(x) \\mathrm{d} x $$\né‚£ä¹ˆå°±æœ‰ï¼š\n$$ \\int_{a}^{b} f(x) \\mathrm{d} x = (b-a)\\frac{\\sum_1^Nf(x_i)}{N} $$\nå‚è€ƒèµ„æ–™ #    https://blog.csdn.net/weixin_41503009/article/details/107853383  https://github.com/wangshusen/DRL "},{"id":10,"href":"/blog/posts/ai/conv-fft/","title":"Convolution with Image Filter \u0026\u0026 Convolution with fft/ifft","section":"Posts","content":"æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‚…ç«‹å¶å˜æ¢å®ç°å·ç§¯ï¼Œå…·ä½“åšæ³•å¤§æ¦‚å°±æ˜¯å…ˆå¯¹æ•°æ®å’Œå·ç§¯æ ¸è¿›è¡Œå‚…ç«‹å¶å˜æ¢å°†æ•°æ®å˜æ¢åˆ°é¢‘åŸŸï¼Œç„¶åå·ç§¯å°±æ˜¯é¢‘åŸŸä¸Šçš„ä¹˜ç§¯, æœ€ååšé€†å‚…ç«‹å¶å˜æ¢è½¬åŒ–å›åŸæ¥çš„ç©ºåŸŸã€‚\nr = ifft(fft(x).*fft(h)) ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯å‚…ç«‹å¶å˜æ¢åšçš„å·ç§¯å¯¹åº”çš„æ˜¯Circular Convolutionã€‚\ng = conv2(f,h,\u0026#39;same\u0026#39;); g_fft2 = ifft2(fft2(circshift(f,[-1,-1])).*fft2(rot90(h,2),2,4)); ä½¿ç”¨å‚…ç«‹å¶å˜æ¢å¦‚æœè¦è·å¾—å’ŒCircular Convolutionä¸€æ¨¡ä¸€æ ·çš„ç»“æœï¼Œéœ€è¦å¯¹æ•°æ®åšä¸€ä¸ªshiftï¼Œshiftçš„å¤§å°å’Œå·ç§¯æ ¸å¤§å°æœ‰å…³ï¼ŒäºŒç»´æƒ…å†µå°±æ˜¯[-w/2, -h/2]ï¼›ç„¶åå·ç§¯æ ¸ä¹Ÿè¦åè½¬ä¸€ä¸‹ï¼Œå› ä¸ºconv2å·ç§¯æ˜¯å€’ç€çš„ã€‚\n"},{"id":11,"href":"/blog/posts/ai/vae-more/","title":"More about Variational Autoencoder","section":"Posts","content":"ä¸€äº›å…³äºVAEçš„æ‰©å±•çŸ¥è¯†ã€‚\nä¸æ–­æ›´æ–°ä¸­\u0026hellip;\nTraining Tips #   spectral regularization1 æœ‰åˆ©äºç¨³å®šVAEçš„è®­ç»ƒ2 è®­ç»ƒçš„æ—¶å€™å¯ä»¥ä½¿ç”¨KL cost annealing3çš„è®­ç»ƒç­–ç•¥ï¼ŒKLé¡¹çš„æƒé‡ä¸€å¼€å§‹ä¸º0ï¼Œåªå…³æ³¨é‡å»ºï¼Œç­‰é‡å»ºèƒ½åŠ›å·®ä¸å¤šäº†ï¼Œå†é€æ¸å¢åŠ KLé¡¹çš„æƒé‡åˆ°1ã€‚ é™åˆ¶KLé¡¹ä¸­logvar.exp()ï¼Œé˜²æ­¢å…¶å€¼è¿‡å¤§ã€‚4  å¼ºåˆ¶clip weightåˆå§‹åŒ–è¦å°, æˆ–ä½¿ç”¨$log(\\sigma^2)$5    Related Models \u0026amp;\u0026amp; Papers #  Beta-VAE #   beta-vae6å°±æ˜¯ç»™KLDåŠ äº†ä¸€ä¸ªæƒé‡betaã€‚\n $$ \\mathcal{F}(\\theta, \\phi, \\beta ; \\mathbf{x}, \\mathbf{z}) \\geq \\mathcal{L}(\\theta, \\phi ; \\mathbf{x}, \\mathbf{z}, \\beta)=\\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\beta D_{K L}\\left(q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right) $$  åŸæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªè¯„æµ‹æŒ‡æ ‡ç”¨äºè¯„æµ‹latent representation disentangleå¥½ä¸å¥½çš„æ–¹æ³•ï¼Œç®€å•æ¥è¯´æ˜¯è¿™æ ·çš„ï¼š\n å‰ææ˜¯ï¼šå‡è®¾æ•°æ®xæ˜¯æœ‰ä¸€ç³»åˆ—çš„disentangle factor yå¾—åˆ°çš„ï¼Œè€Œä¸”æˆ‘ä»¬æœ‰ä¸€ä¸ªground truth simulatorå¯ä»¥æ ¹æ®è¿™äº›factoråˆæˆå‡ºæ•°æ®xã€‚\n å…·ä½“æ–¹æ³•ï¼š\n éšæœºé€‰ä¸€ä¸ªfactor yã€‚ (Choose a factor y âˆ¼ Unif[1\u0026hellip;K]) å¯¹ä¸€ä¸ªbatché‡Œçš„Lä¸ªæ ·æœ¬ï¼ˆä¸€ä¸ªbatchä¸­æ¯ä¸ªsampleçš„factoréƒ½ä¸€æ ·ï¼‰ï¼š  sample ä¸¤ä¸ªlatent representationï¼ˆäººå·¥è®¾è®¡çš„ï¼‰ï¼Œè¿™ä¸¤ä¸ªæ ·æœ¬ï¼Œåœ¨åˆšåˆšé€‰çš„é‚£ä¸ªfactorä¸Šç›¸åŒï¼Œå…¶ä»–éšæœºã€‚ ç”¨simulatoråˆæˆä¸¤å¼ å›¾åƒï¼Œç”¨ä¹‹å‰è®­ç»ƒå¥½çš„vae-encoderé¢„æµ‹ä¸€ä¸ªlatent representaitonï¼ˆç½‘ç»œå­¦åˆ°çš„ï¼‰ è®¡ç®—ä¸¤å¼ å›¾åƒçš„latent representationçš„å·®å€¼ã€‚   ä½¿ç”¨Lä¸ªsampleå·®å€¼çš„å¹³å‡æ•°ä½œä¸ºä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œé¢„æµ‹åˆšåˆšé€‰çš„factoræ˜¯å“ªä¸ªã€‚ç”¨é¢„æµ‹çš„å‡†ç¡®æ€§ä½œä¸ºè¯„æµ‹æŒ‡æ ‡ã€‚  Intuitionï¼š\n åˆ†ç±»å™¨ä¼šè¢«å¼ºè¡Œè®¾è®¡çš„åªæœ‰çº¿æ€§åˆ†ç±»èƒ½åŠ›ã€‚ å¦‚æœvaeå­¦ä¹ åˆ°çš„latent representationå¤Ÿå¥½ï¼Œé‚£ç†è®ºä¸Šå®ƒåº”è¯¥æœ‰ä¸€ç»´å°±æ˜¯ä»£è¡¨factor yï¼Œ åœ¨è¿™ä¸ªç»´åº¦ï¼Œæ¯ä¸ªæ ·æœ¬çš„ä¸¤å¼ å›¾åƒçš„å€¼åº”è¯¥æ˜¯å¾ˆæ¥è¿‘çš„ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒä»¬çš„å·®å€¼ä¼šå¾ˆæ¥è¿‘0ï¼Œé‚£ä¹ˆåˆ†ç±»å™¨åªéœ€è¦æ‰¾åˆ°å¾ˆæ¥è¿‘0çš„é‚£ä¸€é¡¹å°±å¯ä»¥é¢„æµ‹å‡ºåˆšåˆšé€‰çš„factoræ˜¯å“ªä¸ªäº†ã€‚  Extended beta VAE #   åœ¨Understanding disentangling in Î² -VAE7ä¸­ï¼Œä½œè€…è¿›ä¸€æ­¥æ‰©å±•äº†beta VAEï¼Œ æ–¹æ³•å¦‚ä¸‹ï¼š\n $$ \\mathcal{L}(\\theta, \\phi ; \\mathbf{x}, \\mathbf{z}, C)=\\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\gamma\\left|D_{K L}\\left(q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right)-C\\right| $$  å…¶ä¸­Cæ˜¯ä¸€ä¸ªé€æ¸å¢å¤§çš„å¸¸æ•°ã€‚\nIntuitionæ˜¯ï¼Œå½“Cé€æ¸å¢å¤§æ—¶ï¼ŒKLDçš„ä½œç”¨é€æ¸å˜å°ï¼Œé‡æ„è¯¯å·®é€æ¸å ä¸»å¯¼åœ°ä½ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å­¦ä¹ åˆ°æ›´å¥½çš„é‡æ„å›¾åƒã€‚è¿™ç‚¹ä¸KLD annealingæ­£å¥½æ˜¯åè¿‡æ¥çš„ã€‚\nSpectral Regularization #   ç®€å•æ¥è¯´ï¼šspectral regularization1ç”¨äºé™ä½ sensitivity to perturbationï¼ˆæµ‹è¯•æ•°æ®å¾®å°çš„å˜åŠ¨ä¼šå¼•èµ·ç»“æœå¾ˆå¤§çš„å˜åŒ–ï¼‰ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ä¸ªæ­£åˆ™åŒ–ï¼Œçº¦æŸç½‘ç»œæƒé‡çŸ©é˜µçš„è°±èŒƒæ•°ï¼Œä¸è®©å…¶è¿‡å¤§ã€‚\n åŸæ–‡ï¼šTo reduce the sensitivity to perturbation, we propose a simple and effective regularization method, referred to as spectral norm regularization, which penalizes the high spectral norm of weight matrices in neural networks.\n è®¾$f$æ˜¯æˆ‘ä»¬è¦å­¦ä¹ çš„å‡½æ•°ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¦è®©perturbationï¼Œå³ $f(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})$ å°½é‡å°ã€‚é€šå¸¸$f$ éƒ½æ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°åŠ ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚è€ƒè™‘åˆ°æ·±åº¦å­¦ä¹ å¸¸ç”¨ReLUç­‰piecewise linear function[?]ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚å½“$\\boldsymbol{\\xi}$å¾ˆå°çš„ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ$f$çœ‹å‡ºä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚å› æ­¤æˆ‘ä»¬æœ‰ï¼š $$ \\frac{\\left|f_{\\Theta}(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})\\right|{2}}{|\\boldsymbol{\\xi}|{2}}=\\frac{\\left|\\left(W_{\\Theta, \\boldsymbol{x}}(\\boldsymbol{x}+\\boldsymbol{\\xi})+\\boldsymbol{b}{\\Theta, \\boldsymbol{x}}\\right)-\\left(W{\\Theta, \\boldsymbol{x}} \\boldsymbol{x}+\\boldsymbol{b}{\\Theta, \\boldsymbol{x}}\\right)\\right|{2}}{|\\boldsymbol{\\xi}|{2}}=\\frac{\\left|W{\\Theta, \\boldsymbol{x}} \\boldsymbol{\\xi}\\right|{2}}{|\\boldsymbol{\\xi}|{2}} \\leq \\sigma\\left(W_{\\Theta, \\boldsymbol{x}}\\right), $$ å…¶ä¸­$\\boldsymbol{\\xi}$æ˜¯ä¸ªå¸¸æ•°ï¼Œå› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¦è®©$f(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})$ å°½é‡å°ï¼Œæˆ‘ä»¬åº”è¯¥è®©ä¸Šç•Œ$\\sigma\\left(W_{\\Theta, \\boldsymbol{x}}\\right)$å°½é‡å°ã€‚\nå³ï¼šæˆ‘ä»¬åº”å½“çº¦æŸç½‘ç»œæƒé‡çŸ©é˜µçš„è°±èŒƒæ•°ï¼Œä¸è®©å…¶è¿‡å¤§ã€‚\nLadder Variational Autoencoders #   Ladder Variational Autoencoders8\n æå‡ºäº†ä¸€ä¸ªç±»ä¼¼Ladder Networkçš„Ladder Variational Autoencodersã€‚  å…·ä½“ç»“æ„ï¼šTODO\næŒ‡å‡ºBatch Normalå’ŒKL Warm Upå¯¹è®­ç»ƒå¾ˆé‡è¦ã€‚  Re-balancing Variational Autoencoder Loss #   è¿™ç¯‡æ–‡ç« 9åˆ†æäº†RNN-based VAEä¸­posterior collapseé—®é¢˜å‡ºç°çš„åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªlosså‡è½»è¿™ä¸ªé—®é¢˜ã€‚\nposterior collapseï¼šä»»åŠ¡æ˜¯åœ¨RNN-VAEä¸­ï¼Œå› ä¸ºdecoderè®­ç»ƒçš„æ—¶å€™æœ‰ground truthä½œä¸ºteaching forceï¼Œå› æ­¤å½“latent representationå¾ˆå·®çš„æ—¶å€™ï¼Œdecoderä¼šå¿½è§†æ‰latent representationï¼Œå¯¼è‡´latent representationå’Œå…ˆéªŒå¾ˆæ¥è¿‘ï¼ŒKLDè¿™é¡¹å¾ˆå°ï¼Œè™½ç„¶losså¯èƒ½ä¸é«˜ï¼Œreconstructå› ä¸ºæœ‰teaching forceï¼Œlossä¸ä¼šå¤ªé«˜ï¼Œä½†æ˜¯æ€»ä½“æ•ˆæœæ˜¯å¾ˆå·®çš„ã€‚\nåˆ†æåŸå› ï¼Œå°±æ˜¯reconstruct lossè¢«ä½ä¼°äº†ï¼Œå› æ­¤éœ€è¦ç”¨ç³»æ•°è°ƒæ•´ï¼Œæ€è·¯å’Œbeta-vaeæ˜¯ä¸€æ ·çš„ã€‚\n $$ \\begin{aligned} \\mathcal{L}(x ; \\theta, \\phi)=\u0026 \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\\\ \u0026-D_{\\mathrm{KL}}\\left(q_{\\phi}(z \\mid x) \\| p(z)\\right), \\alpha1 \\end{aligned} $$  ä¹Ÿå¯ä»¥æ”¹æˆbeta-vaeçš„å½¢å¼ï¼Œä¸è¿‡è¿™é‡Œbetaæ˜¯åœ¨0-1åŒºé—´ï¼Œè€Œbeta-vaeæ˜¯\u0026gt;1ï¼Œå› ä¸ºç›®æ ‡ä¸ä¸€æ ·ã€‚è¿™é‡Œæ˜¯å‘å¢å¤§reconstructionçš„å æ¯” - ç­‰ä»·äºç¼©å°KLDã€‚\n $$ \\begin{aligned} \\mathcal{L}(x ; \\theta, \\phi)=\u0026 \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\\\ \u0026-\\beta D_{\\mathrm{KL}}\\left(q_{\\phi}(z \\mid x) \\| p(z)\\right), 0 \\leq \\betaCVAE #   Learning Structured Output Representation using Deep Conditional Generative Models10è¿™ç¯‡è®ºæ–‡æå‡ºäº†CVAEã€‚\nä¼ ç»ŸVAEä¼°è®¡çš„æ˜¯è¾¹ç¼˜æ¦‚ç‡$P(X)$ï¼Œ è€ŒCVAEä¼°è®¡çš„æ˜¯æ¡ä»¶æ¦‚ç‡$P(X|Y)$ã€‚ä¾‹å¦‚åœ¨å›¾åƒä¸­ï¼ŒXå°±æ˜¯å›¾åƒï¼ŒYæ˜¯å›¾åƒçš„ç±»åˆ«ï¼ŒCVAEä¸ä»…å¯ä»¥ç”Ÿæˆå›¾åƒï¼Œè¿˜å¯ä»¥æŒ‡å®šç”Ÿæˆå“ªç§ç±»å‹çš„å›¾åƒã€‚\nä¼ ç»ŸVAEçš„ELBOæ˜¯ï¼š $$ \\log P(X)-D_{K L}[Q(z \\mid X) | P(z \\mid X)]=E[\\log P(X \\mid z)]-D_{K L}[Q(z \\mid X) | P(z)] $$ åŠ å…¥æ¡ä»¶æ¦‚ç‡ï¼Œç›´è§‚åœ°ï¼Œç›¸å½“äºæˆ‘ä»¬è®©encoderåŒæ—¶ä¾èµ–äºXå’ŒY - $Q(z|x,y)$ï¼Œè®©decoderä¾èµ–äºZå’ŒY - $P(x|z,y)$ï¼Œé‚£æˆ‘ä»¬çš„ELBOå°±å˜æˆäº†: $$ \\log P(X \\mid c)-D_{K L}[Q(z \\mid X, c) | P(z \\mid X, c)]=E[\\log P(X \\mid z, c)]-D_{K L}[Q(z \\mid X, c) | P(z \\mid c)] $$ è¯¦ç»†çš„æ¨å¯¼ï¼š\n TODO  Conditional Variational Image Deraining #   è¿™ç¯‡æ–‡ç« 11æ˜¯CVAEçš„ä¸€ä¸ªåº”ç”¨ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯æŠŠä¸‹é›¨æ—¶å€™ç…§çš„ç…§ç‰‡ä¸­çš„é›¨ç»™å»æ‰ã€‚\nè¦ç†è§£å®ƒçš„åšæ³•ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°ç†è§£ä¸‹CVAEç©¶ç«Ÿåœ¨åšä»€ä¹ˆã€‚\n ä¼ ç»Ÿçš„VAEéƒ½æ˜¯è¾“å…¥ä¸€å¼ å›¾åƒXï¼Œç„¶åæˆ‘ä»¬ç”¨encoderå­¦å®ƒçš„latent representationï¼Œç„¶åæˆ‘ä»¬å†sampleï¼Œç»è¿‡decoderç”Ÿæˆå›¾åƒã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦ç”ŸæˆæŒ‡å®šç±»åˆ«çš„å›¾åƒï¼Œè¿™ç§æ–¹æ³•æ˜¯åšä¸åˆ°çš„ã€‚\né‚£ä¹ˆä¸€ä¸ªå¾ˆç›´æ¥çš„æ€è·¯æ˜¯ï¼Œæˆ‘ä»¬å¯ä¸å¯ä»¥è¾“å…¥ä¸€ä¸ªç±»åˆ«ï¼Œè®©encoderå­¦ä¹ è¾“å‡ºè¿™ä¸ªç±»åˆ«çš„latent representationï¼Œç„¶åæˆ‘ä»¬å†é€šè¿‡åŒæ ·çš„è¿‡ç¨‹ç”Ÿæˆç‰¹å®šç±»åˆ«çš„å›¾åƒï¼Ÿç­”æ¡ˆæ˜¯å¾ˆéš¾ã€‚\n CVAEçš„åšæ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬ç»™encoderä¸€ä¸ªé¢å¤–ä¿¡æ¯ï¼Œæˆ‘ä»¬å‘Šè¯‰å®ƒæŸä¸ªç±»åˆ«çš„å›¾åƒæ˜¯é•¿ä»€ä¹ˆæ ·çš„ï¼Œè¿™æ ·å®ƒåœ¨å­¦ä¹ çš„æ—¶å€™ä¼šæ›´æœ‰æ•ˆã€‚åœ¨è¿™ç§è§’åº¦ä¸‹ï¼ŒCVAEå…¶å®æ˜¯å¯¹ä¼ ç»ŸVAEçš„ä¸€ä¸ªå°ä¼˜åŒ–ã€‚\nåœ¨è¿™ç¯‡å»é›¨è®ºæ–‡ä¸­ï¼Œå®ƒçš„æ€è·¯æ˜¯ç±»ä¼¼çš„ï¼š\n æˆ‘ä»¬æŠŠä¸‹é›¨ç…§ç‰‡å½“ä½œâ€œç±»åˆ«â€ï¼Œæ¯ä¸€å¼ ä¸‹é›¨ç…§ç‰‡éƒ½æ˜¯ä¸€ä¸ªç±»åˆ« æˆ‘ä»¬å¸Œæœ›å­¦ä¹ æ¯ä¸ªç±»åˆ«çš„latent representationï¼Œå³æ¯å¼ ä¸‹é›¨ç…§ç‰‡ï¼Œå¹²å‡€ç‰ˆæœ¬çš„latent representationã€‚ ç„¶åæˆ‘ä»¬é€šè¿‡å¤šæ¬¡sampleï¼Œdecodeå‡ºå¤šå¼ å¹²å‡€ç…§ç‰‡ï¼Œå–ä¸ªå¹³å‡ä½œä¸ºæœ€ç»ˆç»“æœã€‚ åªè¾“å…¥ä¸‹é›¨ç…§ç‰‡ä¸å¥½è®­ç»ƒencoderï¼Œå› æ­¤æˆ‘ä»¬åŠ å…¥ä¸€ä¸ªconditionï¼Œè®­ç»ƒçš„æ—¶å€™æˆ‘ä»¬æŠŠå¹²å‡€ç…§ç‰‡ä¹ŸåŠ è¿›å»ï¼Œå¸®åŠ©encoderè¿›è¡Œè®­ç»ƒã€‚ ä½†æ˜¯é—®é¢˜æ˜¯é¢„æµ‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬æ²¡æœ‰å¹²å‡€ç…§ç‰‡ä½œä¸ºencoderçš„é¢å¤–è¾“å…¥ï¼Œè¿™æ—¶å€™encoderå°±æ²¡æ³•ç”¨äº†ï¼Œä¸ºæ­¤ä½œè€…å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„prior networkå»æ¨¡ä»¿encoderçš„encodeè¿‡ç¨‹ï¼Œä½†æ˜¯åªç”¨rainy imageä½œä¸ºè¾“å…¥ã€‚   é¢„æµ‹çš„æ—¶å€™ä½¿ç”¨çš„æ˜¯prior networkã€‚\n NVAE #   è‹±ä¼Ÿè¾¾2020çš„ä¸€ä¸ªå·¥ä½œ2ï¼Œä¸»è¦è¯´ä¸¤ç‚¹ï¼š\n æå‡ºäº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„VAEç½‘ç»œï¼Œå¹¶æŒ‡å‡ºVAEå¯ä»¥åœ¨ç½‘ç»œè®¾è®¡ä¸Šå¤šä¸‹ç‚¹åŠŸå¤«ã€‚ æå‡ºäº†å¤šä¸ªç¨³å®šKLDçš„æ–¹æ³•ï¼š  Residual Normal Distributions: Spectral Regularization (SR) More Expressive Approximate Posteriors with Normalizing Flows    å…·ä½“è¿˜æ²¡ç»†çœ‹ã€‚\nResources #    Denoising Criterion for Variational Autoencoding Framework  Some useful tricks in training variational autoencoder  https://github.com/loliverhennigh/Variational-autoencoder-tricks-and-tips/blob/master/README.md  Lossè®¾è®¡ #     Balancing Reconstruction vs KL Loss Variational Autoencoder\n   Why don\u0026rsquo;t we use MSE as a reconstruction loss for VAE ? #399\n   how to weight KLD loss vs reconstruction loss in variational auto-encoder\n  Github #  è¿™é‡Œæ˜¯ä¸€äº›Githubä¸Šä½¿ç”¨VAEçš„é¡¹ç›®ä»£ç ï¼š\n  AntixK/PyTorch-VAE  NVlabs/NVAE  Reference #    Yoshida å’Œ Miyato - 2017 -Spectral Norm Regularization for Improving the Generalizability of Deep Learning\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Vahdat å’Œ Kautz - 2020 - NVAE A Deep Hierarchical Variational Autoencoder\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Bowman ç­‰ã€‚ - 2016 - Generating Sentences from a Continuous Space\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  https://discuss.pytorch.org/t/kld-loss-goes-nan-during-vae-training/42305\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  https://github.com/y0ast/VAE-Torch/issues/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Higgins ç­‰ã€‚ - 2017 - Î²-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Burgess ç­‰ã€‚ - 2018 - Understanding disentangling in $\\beta$-VAE\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n SÃ¸nderby ç­‰ã€‚ - 2016 - Ladder Variational Autoencoders\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Yan ç­‰ã€‚ - 2020 - Re-balancing Variational Autoencoder Loss for Molecule Sequence Generation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Sohn ç­‰ã€‚ - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Du ç­‰ã€‚ - 2020 - Conditional Variational Image Deraining\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  "},{"id":12,"href":"/blog/posts/misc/hugo-book/","title":"Hugo-Book Shortcodes Usages","section":"Posts","content":"Hugo-Book Shortcodes Usages\n å¦‚æœå…¬å¼é‡Œå‡ºç°çŸ©é˜µï¼Œéœ€è¦ç”¨divå°†$$åŒ…æ‹¬èµ·æ¥ï¼Œå¦åˆ™æ— æ³•æ­£å¸¸æ˜¾ã€‚è§ Link å¦‚æœå…¬å¼éœ€è¦æ¢è¡Œï¼Œéœ€è¦ç”¨å…­ä¸ªæ–œæ \\\\\\ï¼Œè§ Link   Buttons #  {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}}{{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Get Home  Contribute  Columns #  Columns help organize shorter pieces of content horizontally for readability.\n{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example #  Left Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.  Mid Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!  Right Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.   Details #  Details shortcode is a helper for details html5 element. It is going to replace expand shortcode.\nExample #  {{\u0026lt; details \u0026#34;Title\u0026#34; [open] \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}}{{\u0026lt; details title=\u0026#34;Title\u0026#34; open=true \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}}Title Markdown content #  Lorem markdownum insigne\u0026hellip;   Expand #  Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nExample #  Default #  {{\u0026lt; expand \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}  å±•å¼€ â†•  Markdown content Lorem markdownum insigne\u0026hellip;    With Custom Label #  {{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}  Custom Label ...  Markdown content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.    Hints #  Hint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{\u0026lt; hint [info|warning|danger] \u0026gt;}}**Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}}Example #  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Mermaid Chart #   Mermaid is library for generating svg charts and diagrams from text.\nExample #  {{\u0026lt; mermaid [class=\u0026#34;text-center\u0026#34;]\u0026gt;}}sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}}   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) sequenceDiagram Alice-Bob: Hello Bob, how are you? alt is sick Bob-Alice: Not so good :( else is well Bob-Alice: Feeling fresh like a daisy end opt Extra response Bob-Alice: Thanks for asking end   Tabs #  Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}}{{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}}# MacOS Content {{\u0026lt; /tab \u0026gt;}}{{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}}# Linux Content {{\u0026lt; /tab \u0026gt;}}{{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}}# Windows Content {{\u0026lt; /tab \u0026gt;}}{{\u0026lt; /tabs \u0026gt;}}Example #  MacOS MacOS #  This is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux Linux #  This is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows Windows #  This is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":13,"href":"/blog/posts/algorithms/AC%E8%87%AA%E5%8A%A8%E6%9C%BA/","title":"ACè‡ªåŠ¨æœº","section":"Posts","content":"ACè‡ªåŠ¨æœºç”¨äºè§£å†³ä¸‹è¿°é—®é¢˜åŠå…¶åŒç±»é—®é¢˜:\n ç»™å®šä¸€ç³»åˆ—æ¨¡å¼ä¸²å’Œä¸€ä¸ªæ–‡æœ¬ä¸²ï¼Œåˆ¤æ–­æœ‰å¤šå°‘æ¨¡å¼ä¸²å‡ºç°åœ¨æ–‡æœ¬ä¸²ä¸­ï¼Œç»™å‡ºæ•°ç›®å’Œå¯¹åº”æ¨¡å¼ä¸²çš„å‡ºç°ä½ç½®ã€‚\n ä¸»è¦æ€æƒ³ #  ACè‡ªåŠ¨æœºä¸»è¦ç”¨åˆ°äº† Trieæ ‘å’Œ KMPç®—æ³•çš„æ€æƒ³ã€‚\næ„é€  #  æˆ‘ä»¬é¦–å…ˆå°†æ‰€æœ‰æ¨¡å¼ä¸²æ„å»ºæˆä¸€ä¸ªTrieæ ‘ï¼Œæ„å»ºæ–¹æ³•ä¸æ™®é€šTrieæ ‘çš„æ„å»ºæ–¹æ³•æ²¡æœ‰åŒºåˆ«ã€‚\nACè‡ªåŠ¨æœºçš„æ ‘ç»“æ„ä¸»è¦æ˜¯æ¯”Trieæ ‘å¤šä¸€ä¸ªfailæŒ‡é’ˆï¼ˆç±»ä¼¼äºKMPç®—æ³•çš„nextæ•°ç»„ï¼‰ã€‚\nclass AcNode { public: AcNode* m_childs[26] = { nullptr }; bool m_is_end; // æ˜¯å¦æ˜¯å•è¯è¾¹ç•Œ  char m_c; // æ–°å¢å†…å®¹  AcNode* fail = nullptr; }; è¯¥failæŒ‡é’ˆæ˜¯ACè‡ªåŠ¨æœºçš„æ ¸å¿ƒï¼Œå®ƒè¡¨ç¤ºå½“å‰æ ‘èŠ‚ç‚¹åŒ¹é…å¤±è´¥ï¼Œä¸‹ä¸€ä¸ªè¦å°è¯•åŒ¹é…çš„æ ‘èŠ‚ç‚¹ã€‚å®é™…ä¸Šï¼ŒfailæŒ‡é’ˆæ‰€æŒ‡çš„æ ‘èŠ‚ç‚¹æ‰€æ„æˆçš„å­—ç¬¦ä¸²æ˜¯å½“å‰æ ‘èŠ‚ç‚¹æ„æˆå­—ç¬¦ä¸²çš„åç¼€ã€‚\nFailæŒ‡é’ˆçš„æ±‚æ³•\nrootçš„å­©å­çš„failæŒ‡é’ˆæ˜¯root, å¯¹äºå…¶ä»–èŠ‚ç‚¹ï¼Œå…¶failæŒ‡é’ˆæ˜¯çˆ¶äº²çš„failæŒ‡é’ˆæ‰€æŒ‡çš„æ ‘èŠ‚ç‚¹çš„å­©å­ä¸­å’Œå½“å‰èŠ‚ç‚¹å­—æ¯ç›¸åŒçš„èŠ‚ç‚¹ã€‚å¦‚æœä¸å­˜åœ¨è¿™æ ·çš„èŠ‚ç‚¹ï¼Œåˆ™å°è¯•å¯»æ‰¾failæŒ‡é’ˆæ‰€æŒ‡æ ‘èŠ‚ç‚¹çš„failæŒ‡é’ˆæ‰€æŒ‡çš„èŠ‚ç‚¹æœ‰æ— è¿™æ ·çš„èŠ‚ç‚¹ï¼Œç›´åˆ°rootèŠ‚ç‚¹ã€‚\næ¯”è¾ƒç»•å£ï¼Œä¸¾ä¸ªä¾‹å­ï¼š\n{% include image.html url=\u0026quot;/assets/img/ac_automaton1.png\u0026quot; description=\u0026ldquo;å›¾1. FailæŒ‡é’ˆæ±‚è§£\u0026rdquo; size=\u0026ldquo;30%\u0026rdquo; %}\nå¦‚å›¾1æ‰€ç¤ºï¼Œh2çš„failæŒ‡é’ˆæ˜¯h1ã€‚æ±‚è§£è¿‡ç¨‹æ˜¯h2 -\u0026gt; s2 -\u0026gt; s2.fail = s1 -\u0026gt; h1ã€‚\nåŒ¹é… #  åŒ¹é…çš„æ—¶å€™ä¹Ÿå¾ˆç®€å•ï¼Œå¯¹äºæ–‡æœ¬ä¸²ä¸­çš„æ¯ä¸ªå­—æ¯ï¼Œä»ACè‡ªåŠ¨æœºçš„rootå¼€å§‹å°è¯•åŒ¹é…ã€‚\n åŒ¹é…æˆåŠŸï¼Œåˆ™è·³è‡³å…¶failæŒ‡é’ˆï¼Œç»§ç»­å°è¯•åŒ¹é…ï¼Œç›´åˆ°è·³å›rootæŒ‡é’ˆ - å¤„ç†ç±»ä¼¼sheåŒæ—¶åŒ…å«sheå’Œheçš„æƒ…å†µã€‚ åŒ¹é…å¤±è´¥ï¼Œä¹Ÿè¦è·³åˆ°å…¶failæŒ‡é’ˆç»§ç»­å°è¯•åŒ¹é…ã€‚  æ ¹æ®éœ€è¦è®°å½•åŒ¹é…æˆåŠŸçš„æ•°ç›®ï¼Œä½ç½®ä»¥åŠå¯¹åº”çš„æ¨¡å¼ä¸²ã€‚\nC++å®ç° #  å…·ä½“å®ç°å‚è§ ac_automaton.cppï¼Œè¿™é‡Œè§£é‡Šä¸€ä¸‹æ±‚failæŒ‡é’ˆçš„buildå‡½æ•°ã€‚\nvoid build() { queue\u0026lt;AcNode*\u0026gt; q; for(int i=0; i\u0026lt;26; i++) { if(root-\u0026gt;m_childs[i] != NULL) { root-\u0026gt;m_childs[i]-\u0026gt;fail = root; q.push(root-\u0026gt;m_childs[i]); } else { root-\u0026gt;m_childs[i] = root; } } AcNode* current; while(!q.empty()) { current = q.front(); q.pop(); for(int i=0; i\u0026lt;26;i ++) { if(current-\u0026gt;m_childs[i] != NULL) { current-\u0026gt;m_childs[i]-\u0026gt;fail = current-\u0026gt;fail-\u0026gt;m_childs[i]; q.push(current-\u0026gt;m_childs[i]); } else { current-\u0026gt;m_childs[i] = current-\u0026gt;fail-\u0026gt;m_childs[i]; // è·¯å¾„å‹ç¼©  } } } } è¦ç‚¹:\n ä½¿ç”¨BFSé€å±‚çš„æ±‚è§£failæŒ‡é’ˆã€‚ ä¸å­˜çˆ¶äº²æŒ‡é’ˆï¼Œéå†åˆ°å½“å‰èŠ‚ç‚¹æ—¶ï¼Œè®¡ç®—å­èŠ‚ç‚¹çš„failæŒ‡é’ˆã€‚ æ³¨æ„æŠŠrootæŒ‡é’ˆçš„æ‰€æœ‰â€œç©ºâ€å­©å­è®¾ç½®ä¸ºrootï¼Œå¦åˆ™å®¹æ˜“åœ¨è®¡ç®—å­©å­çš„failæŒ‡é’ˆæ—¶å‘ç”Ÿsegemntation faultï¼Œcurrent-\u0026gt;fail-\u0026gt;m_childs[i]è¿™ä¸ªè¡¨è¾¾å¼failå¯èƒ½æ˜¯rootã€‚ ä¸ç”¨é€’å½’å¯»æ‰¾ç¬¦åˆè¦æ±‚çš„failæŒ‡é’ˆ, ä½¿ç”¨ç±»ä¼¼å¹¶æŸ¥é›†çš„æ€æƒ³è¿›è¡Œè·¯å¾„å‹ç¼©ã€‚  è·¯å¾„å‹ç¼©\næ ¸å¿ƒä¸ºè¿™è¡Œä»£ç :\ncurrent-\u0026gt;m_childs[i] = current-\u0026gt;fail-\u0026gt;m_childs[i]; å½“current-\u0026gt;fail-\u0026gt;m_childs[i] = NULLçš„æ—¶å€™ï¼Œè¿™è¡Œä»£ç å°†m_childs[i]ç½®ä¸ºå…¶fail-\u0026gt;m_childs[i]ï¼Œç”±äºæ˜¯é€å±‚æ±‚è§£failçš„ï¼Œå› æ­¤å½“fail-\u0026gt;m_childs[i]ä¹Ÿç­‰äºNULLçš„æ—¶å€™ï¼Œå…¶å€¼ä¼šåœ¨ä¹‹å‰è¢«è®¾ç½®æˆå…¶fail-\u0026gt;m_childs[i]ã€‚\nå› æ­¤current-\u0026gt;fail-\u0026gt;m_childs[i];æ€»æ˜¯æŒ‡å‘æ»¡è¶³è¦æ±‚çš„èŠ‚ç‚¹æˆ–è€…rootã€‚\n æ»¡è¶³è¦æ±‚: failæŒ‡é’ˆæ‰€æŒ‡çš„æ ‘èŠ‚ç‚¹æ‰€æ„æˆçš„å­—ç¬¦ä¸²æ˜¯å½“å‰æ ‘èŠ‚ç‚¹æ„æˆå­—ç¬¦ä¸²çš„åç¼€ã€‚\n å‚è€ƒèµ„æ–™ #   [æ´›è°·æ—¥æŠ¥ç¬¬44æœŸ]å¼ºåŠ¿å›¾è§£ACè‡ªåŠ¨æœº: Link "},{"id":14,"href":"/blog/posts/algorithms/trie%E6%A0%91/","title":"Trieæ ‘","section":"Posts","content":"Trieæ ‘å¯ä»¥:\n å‹ç¼©å­˜å‚¨å¤§é‡çš„å­—ç¬¦ä¸² å¿«é€Ÿæ‰¾å‡ºå…·æœ‰ç›¸åŒå‰ç¼€çš„å­—ç¬¦ä¸² å¿«é€ŸæŒ‰å­—å…¸åºå¯¹å­—ç¬¦ä¸²è¿›è¡Œæ’åº \u0026hellip;  ä¸»è¦æ“ä½œ #  Trieæ ‘çš„ä¸»è¦æ“ä½œåŒ…æ‹¬:\n insert: æ’å…¥å­—ç¬¦ä¸²ã€‚ search: æŸ¥è¯¢æŸå­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨ã€‚ startsWith: æŸ¥è¯¢åŒ…å«æŸå‰ç¼€çš„å­—ç¬¦ä¸²ã€‚  [str]: è¿”å›æ‰€æœ‰åŒ…å«è¯¥å‰ç¼€çš„å­—ç¬¦ä¸²ã€‚ bool: è¿”å›æ˜¯å¦å­˜åœ¨ã€‚    å®ç° #  Trieæ ‘çš„å®ç°å’Œæ™®é€šæ ‘çš„å®ç°ç±»ä¼¼ï¼Œå¯ä»¥ç”¨æ•°ç»„æˆ–é“¾è¡¨å®ç°ã€‚ è¿™é‡Œçš„ä¾‹å­æ˜¯é“¾è¡¨å®ç°çš„ï¼Œå¦‚ä¸‹æ˜¯æ ‘èŠ‚ç‚¹çš„å®šä¹‰ã€‚\nclass TrieNode { public: TrieNode* m_childs[26] = { nullptr }; bool m_is_end; // æ˜¯å¦æ˜¯å•è¯è¾¹ç•Œ  char m_c; }; Trieæ ‘çš„å®šä¹‰å¦‚ä¸‹:\nclass Trie { public: Trie(); void insert(string word); bool search(string word); bool startsWith(string prefix); protected: TrieNode* root; }; "},{"id":15,"href":"/blog/posts/programming/semaphores/","title":"Semaphores(ä¿¡å·é‡)","section":"Posts","content":"Semaphoresæ˜¯ä¸€ç§åŒæ­¥æœºåˆ¶ï¼ˆConcurrency Mechanismsï¼‰ï¼Œå®ƒç”¨æ¥åè°ƒå„ä¸ªè¿›ç¨‹è®¿é—®å…¬å…±èµ„æºã€‚å…¶åŸºæœ¬æ€æƒ³å¦‚ä¸‹æ‰€è¿°ï¼š\n ä¸¤ä¸ªæˆ–å¤šä¸ªè¿›ç¨‹é€šè¿‡ä¸€ä¸ªä¿¡å·é‡è¿›è¡Œåè°ƒï¼Œå½“ä¸€ä¸ªè¿›ç¨‹éœ€è¦æŸä¸ªèµ„æºæ—¶ï¼Œå®ƒéœ€è¦ç”³è¯·å¹¶ç­‰å¾…ä¸€ä¸ªä¿¡å·ï¼Œå¦‚æœä¿¡å·æ²¡æœ‰æ¥ä¸´åˆ™ç­‰å¾…ã€‚\n General Semaphores #  general semaphoreçš„ä¸€ç§å®ç°ã€‚\nstruct semahpore { int count; queueType queue; }; void semWait(semaphore a) { s.count--; if (s.count \u0026lt; 0) { /* place this process in s.queue. */ /* block this process. */ } } void semSignal(semaphore a) { s.count++; if(s.count \u0026lt;= 0) { /* remove a process from s.queue. */ /* place process p on ready list. */ } }   countçš„å«ä¹‰\n å¤§äº0æ—¶ï¼šæŒ‡ç¤ºå½“å‰å¯ä»¥åŠ è¿›æ¥çš„è¿›ç¨‹æ•°ã€‚ å°äºç­‰äº0æ—¶ï¼šç»å¯¹å€¼æŒ‡ç¤ºå½“å‰æƒ³åŠ å…¥è¿˜æœªåŠ å…¥çš„è¿›ç¨‹æ•°ã€‚    é˜Ÿåˆ—é‡Œå­˜å‚¨çš„å°±æ˜¯ç­‰å¾…åŠ å…¥çš„è¿›ç¨‹ã€‚\n  Binary Semaphore #  struct binary_semaphore { enum {zero, one} value; queueType queue; }; // B: Binary void semWaitB(binary_semaphore s) { if (s.value == one) s.value = zero; else { /* place this process in s.queue. */ /* block this process. */ } } void semSignalB(semaphore s) { if (s.queue is empty()) s.value = one; else { /* remove a process P from s.queue. */ /* place process P on ready list. */ } } ä¼˜ç¼ºç‚¹ #   æ¯”è¾ƒéš¾ç¼–ç¨‹ï¼Œå½“éœ€è¦åŒæ—¶ä½¿ç”¨å¤šä¸ªsemaphoreçš„æ—¶å€™ï¼Œéœ€è¦ä»”ç»†å®‰æ’semaphoreè°ƒç”¨é¡ºåºç­‰ï¼Œå¦åˆ™å®¹æ˜“å‡ºç°æ­»é”ã€‚  ä¾‹å­ï¼š\n/* program producersonsumer */ semaphore n = 0, s = 1; void producer() { while (true) { produce(); semWait(s); append(); semSignal(s); semSignal(n); } } void consumer() { while(true) { semWait(n); semWait(s); take(); semSignal(s); consume(); } } void main() { parbegin (producer, consumer); }  è¿™é‡Œ semaphore n ç”¨äºé˜²æ­¢consumeræ¶ˆè€—ç©ºbufferï¼Œsç”¨äºbufferçš„mutual exclusionã€‚ å¦‚æœbufferä¸ºç©ºï¼Œä¸”consumerçš„ semWait(n); ä¸ semWait(s); äº’æ¢ï¼Œåˆ™ç¬¬ä¸€ä¸ªsemWaitèƒ½æˆåŠŸè¿›å…¥ï¼Œä½†ä¼šåœ¨ç¬¬äºŒsemWaitè¢«é˜»å¡ã€‚ è¿™æ—¶ç”±äºï¼Œbufferè¢«consumerå ç”¨ï¼Œproduceræ— æ³•ç”Ÿäº§æ–°çš„productåŠ å…¥bufferï¼Œ semWait(s);å°†è¢«ä¸€ç›´é˜»å¡ï¼Œäº§ç”Ÿæ­»é”ã€‚ "},{"id":16,"href":"/blog/posts/algorithms/%E6%B5%B7%E6%98%8E%E7%A0%81/","title":"é”™è¯¯æ£€æµ‹-æµ·(æ±‰)æ˜ç ","section":"Posts","content":"Hamming codeï¼Œæµ·æ˜ç ï¼Œæ±‰æ˜ç éƒ½æ˜¯ä¸€ä¸ªä¸œè¥¿ã€‚å®ƒæ˜¯ä¸€ç§ç¼–ç æ–¹å¼ï¼Œé€šå¸¸ç”¨åœ¨ç½‘ç»œä¿¡æ¯ä¼ è¾“ä¸­ï¼Œé€šè¿‡è¿™ç§ç¼–ç æ–¹å¼ç¼–ç å‡ºæ¥çš„äºŒè¿›åˆ¶æ•°æ®å…·æœ‰æ£€æµ‹ä¸€ä½é”™è¯¯ä½çš„èƒ½åŠ›ã€‚\nä¾‹å¦‚ï¼š\n å‡è®¾è¦ä¼ è¾“çš„äºŒè¿›åˆ¶æ•°æ®ä¸º 1011001 ã€‚ é€šè¿‡æµ·æ˜ç ç¼–ç åï¼ˆå¦‚ä½•ç¼–ç åé¢ä¼šè¯´ï¼‰ï¼Œå¾—åˆ° 101 0100 1110 ã€‚ å‡è®¾ä»å³å¾€å·¦ç¬¬6ä½ä»0å˜ä¸ºäº†1ã€‚ é€šè¿‡æŸç§æ–¹æ³•æˆ‘ä»¬å¯ä»¥ä»äº§ç”Ÿå˜åŒ–åçš„æ•°æ® 101 0110 1110 å¾—çŸ¥ç¬¬å…­ä½å‘ç”Ÿæ”¹å˜ã€‚  ä¸‹é¢åˆ™å°†å…·ä½“è®²è§£è¿™äº›è¿‡ç¨‹ã€‚\nç¼–ç è¿‡ç¨‹ #  ç¬¬ä¸€æ­¥æ˜¯ç¡®å®šå†—ä½™ä½(bit)çš„æ•°ç›®ï¼Œè¿™äº›ä½å°†ä¸ºæ£€é”™æä¾›å¿…è¦çš„ä¿¡æ¯ã€‚\næ ¹æ®ç®—æ³•è¦æ±‚ï¼Œå†—ä½™ä½çš„æ•°ç›®(r)å¿…é¡»æ»¡è¶³ï¼š $$ 2^r \\geq m + r + 1 $$ å…¶ä¸­mä¸ºåŸå§‹æ•°æ®ä½çš„æ•°ç›®ã€‚\nç¬¬äºŒæ­¥æ˜¯ç¡®å®šå†—ä½™ä½çš„ä½ç½®\nåŒæ ·æ ¹æ®ç®—æ³•è®¾è®¡ï¼Œå†—ä½™ä½å°†æ”¾åœ¨ä½ç½®ç¼–å·ä¸º2çš„å¹‚æ¬¡çš„ä½ç½®ä¸Šã€‚\nå¦‚ï¼š1,2,4,8\u0026hellip;\nå‡è®¾è¦ä¼ è¾“çš„æ•°æ®ä¸º 1011001 ï¼Œåˆ™ç¼–ç åå„ä¸ªæ•°æ®ä½çš„æ’å¸ƒåº”å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n ç¬¬ä¸‰æ­¥æ˜¯ç¡®å®šå†—ä½™ä½çš„å€¼\nè®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š\n å¤„äºç¬¬$2^i$ä½çš„å†—ä½™ä½çš„å€¼ï¼Œå°†æ˜¯æ‰€æœ‰ä½ç½®ç¼–ç (äºŒè¿›åˆ¶è¡¨ç¤º)ä¸­ç¬¬iä½ä¸º1çš„é‚£äº›ä½ç½®çš„åŸå§‹æ•°æ®çš„å¶æ ¡éªŒã€‚\n å¦‚R1ï¼Œå®ƒçš„å€¼å°†æ˜¯ç¬¬1,3,5,7,9,11è¿™äº›ä½ç½®ä¸Šçš„æ•°æ®çš„å¶æ ¡éªŒï¼Œæ‰€ä»¥\nç”±äº3ï¼Œ5ï¼Œ7ï¼Œ9ï¼Œ11ä¸­1çš„ä¸ªæ•°ä¸ºå¶æ•°ï¼Œæ•…R1=0ï¼ˆå¶æ ¡éªŒï½1çš„ä¸ªæ•°ä¸ºå¶æ•°ä¸ªï¼‰ã€‚\n å‡è®¾è¦ä¼ è¾“çš„æ•°æ®ä¸º 1011001 ï¼Œåˆ™ç¼–ç åç»“æœå¦‚ä¸‹ï¼š\n æ£€é”™ä¸çº é”™ #  åŒä¸Šï¼Œå‡è®¾è¦ä¼ è¾“çš„æ•°æ®ä¸º 1011001 ï¼Œç¼–ç åä¸º1010 1001 110 ã€‚\nå‡è®¾å®é™…ä¼ è¾“è¿‡å»çš„æ•°æ®ä¸º 1010 1101 110 ï¼Œå³ä»å³å¾€å·¦ç¬¬6ä½ç”±0å˜ä¸ºäº†1ã€‚\nè¿™æ—¶å€™ï¼Œå¦‚æœæˆ‘ä»¬å†ç”¨è®¡ç®—å†—ä½™ä½å€¼çš„æ–¹æ³•é‡æ–°è®¡ç®—ä¸€éå†—ä½™ä½çš„å€¼ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼š\nR4R3R2R1 = 0110 å³åè¿›åˆ¶ä¸‹çš„6ã€‚\nè¿™è¡¨æ˜ä»å³å¾€å·¦ç¬¬6ä½å‘ç”Ÿé”™è¯¯ï¼Œè¿™æ—¶å€™æˆ‘ä»¬å°†å…¶åè½¬å³å¯çº æ­£ã€‚\nIntuition #  å®é™…ä¸Šï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¾—åˆ°ä¸€ä¸ªæ•°å­—ï¼Œè¿™ä¸ªæ•°å­—è¡¨ç¤ºæ•°æ®ä¸­å‡ºé”™é‚£ä¸€ä½çš„ä½ç½®ã€‚\nå¦‚æœæˆ‘ä»¬å°†å…¶è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶çš„å½¢å¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ä»»åŠ¡å³å˜ä¸ºäº†ç¡®å®šå„ä¸ªbitä½æ˜¯0è¿˜æ˜¯1ã€‚\nåœ¨å‰é¢æˆ‘æ›¾è¦æ±‚è¿‡è¿™ä¸ªè¡¨è¾¾å¼ï¼š\n$2^r \\geq m + r + 1$\nå®ƒçš„å«ä¹‰å³åœ¨äºï¼Œå¦‚æœæˆ‘ä»¬åˆ©ç”¨rä¸ªå†—ä½™ä½ç¡®å®šè¿™ä¸ªäºŒè¿›åˆ¶æ•°å­—ï¼ˆrä¸ªå†—ä½™ä½ç¡®å®šrä¸ªæ•°å­—ä¸­çš„bitä½ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»ä¿è¯è¿™ä¸ªæ•°å­—çš„èŒƒå›´èƒ½å¤Ÿè¡¨ç¤ºç¼–ç æ•°æ®ä¸²ä¸­çš„æ‰€æœ‰ä½ç½®ã€‚\nå¯èƒ½æœ‰äººä¼šé—®ï¼Œæ—¢ç„¶å¦‚æ­¤ï¼Œé‚£æ»¡è¶³è¿™ä¸ªè¡¨è¾¾å¼$2^r \\geq m + r$ ä¸å°±å¯ä»¥äº†å—ï¼Œä¸ºä»€ä¹ˆè¦+1å‘¢ï¼Ÿ\n å› ä¸ºä½ç½®ç¼–ç å¿…é¡»ä»1å¼€å§‹ï¼Œä»0å¼€å§‹æ²¡æ³•çº é”™ã€‚å› æ­¤ä½ç½®ç¼–ç çš„æœ€å¤§å€¼ä½m+r+1ã€‚\n  æµ·æ˜ç çš„å·§å¦™ä¹‹å¤„å°±åœ¨äºå®ƒå¶æ ¡éªŒçš„åˆ†ç»„ä¸Šï¼Œæ¯ä¸ªå†—ä½™ä½éƒ½å¯¹åº”äº†ä¸€ä¸ªbitä½ä¸º1çš„é‚£äº›ä½ç½®ã€‚\n  ä¸€å¼€å§‹å°†æ•°æ®ç¼–ç æˆæµ·æ˜ç çš„æ—¶å€™ï¼Œæ‰€æœ‰ç»„éƒ½æ˜¯æ»¡è¶³å¶æ ¡éªŒçš„ï¼Œå³1çš„ä¸ªæ•°ä¸ºå¶æ•°ã€‚\n  å¦‚æœæŸä¸€ä½å‘ç”Ÿå˜åŒ–ï¼Œä¸ç®¡æ˜¯ä»0å˜ä¸º1è¿˜æ˜¯ä»1å˜ä¸º0ï¼Œå®ƒæ‰€åœ¨ç»„å¯¹åº”çš„å¶æ ¡éªŒå‡ä¼šå˜ä¸º1ã€‚\n  è€Œä¸”å®ƒåªå±äº-å®ƒä½ç½®ç¼–ç ä¸º1çš„é‚£å‡ ä½å¯¹åº”çš„é‚£äº›ç»„ï¼Œä¹Ÿåªæœ‰è¿™äº›ç»„çš„å¶æ ¡éªŒä¼šå˜æˆ1ã€‚\n  è¿™æ ·ï¼Œå¦‚æœæˆ‘ä»¬å¯¹æ‰€æœ‰ç»„æ±‚ä¸€æ¬¡å¶æ ¡éªŒï¼Œç»„æˆçš„äºŒè¿›åˆ¶æ•°è¡¨ç¤ºçš„æ­£å¥½æ˜¯å‘ç”Ÿå˜åŒ–é‚£ä¸ªä½çš„ä½ç½®ã€‚\n  æ¢å¥è¯è¯´ï¼Œæµ·æ˜ç å°†é”™è¯¯ä½ç½®çš„ç¡®è®¤åˆ†æ•£åˆ°äº†å„ä¸ªç»„ä¸Šï¼Œé€šè¿‡ä¸€æ¬¡èƒ½å¶æ ¡éªŒï¼Œèƒ½å¤Ÿç¡®å®šé”™è¯¯ä½ç½®å±ä¸å±äºè¿™ä¸ªç»„ï¼Œç»è¿‡ræ¬¡å¶æ ¡éªŒåï¼Œä¾¿èƒ½ç¡®å®šå…¶å…·ä½“ä½ç½®ã€‚\nå‚è€ƒèµ„æ–™ #   https://www.geeksforgeeks.org/computer-network-hamming-code/\n"},{"id":17,"href":"/blog/posts/ai/Generalized-Linear-Model/","title":"How genralized linear model work?","section":"Posts","content":"æœ¬æ–‡å°†ç®€å•çš„è®²è¿°ï¼šGLMæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ\nOur goal åœ¨è®¨è®ºGLMä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆè¦æ˜ç¡®æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼š\n ç»™å®šä¸€äº› feature Xï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹ä¸€ä¸ªyã€‚ å³æˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ª hpythoesis: $h_\\theta(x) = y$\n How GLM work?  å‡è®¾ y çš„å–å€¼æœä»æŸä¸ªåˆ†å¸ƒ å¦‚æœè¿™ä¸ªåˆ†å¸ƒå¯ä»¥å†™æˆæŒ‡æ•°æ—çš„å½¢å¼ï¼š$p(y; \\eta) = b(y) exp(\\eta^T T(y) âˆ’ a(\\eta))$ åˆ™æœ‰ä¸€ä¸ªæ€§è´¨ï¼šT(y)æ˜¯yçš„å……åˆ†ç»Ÿè®¡é‡ ç„¶åæˆ‘ä»¬åˆ™ç”¨ y åœ¨è¯¥åˆ†å¸ƒä¸‹çš„æ•°å­¦æœŸæœ›å»é¢„æµ‹yï¼Œå³æˆ‘ä»¬è®© $h_\\theta(x) = E(y)$ é™¤æ­¤ä¹‹å¤–è¿˜å‡è®¾ $\\eta$ ä¸ X çº¿æ€§ç›¸å…³ï¼Œå³ï¼š $\\eta = \\theta^TX$  Example  Logistic Regression   å‡è®¾yçš„å–å€¼æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒï¼š $y|x; \\theta âˆ¼ Bernoulli(\\phi)$ ä¼¯åŠªåˆ©åˆ†å¸ƒåœ¨æŒ‡æ•°æ—ä¸­ï¼Œå°†å…¶æ”¹å†™æˆæŒ‡æ•°æ—çš„å½¢å¼ï¼š  \\[ \\begin{align} p(y; \\phi) \u0026= \\phi^y (1-\\phi)^{1-y} \\\\ \u0026= exp(ylog(\\phi) + (1-y)log(1-\\phi))\\\\ \u0026= exp(ylog\\frac{\\phi}{1-\\phi} +log(1-\\phi)) \\end{align} \\]\nâ€‹ ç”±æ­¤å¯å¾—ï¼š \\( T(y) = y \\\\ \\eta = log\\frac{\\phi}{1-\\phi} \\) â€‹ åè§£$\\phi$ ï¼š \\( \\phi = \\frac{e^{\\eta}}{1+e^{\\eta}}= \\frac{1}{1+e^{-\\eta}} \\)\n åˆ©ç”¨yåœ¨ä¼¯åŠªåˆ©åˆ†å¸ƒä¸‹çš„æ•°å­¦æœŸæœ›é¢„æµ‹yï¼š  \\[ \\begin{align} y \u0026= E[y|x;\\theta] \\\\ \u0026=\\phi \\\\ \u0026= \\frac{1}{1+e^{-\\eta}} \\end{align} \\]\n å‡è®¾ $\\eta$ ä¸ X çº¿æ€§ç›¸å…³ï¼Œå³ï¼š $\\eta = \\theta^TX$ ã€‚å¯å¾—ï¼š  \\[ y = \\frac{1}{1+e^{-\\theta^TX}} \\]\n"},{"id":18,"href":"/blog/posts/algorithms/%E5%85%8B%E9%B2%81%E6%96%AF%E5%8D%A1%E5%B0%94%E7%AE%97%E6%B3%95/","title":"å…‹é²æ–¯å¡å°”ç®—æ³•","section":"Posts","content":"é¦–å…ˆï¼Œå…‹é²æ–¯å¡å°”ç®—æ³•æ˜¯ç”¨æ¥æ±‚æœ€å°ç”Ÿæˆæ ‘çš„ã€‚å¦ä¸€ç§æ±‚æœ€å°ç”Ÿæˆæ ‘çš„ç®—æ³•å«æ™®æ—å§†ç®—æ³•ï¼ˆPrimï¼‰ã€‚\nå…‹é²æ–¯å¡å°”ç®—æ³•æœ¬è´¨æ˜¯è´ªå¿ƒï¼Œæ¯æ¬¡é€‰å– ä¸ä¸å½“å‰å·²æœ‰è¾¹æ„æˆç¯ æƒå€¼æœ€å° çš„è¾¹ä½œä¸ºç”Ÿæˆæ ‘çš„è¾¹ã€‚\nç®—æ³•ç»†èŠ‚ #   åˆ¤æ–­æ˜¯å¦æ„æˆç¯ä½¿ç”¨å¹¶æŸ¥é›†  å¤æ‚åº¦åˆ†æ #  ä½¿ç”¨å¹¶æŸ¥é›†çš„å…‹é²æ–¯å¡å°”ç®—æ³•æ—¶é—´å¤æ‚åº¦ä¸ºO(eloge)ï¼Œå…¶ä¸­logeä¸ºå¹¶æŸ¥é›†åˆ¤æ–­ç¯æ‰€éœ€æ—¶é—´ï¼Œæ¯æ¡è¾¹éƒ½è¦åˆ¤æ–­ä¸€æ¬¡ï¼Œå› æ­¤ä¸ºO(eloge)ã€‚\n"},{"id":19,"href":"/blog/posts/algorithms/%E6%99%AE%E6%9E%97%E5%A7%86%E7%AE%97%E6%B3%95/","title":"æ™®æ—å§†ç®—æ³•","section":"Posts","content":"æ™®æ—å§†ç®—æ³•ä¹Ÿæ˜¯ç”¨æ¥æ±‚æœ€å°ç”Ÿæˆæ ‘çš„ï¼Œä¸å…‹é²æ–¯å¡å°”ç®—æ³•éå†è¾¹ä¸åŒï¼Œæ™®æ—å§†éå†çš„æ˜¯ç‚¹ã€‚\næ™®æ—å§†ç®—æ³•åŒæ ·åŸºäºè´ªå¿ƒï¼Œä»¥ä»»æ„ç‚¹ä¸ºåˆå§‹ç‚¹ï¼Œæ¯æ¬¡é€‰å–ä¸å·²é€‰ç‚¹ç›¸è¿çš„è¾¹ä¸­æƒå€¼æœ€å°çš„è¾¹ï¼Œå¹¶æŠŠä¸è¿™æ¡è¾¹ç›¸è¿çš„ç‚¹åŠ å…¥å·²é€‰ç‚¹é›†åˆã€‚\nç®—æ³•å®ç° #   ç»´æŠ¤ä¸€ä¸ªæ•°ç»„ minEdge[i]  å«ä¹‰ä¸ºå·²é€‰ç‚¹é›†åˆä¸­åˆ°ç¬¬iä¸ªç‚¹ æœ€å°æƒå€¼çš„è¾¹çš„ç»ˆç‚¹ï¼Œæ²¡æœ‰è¾¹åˆ™ä¸ºæ— ç©·å¤§ã€‚\næ¯æ¬¡é€‰minEdgeä¸­æœ€å°çš„ç‚¹åŠ å…¥å·²é€‰ç‚¹é›†åˆï¼Œå¹¶æ›´æ–°minEdgeæ•°ç»„ã€‚ é‡å¤æ“ä½œï¼Œç›´åˆ°æ‰€æœ‰ç‚¹åŠ å…¥å·²é€‰ç‚¹é›†åˆã€‚  ä¼˜åŒ– #   ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—ç»´æŠ¤minEdge  æ¯å¾€å·²é€‰ç‚¹é›†åˆåŠ å…¥ç‚¹æ—¶ï¼Œå°±æŠŠä¸è¯¥ç‚¹ç›¸è¿çš„æ‰€æœ‰è¾¹éƒ½åŠ å…¥ä¼˜å…ˆé˜Ÿåˆ—ã€‚è€Œå½“å–è¾¹æ—¶ï¼Œéœ€è¦åˆ¤æ–­ä¸€ä¸‹è¾¹çš„ç»ˆç‚¹æ˜¯å¦åœ¨å·²é€‰ç‚¹é›†åˆä¸­ï¼ˆå¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ ‡è®°æ•°ç»„ï¼‰ã€‚\næ—¶é—´å¤æ‚åº¦åˆ†æ #   éä¼˜å…ˆé˜Ÿåˆ—æ³•   åœ¨minEdgeä¸­æ‰¾æœ€å°çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n)ã€‚ æ›´æ–°æ•°ç»„æ—¶é—´å¤æ‚åº¦æ€»å’Œä¸ºO(e) â€”â€” æ¯ä¸ªç‚¹éƒ½ä¼šæ›´æ–°ä¸å®ƒç›¸è¿çš„è¾¹ï¼Œæ‰€æœ‰è¾¹åŠ èµ·æ¥ä¸ºeã€‚ ä¸€å…±è¦è¿›è¡Œnæ¬¡åœ¨minEdgeä¸­æ‰¾æœ€å°çš„æ“ä½œã€‚  æ•…æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n^2+e)ã€‚\nä¼˜å…ˆé˜Ÿåˆ—æ³•  ä¼˜å…ˆé˜Ÿåˆ—åŠ è¾¹å‡ºè¾¹çš„æ—¶é—´å¤æ‚åº¦éƒ½æ˜¯O(loge)ã€‚\n æ‰€æœ‰è¾¹éƒ½ä¼šè¿›å…¥ä¼˜å…ˆé˜Ÿåˆ—è‡³å°‘ä¸€æ¬¡ï¼Œè‡³å¤šä¸¤æ¬¡ã€‚æ—¶é—´å¤æ‚åº¦ä¸ºO(eloge)ã€‚ å‡ºè¾¹æ¬¡æ•°æœ€å°‘ä¸ºnæ¬¡ï¼Œæœ€å¤šä¸ºeæ¬¡ï¼Œæ—¶é—´å¤æ‚åº¦ä¹Ÿæ˜¯O(eloge)ã€‚  æ•…æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(eloge)ã€‚\nåˆ†æï¼š å½“è¾¹æ•°è¾¾åˆ°cn^2æˆ–ä»¥ä¸Šæ—¶ï¼Œç”¨éä¼˜å…ˆé˜Ÿåˆ—æ³•è¦å¥½ï¼Œåä¹‹å¯ä»¥ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—æ³•ã€‚\nå½“e = n^2 ,  O(eloge) = O(n^2logn) , O(n^2+e) = O(n^2) ã€‚\n"},{"id":20,"href":"/blog/posts/algorithms/%E5%B9%B6%E6%9F%A5%E9%9B%86/","title":"å¹¶æŸ¥é›†","section":"Posts","content":"ä½¿ç”¨å¹¶æŸ¥é›†å¯ä»¥å¿«é€Ÿåˆ¤æ–­ä¸¤ä¸ªå…ƒç´ æ˜¯å¦å±äºåŒä¸€ä¸ªé›†åˆã€‚\nç®—æ³• #  åŸºæœ¬æ€è·¯ #  ä½¿ç”¨æ ‘æ¥è¡¨ç¤ºé›†åˆã€‚\n åˆå§‹æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åˆ†åˆ«æ˜¯ä¸€æ£µæ ‘ã€‚ è‹¥ä¸¤ä¸ªå…ƒç´ å±äºåŒä¸€ä¸ªé›†åˆï¼Œåˆ™ç”¨ä¸€ä¸ªå…ƒç´ ä½œä¸ºå¦ä¸€ä¸ªå…ƒç´ çš„å­©å­ã€‚  å®ç°\næ•°æ®ç»“æ„ï¼š\n fa[i] : ä¿å­˜å…ƒç´ içš„ç¥–å…ˆ  ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–\nfor(int i=0; i \u0026lt; n; ++i) fa[i] = i;\t// åˆå§‹æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åˆ†åˆ«æ˜¯ä¸€æ£µæ ‘(ç¥–å…ˆæ˜¯è‡ªå·±ï¼‰ ç¬¬äºŒæ­¥ï¼šæ ¹æ®å…³ç³»æ„é€ å¹¶æŸ¥é›†\n// æŸ¥æ‰¾æ ‘æ ¹ int find(int x) { if(fa[x] == x) return x; return find(fa[x]); } int temp_a, temp_b; for(int i=0; i \u0026lt; m; ++i){ cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, bå±äºåŒä¸€é›†åˆ  temp_a = find(a), temp_b = find(b); fa[a] = b; } Note: å…ˆæ‰¾åˆ°æ ‘æ ¹ï¼Œä¸ç®¡ç›¸ä¸ç›¸åŒï¼Œéƒ½è®©aæ˜¯bçš„å­©å­ã€‚\nç¬¬ä¸‰æ­¥ï¼šæŸ¥è¯¢æ–¹æ³•\nfor(int i=0; i \u0026lt; T; ++i){ cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; if(find(a) == find(b)) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } Note: åˆ¤æ–­æ ‘æ ¹æ˜¯å¦ç›¸åŒå³å¯ã€‚\nä¼˜åŒ– #   é™ä½æ ‘çš„æ·±åº¦ï¼ˆè·¯å¾„å‹ç¼©ï¼‰  find æ“ä½œè€—è´¹æ—¶é—´å–å†³äºæ ‘çš„æ·±åº¦ï¼Œå› æ­¤ï¼Œå¦‚æœåœ¨ find  æ‰§è¡Œè¿‡ç¨‹è®©æ ‘çš„æ·±åº¦é™ä½ï¼Œåˆ™å¯ä»¥é™ä½åç»­æ“ä½œæ‰€éœ€çš„æ—¶é—´ã€‚\næ–¹æ³•ï¼š è®©æ ‘æ ¹çš„åä»£ä»¬å°½é‡éƒ½æ˜¯æ ‘æ ¹çš„å­©å­ã€‚\n// æŸ¥æ‰¾æ ‘æ ¹ int find(int x) { if(fa[x] == x) return x; //return find(fa[x]);  return fa[x] = find(fa[x]); } Note: fa[x] = find(fa[x]) çš„å€¼ä¸ºå·¦å€¼ fa[x] ï¼Œç­‰ä»·äºå…ˆfa[x] = find(fa[x]);  å† return fa[x]; ã€‚\nè®©å°æ ‘æ¥åœ¨å¤§æ ‘ä¸Š  æ¯æ¬¡å‘ç”Ÿæ ‘çš„æ‹¼æ¥ä¹‹åçš„ find æ“ä½œéƒ½éœ€è¦é‡æ–°é™ä½æ ‘çš„æ·±åº¦ï¼Œè€Œæ˜¾ç„¶å…ƒç´ å°‘çš„æ ‘æ¥åˆ°å…ƒç´ å¤šçš„æ ‘ä¸Šï¼Œéœ€è¦çš„ é‡ç»„æ¬¡æ•°è¾ƒå°‘ã€‚\nä¾‹å­ï¼š\n ç°è‰²åœ†åœˆå³ä¸ºè¦é‡ç»„çš„å…ƒç´ ï¼Œæ˜¾ç„¶å°æ¥å¤§è¦åˆç®—ã€‚\nå®ç°æ–¹æ³•ï¼š\n r[i] : è¡¨ç¤ºä»¥å…ƒç´ iä¸ºæ ¹çš„æ ‘çš„ç›¸å¯¹å¤§å°  // å°è£…ä¸€ä¸‹ void Union(int a, int b) { int temp_a = find(a), temp_b = find(b); if(r[temp_a] \u0026gt;= r[temp_b]) { fa[b] = a; //å°æ ‘çš„çˆ¶äº²ç­‰äºå¤§æ ‘  if(r[temp_a] == r[temp_b]) ++r[a]; // bä¸ºaçš„å­©å­-\u0026gt;æ ‘å˜å¤§äº†  } else fa[a] = b; } for(int i=0; i \u0026lt; n; ++i) { fa[i] = i;\tr[i] = 0;\t// è®°å¾—åˆå§‹åŒ– } for(int i=0; i \u0026lt; m; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, bå±äºåŒä¸€é›†åˆ \tUnion(a,b); } å®éªŒ #  è¾“å…¥ï¼š\n 10 8 1 2 1 3 1 4 1 5 2 6 5 7 5 8 0 9 5 1 6 1 3 5 7 0 9 1 0 è¾“å‡ºï¼š\nYes Yes Yes Yes No å®Œæ•´ä»£ç ï¼š\n// å¹¶æŸ¥é›† #include \u0026lt;iostream\u0026gt;using namespace std; const int MAXN = 100; int fa[MAXN], r[MAXN], n, m; // nä¸ºå…ƒç´ æ•°,mä¸ºå…³ç³»æ•°  // æŸ¥æ‰¾æ ‘æ ¹ int find(int x) { if(fa[x] == x) return x; return find(fa[x]); } void Union(int a, int b) { int temp_a = find(a), temp_b = find(b); if(r[temp_a] \u0026gt;= r[temp_b]){ fa[b] = a; //å°æ ‘çš„çˆ¶äº²ç­‰äºå¤§æ ‘  if(r[temp_a] == r[temp_b]) ++r[a]; // bä¸ºaçš„å­©å­-\u0026gt;æ ‘å˜å¤§äº†  } else fa[a] = b; } int main() { cin \u0026gt;\u0026gt; n; // åˆå§‹åŒ–  for(int i=0; i \u0026lt; n; ++i) { fa[i] = i;\t// åˆå§‹æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åˆ†åˆ«æ˜¯ä¸€æ£µæ ‘(ç¥–å…ˆæ˜¯è‡ªå·±ï¼‰  r[i] = 0; } cin \u0026gt;\u0026gt; m; // æ„é€ å¹¶æŸ¥é›†  int a, b; for(int i=0; i \u0026lt; m; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, bå±äºåŒä¸€é›†åˆ  Union(a,b); } // æŸ¥è¯¢  int T; cin \u0026gt;\u0026gt; T; for(int i=0; i \u0026lt; T; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; if(find(a) == find(b)) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } return 0; } å¤æ‚åº¦åˆ†æ #   ä¼˜åŒ–å‰çš„ç®—æ³•  ç®—æ³•å¤æ‚åº¦ä¸»è¦å–å†³äºï¼š1. æ„é€ å¹¶æŸ¥é›† 2. æŸ¥è¯¢æ“ä½œ\n æ„é€ å¹¶æŸ¥é›†å¤æ‚åº¦åˆ†æï¼š  æŸ¥è¯¢findæ“ä½œé€’å½’æ¬¡æ•°ä¸ä¼šè¶…è¿‡æ ‘çš„æ·±åº¦ï¼Œæ ‘çš„æ·±åº¦ä¸ä¼šå¤§äºnï¼Œå› æ­¤ï¼Œfindæ“ä½œå¤æ‚åº¦ä¸ºO(logn)ã€‚\næ„é€ æ—¶æ—¶é—´èŠ±è´¹ä¸»è¦åœ¨findæ“ä½œä¸Šï¼Œä¸”è¦æ‰§è¡Œ2mæ¬¡findæ“ä½œã€‚å› æ­¤æ„é€ å¹¶æŸ¥é›†çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(2mlogn), å¿½ç•¥å¸¸æ•°ä¸ºO(mlogn)ã€‚\n æŸ¥è¯¢æ“ä½œå¤æ‚åº¦åˆ†æï¼š  æŸ¥è¯¢æ“ä½œéœ€è¦ä¸¤æ¬¡findæ“ä½œï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(logn)ã€‚\nä¼˜åŒ–åçš„ç®—æ³•  å…ˆè¯´ç»“è®ºï¼šO((n+m)log*n) ï¼Œå…¶ä¸­log*ä¸ºå¢é•¿åŠå…¶ç¼“æ…¢çš„ è¿­ä»£å‡½æ•°ï¼Œé€šå¸¸å¯ä»¥è§†ä¸ºå¸¸æ•°ã€‚\nè¯æ˜ä¸ºæ„é€ æ€§è¯æ˜ï¼Œè¾ƒä¸ºå¤æ‚ï¼Œè¿™é‡Œä¸åšæ‰©å±•ã€‚\n"},{"id":21,"href":"/blog/posts/ai/Principal-component-analysis/","title":"Principal component analysis","section":"Posts","content":"PCA (Principal component analysis) æ˜¯ä¸€ç§ç»™æ•°æ®é™ç»´çš„æ–¹æ³•ã€‚\nåˆ©ç”¨PCAï¼Œèƒ½å°†ä¸€å †é«˜ç»´ç©ºé—´çš„æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ï¼Œå¹¶æœ€å¤§é™åº¦ä¿æŒå®ƒä»¬ä¹‹é—´çš„å¯åŒºåˆ†æ€§ã€‚\n å¯ä»¥çœ‹åˆ°ï¼Œå›¾ä¸­çš„æ•°æ®ç‚¹å¤§è‡´åˆ†å¸ƒåœ¨ä¸€æ¡ç›´çº¿ä¸Šã€‚\nå› æ­¤ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ç‚¹æŠ•å½±åˆ°ç›´çº¿ä¸Šï¼Œç”¨ç›´çº¿ä¸Šçš„ç‚¹åˆ°åŸç‚¹çš„è·ç¦»ä»£æ›¿åŸæ¥äºŒç»´å‘é‡ã€‚\nè¿™æ ·æˆ‘ä»¬çš„æ•°æ®å°±ä» äºŒç»´é™åˆ°äº†ä¸€ç»´ã€‚\n æ¨å¯¼è¿‡ç¨‹ #   è®¾è¾“å…¥æ•°æ®ä¸ºX ï¼ŒX ä¸º (n * m) çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªsampleã€‚  å¦‚æœæˆ‘ä»¬è¦å°† æ•°æ® è½¬æ¢åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹ X åšä¸€æ¬¡çº¿æ€§å˜æ¢ï¼ˆå³æ›´æ¢åŸºåº•ï¼‰ã€‚\n è®¾æ–°çš„ åŸºåº• ä¸º P ï¼Œ P ä¸ºä¸€ä¸ª (m * k) çš„çŸ©é˜µï¼Œæ¯ä¸€åˆ—ä¸ºä¸€ä¸ªåŸºåº•ã€‚ ç»è¿‡å˜æ¢åï¼Œè®¾æ–°çš„æ•°æ®ä¸º Y, åˆ™ $Y=XP$ ã€‚  X çš„åæ–¹å·®çŸ©é˜µä¸º $X^TX$ï¼Œæˆ‘ä»¬å¸Œæœ›ç»è¿‡å˜æ¢åçš„ Y çš„åæ–¹å·®çŸ©é˜µï¼ˆ$Y^TY$ï¼‰\n å¯¹è§’çº¿ä¸Šå…ƒç´ ç»å¯¹å€¼å°½å¯èƒ½å¤§ éå¯¹è§’çº¿ä¸Šå…ƒç´ ç»å¯¹å€¼ç»å¯¹å€¼å°½å¯èƒ½å°ã€‚  å³æˆ‘ä»¬å¸Œæœ›ï¼Œåœ¨æ–°çš„åŸºåº•ä¸‹ï¼Œå„ä¸ªfeatureçš„å…³è”æ€§å¾ˆå°ï¼Œè€Œåœ¨åŒä¸€ä¸ªfeatureä¸Šï¼Œèƒ½å°½æœ€å¤§å¯èƒ½ä¿æŒæ•°æ®çš„varianceã€‚\nYçš„åæ–¹å·®çŸ©é˜µçš„æ•°å­¦è¡¨è¾¾å¼:\n$$C_Y = Y^TY = (XP)^T(XP) = P^TX^TXP$$\nå¯ä»¥è¯æ˜ï¼Œä¸ºäº†ä½¿ $C_Y$ æ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼ŒP åº”ä¸º $X^TX$ çš„ç‰¹å¾å‘é‡çŸ©é˜µï¼Œå…¶ä¸­ P çš„æ¯ä¸€åˆ—ä¸ºä¸€ä¸ªç‰¹å¾å‘é‡ã€‚\né‚£ä¹ˆï¼Œç”±çŸ©é˜µå¯¹è§’åŒ–çš„çŸ¥è¯†ï¼Œå¯ä»¥å¾—åˆ°\n$$C_Y = P^TX^TXP = D$$\nå…¶ä¸­Dä¸ºä¸€å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šå…ƒç´ ä¸ºç‰¹å¾å‘é‡å¯¹åº”çš„ç‰¹å¾å€¼ã€‚\nåœ¨è¿™é‡Œï¼Œæ¯ä¸ªç‰¹å¾å€¼è¿˜å¯¹åº”äº†æ–°çš„featureçš„æ–¹å·®ï¼ˆvarianceï¼‰ï¼Œå®ƒçš„å€¼çš„å¤§å°åæ˜ äº†æ–°çš„featureç”¨äºåŒºåˆ†æ•°æ®çš„èƒ½åŠ›ã€‚æ–¹å·®å°è¯´æ˜è¿™ä¸ªfeatureå¯¹å¤§éƒ¨åˆ†æ•°æ®æ¥è¯´åŸºæœ¬éƒ½ä¸€æ ·ï¼ˆç›´è§‚æ¥è¯´ï¼Œæ—¢ç„¶å¤§å®¶éƒ½ä¸€æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯´è¿™ä¸ªfeatureæ˜¯å¤šä½™çš„ï¼‰ã€‚\nå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç‰¹å¾å‘é‡æ ¹æ®ç‰¹å¾å€¼æ’åºï¼Œæ ¹æ®éœ€è¦å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºä½ç»´æ•°æ®ï¼Œå¹¶å°½æœ€å¤§å¯èƒ½ä¿æŒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚\nå‚è€ƒèµ„æ–™ #  Stackexhange\n  How would you explain covariance to someone who understands only the mean?  Making sense of principal component analysis, eigenvectors \u0026amp; eigenvalues  Why is the eigenvector of a covariance matrix equal to a principal component?  Intuition on the definition of the covariance  Does the magnitude of covariance have any real meaning?  Intuitive understanding covariance, cross-covariance, auto-/cross-correliation and power spectrum density  Wikipedia\n  Covariance  Cross product  Covariance matrix  Other\n A Tutorial on Principal Component Analysis\u0026rsquo;by Jonathon Shlens  Principal Components Analysis "},{"id":22,"href":"/blog/posts/misc/%E6%B4%9B%E4%BC%A6%E5%85%B9%E5%8F%98%E6%8D%A2/","title":"æ´›ä¼¦å…¹å˜æ¢","section":"Posts","content":"æ´›ä¼¦å…¹å˜æ¢æ˜¯ä»å…‰é€Ÿä¸å˜åŸç†æ¨å‡ºçš„ï¼Œä¸åŒåæ ‡ç³»åæ ‡ä¹‹é—´çš„è½¬æ¢å…³ç³»ã€‚\n**å…‰é€Ÿä¸å˜åŸç†ï¼š**å¯¹ä»»ä½•å‚è€ƒç³»ï¼Œå…‰é€Ÿéƒ½ä¸ºä¸€ä¸ªå›ºå®šå€¼C\nç‹­ä¹‰ç›¸å¯¹æ€§åŸç†ï¼š åœ¨æ‰€æœ‰æƒ¯æ€§ç³»ä¸­ï¼Œç‰©ç†å®šå¾‹éƒ½æœ‰ç›¸åŒçš„è¡¨è¾¾å½¢å¼\n æ´›ä¼¦å…¹å˜æ¢æ¨å¯¼ #   ä»¥äºŒç»´ç›´è§’åæ ‡ä¸ºä¾‹ï¼Œåªè€ƒè™‘xè½´æ–¹å‘è¿åŠ¨\nå‡è®¾ä½ ç›¸å¯¹äºæˆ‘å‘xè½´æ­£æ–¹å‘ä»¥é€Ÿåº¦ u è¿åŠ¨ï¼Œæˆ‘å’Œä½ åœ¨æŸä¸ªæ—¶é—´ç›¸é‡ï¼Œè¿™æ—¶å€™\n æˆ‘å’Œä½ åˆ†åˆ«ä»¥è‡ªå·±ä¸ºåŸç‚¹å»ºç«‹åæ ‡ç³»ã€‚ å°†æˆ‘å’Œä½ çš„æ—¶é’Ÿå½’é›¶    ä»¥æˆ‘çœ‹æ¥ï¼Œå‡è®¾åœ¨ t æ—¶åˆ»ï¼Œ x ä½ç½®å‘ç”Ÿäº†ä¸€ä¸ªäº‹ä»¶ã€‚\nè®¾è¯¥äº‹ä»¶ï¼Œåœ¨ä½ çœ‹æ¥ï¼Œå‘ç”Ÿåœ¨ tâ€˜ æ—¶åˆ»ï¼Œ xâ€˜ ä½ç½®ã€‚\næ¥ä¸‹æ¥ï¼Œæœ‰å…³æˆ‘çš„é‡éƒ½ä¸å¸¦ ' ,è€Œæœ‰å…³ä½ çš„éƒ½å¸¦ 'ã€‚\n###ç›¸å¯¹è®ºå‡ºç°ä¹‹å‰\né‚£ä¹ˆç”±äºä½ ç›¸å¯¹æˆ‘åœ¨è¿åŠ¨ï¼Œåˆ™åœ¨æˆ‘çœ‹æ¥ï¼Œä½ è¿åŠ¨äº† ut çš„è·ç¦»ï¼Œå¹¶ä¸”æˆ‘è®¤ä¸º ä½ è®¤ä¸ºäº‹ä»¶å‘ç”Ÿçš„ä½ç½®æ˜¯ $$ x' = x-ut $$ è€Œåè¿‡æ¥ï¼Œåœ¨ä½ çœ‹æ¥ï¼Œä½ è®¤ä¸ºä½ è¿åŠ¨äº† ut' çš„è·ç¦»ï¼Œä½ è®¤ä¸º æˆ‘è®¤ä¸ºäº‹ä»¶å‘ç”Ÿçš„ä½ç½®æ˜¯ $$ x = x' + ut' $$\n###ç›¸å¯¹è®ºå‡ºç°ä¹‹å\nå¦‚æœç°åœ¨ï¼Œæˆ‘ä»¬è´¨ç–‘è¿™ä¸¤ä¸ªå¼å­çš„æ­£ç¡®æ€§ã€‚\næ¯”å¦‚ï¼Œå¦‚æœåœ¨ä½ çœ‹æ¥ï¼Œä½ è§‰å¾—ä½ çš„ x' ä¸æˆ‘è®¤ä¸º ä½ è®¤ä¸ºçš„ x' ä¸ç›¸ç­‰ï¼Œé‚£ä¹ˆä¸å¦‚ç»™å¼å­ä¹˜ä¸Šä¸€ä¸ªç³»æ•° $\\gamma$ ï¼Œé€šè¿‡åˆ«çš„æ¡ä»¶è®¡ç®—ï¼Œå¦‚æœç³»æ•°æ˜¯1ï¼Œé‚£æˆ‘å°±æ˜¯å¯¹çš„ï¼Œå¦‚æœä¸æ˜¯ï¼Œé‚£ç»“æœå°±æ˜¯é‚£æ ·ã€‚ $$ x' = \\gamma (x-ut) $$ åŒç†ï¼Œæˆ‘è§‰å¾—ä½ çš„ x ä¸ä½ è®¤ä¸ºçš„æˆ‘çš„ x ä¸ç›¸ç­‰ï¼ŒåŒæ ·çš„ï¼Œä¹˜ä¸€ä¸ªç³»æ•° $\\gamma$ ï¼Œ è¿™ä¸ªç³»æ•°ä¸ä¹‹å‰çš„ä¸€æ · [1]ã€‚ $$ x = \\gamma (x'+ut') $$ ä¸ºäº†è§£å‡ºè¿™ä¸ªç³»æ•° $\\gamma$ ï¼Œæˆ‘ä»¬ä¸å¦¨ä»ä¸€ä¸ªå…·ä½“äº‹ä»¶å‡ºå‘ï¼ˆå½“ç„¶è¿™å¯èƒ½å¯¼è‡´æ¨å¯¼ä¸å¤ªä¸¥è°¨ï¼Œä½†æ˜¯å¯ä»¥è¯æ˜ï¼Œç»“æœæ˜¯æ­£ç¡®çš„ï¼‰ã€‚\n  å‡è®¾è¿™ä¸ªäº‹ä»¶ä»¥ä¸€æŸå…‰è§¦å‘ï¼Œå¹¶ä¸”åœ¨æˆ‘çœ‹æ¥åœ¨ t æ—¶é—´çš„æ—¶å€™å‘ç”Ÿ\n ç”±å…‰é€Ÿä¸å˜åŸç†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ° [2]\n åœ¨æˆ‘çœ‹æ¥ï¼Œå‘ç”Ÿçš„è·ç¦» $x = ct$ åœ¨ä½ çœ‹æ¥ï¼Œè·ç¦» $x' = ct'$  ç”±ä¸Šé¢å››ä¸ªæ–¹ç¨‹ï¼Œåˆ™èƒ½æ¨å‡ºæ´›ä¼¦å…¹å˜æ¢çš„åæ ‡å˜æ¢å¼ [3]\n$$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx' = \\frac{x-ut}{\\sqrt{1-u^2/c^2}} \u0026amp; (1)\\\nx = \\frac{x'+ut'}{\\sqrt{1-u^2/c^2}} \u0026amp; (2) \\end{array}\n\\right.\n\\end{equation} $$ æ–¹ç¨‹ï¼ˆ1ï¼‰å³ç”±æˆ‘çš„åæ ‡è½¬æ¢ä¸ºä½ çš„ã€‚\næ–¹ç¨‹ï¼ˆ2ï¼‰åè¿‡æ¥ã€‚\næ³¨æ„ï¼š uè¡¨ç¤ºä½ ç›¸å¯¹æˆ‘å‘æ­£æ–¹å‘è¿åŠ¨çš„é€Ÿåº¦ã€‚\nè€Œåˆ©ç”¨ (1), (2)å¼è¿˜èƒ½å¤Ÿå¯¼å‡º æ´›ä¼¦å…¹å˜æ¢çš„æ—¶é—´å˜æ¢å¼ [4]ã€‚ $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nt' = \\frac{t-xu/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp; (3)\\\nt = \\frac{t'+x\u0026rsquo;u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp; (4) \\end{array}\n\\right.\n\\end{equation} $$ ä»…ä»…é€šè¿‡è¿™å››ä¸ªå¼å­å¯èƒ½ä¸å¤ªå®¹æ˜“å¼„æ˜ç™½ æ´›ä¼¦å…¹å˜æ¢ç©¶ç«Ÿæ„å‘³ç€ä»€ä¹ˆã€‚\nä¸‹é¢å°†å±•ç¤ºä¸€ç³»åˆ—é€šè¿‡æ´›ä¼¦å…¹å˜æ¢çš„åˆ°çš„ç»“æœã€‚\næ—¶é—´å·®ä¸ç©ºé—´å·® #  å‡è®¾åœ¨æˆ‘çœ‹æ¥åœ¨(åœ¨æˆ‘çš„åæ ‡ç³»ä¸­)ï¼Œ åœ¨$(x_1,t_1)$ å’Œ $(x_2,t_2)$ åˆ†åˆ«å‘ç”Ÿäº†ä¸¤ä¸ªäº‹ä»¶ã€‚\né‚£ä¹ˆç”±æ´›ä¼¦å…¹å˜æ¢ï¼Œåœ¨ä½ çœ‹æ¥ $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx_1' = \\frac{x_1-ut_1}{\\sqrt{1-u^2/c^2}} \u0026amp; \\\nt_1' = \\frac{t_1-x_1u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp;\n\\end{array}\n\\right.\n\\end{equation} $$\n$$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx_2' = \\frac{x_2-ut_2}{\\sqrt{1-u^2/c^2}} \u0026amp; \\\nt_2' = \\frac{t_2-x_2u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp;\n\\end{array}\n\\right.\n\\end{equation} $$\nä¸Šä¸‹åˆ†åˆ«ç›¸å‡ï¼Œåˆ™å¯ä»¥å¾—åˆ°æ—¶é—´å·®å’Œç©ºé—´å·®çš„å…³ç³» $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\n\\Delta x' = \\frac{\\Delta x -u\\Delta t }{\\sqrt{1-u^2/c^2}}\u0026amp; (5)\\\n\\Delta t' = \\frac{\\Delta t - \\Delta x u/c^2}{\\sqrt{1-u^2/c^2}}\u0026amp; (6) \\end{array}\n\\right.\n\\end{equation} $$ (5) æ„å‘³ç€åœ¨æˆ‘çœ‹æ¥çš„è·ç¦» $\\Delta x$ï¼Œåœ¨ç›¸å¯¹æˆ‘è¿åŠ¨çš„ä½ çœ‹æ¥ï¼Œè¦é•¿ä¸€äº›ã€‚\n(6) æ„å‘³ç€åœ¨æˆ‘çœ‹æ¥çš„æ—¶é—´å·® $\\Delta t$ ï¼Œå¯¹äºç›¸å¯¹æˆ‘è¿åŠ¨çš„ä½ æ¥è¯´ï¼Œä¸å†æ˜¯ç›¸åŒçš„æ—¶é—´å·®ã€‚\né•¿åº¦æ”¶ç¼© #  ç°åœ¨ï¼Œå‡è®¾æˆ‘å’Œä½ è¦æµ‹é‡ä¸€ä¸ªç‰©ä½“çš„é•¿åº¦ï¼Œè¿™ä¸ªç‰©ä½“ä»¥uçš„é€Ÿåº¦ç›¸å¯¹æˆ‘å‘æ­£æ–¹å‘è¿åŠ¨ï¼Œè€Œä½ å’Œè¿™ä¸ªç‰©ä½“ä¸€èµ·è¿åŠ¨ã€‚\næˆ‘å°†é€šè¿‡ä¸¤ä¸ªäº‹ä»¶æµ‹é‡è¿™ä¸ªç‰©ä½“çš„é•¿åº¦ã€‚\n åœ¨æŸä¸€æ—¶åˆ»ï¼Œåœ¨ç‰©ä½“ä¸¤ç«¯å‘ç”Ÿäº†ä¸¤ä¸ªäº‹ä»¶ï¼Œåˆ†åˆ«è®°ä¸º $(x_1,t),(x_2,t)$ é‚£ä¹ˆä¸¤ä¸ªäº‹ä»¶çš„ç©ºé—´å·®å³ä¸ºç‰©ä½“çš„é•¿åº¦  é‚£ä¹ˆç”±å‰é¢çš„ç©ºé—´å·®çš„å…³ç³»ï¼Œä½ æµ‹é‡å‡ºçš„é•¿åº¦ä¸º $\\Delta x'$ ï¼Œè€Œæˆ‘æµ‹é‡å‡ºçš„é•¿åº¦ä¸º $\\Delta x$ï¼Œç”±å…¬å¼5å¯çŸ¥ï¼Œæˆ‘æµ‹å‡ºçš„é•¿åº¦æ¯”ä½ æµ‹å‡ºçš„è¦çŸ­ä¸€äº›ã€‚\nå³ç›¸å¯¹ç‰©ä½“è¿åŠ¨çš„è§‚å¯Ÿè€… æµ‹å‡ºçš„ç‰©ä½“é•¿åº¦ è¦æ¯”ç›¸å¯¹ç‰©ä½“é™æ­¢çš„è§‚å¯Ÿè€… æµ‹å‡ºçš„è¦çŸ­ã€‚\næ—¶é—´å»¶ç¼“ #  å‰é¢æåˆ°ï¼Œæˆ‘å’Œè¿åŠ¨çš„ä½ ï¼Œå¯¹äºä¸¤ä¸ªäº‹ä»¶çš„æ—¶é—´å·®çš„è®¤è¯†å‘ç”Ÿäº†åå·®ã€‚\nå‡è®¾è¿™ä¸¤ä¸ªäº‹ä»¶å¯¹åº”æ—¶é’Ÿ\n äº‹ä»¶1: èµ°åˆ°è¿™ä¸€ç§’ æ»´ äº‹ä»¶2: èµ°åˆ°ä¸‹ä¸€ç§’ å—’  é‚£ä¹ˆï¼Œç”±å‰é¢çš„ï¼ˆ6ï¼‰å¼å¯ä»¥çŸ¥é“ï¼Œåœ¨æˆ‘çš„æ—¶ç©ºåæ ‡ä¸­çš„ 1s ä¸ä½ çš„æ—¶ç©ºåæ ‡ä¸­çš„ 1s ä¸å†ç›¸åŒã€‚ å¦‚æœæˆ‘ä»¬å¿½ç•¥ä¸¤ä¸ªèµ°é’ˆçš„è·ç¦»ï¼Œå³$\\Delta x= 0$ (å¯¹åº”å…¶ä»–äº‹ä»¶ï¼Œå³åŒä¸€åœ°ç‚¹å‘ç”Ÿçš„ä¸¤ä¸ªäº‹ä»¶ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ° $$ \\Delta t' = \\frac{\\Delta t }{\\sqrt{1-u^2/c^2}} $$ åœ¨ç›¸å¯¹æˆ‘è¿åŠ¨çš„ä½ çš„æ—¶ç©ºåæ ‡ä¸­ï¼Œä½ çš„1så˜é•¿äº†ã€‚ä¹Ÿå³ä½ çš„æ—¶é—´å»¶ç¼“äº†ã€‚\nå…¶å®ï¼Œè¿™é‡Œçš„é’Ÿå¯ä»¥æé«˜åˆ°æ›´åŠ æ™®éçš„èŒƒç•´ä¸Šï¼Œä»»ä½•ä¸¤ä¸ªäº‹ä»¶çš„æ—¶é—´å·®éƒ½ä¼šå˜é•¿ã€‚æ¯”å¦‚æˆ‘ä»¬çš„èº«ä½“çš„åŒ–å­¦ååº”çš„è¿›ç¨‹ç­‰ç­‰ï¼Œéƒ½è¢«å»¶ç¼“äº†ã€‚\nå­ªç”Ÿå­æ‚–è®º\nå¦‚æœæˆ‘æ´»äº†50å²ï¼Œä½ å¯èƒ½æ‰åˆ°20å²ã€‚\n å¦‚æœæˆ‘ç°åœ¨ç™»ä¸Šé£èˆ¹ï¼Œä»¥å¾ˆé«˜çš„é€Ÿåº¦è¿›è¡Œæ˜Ÿé™…æ—…è¡Œï¼Œé‚£ä¹ˆç­‰æˆ‘å›åˆ°åœ°çƒæ—¶ï¼Œä¼šå‘ç°ä½ æ¯”æˆ‘å¹´è½»ã€‚\nè€Œè¿™åœ¨ä½ çœ‹æ¥ï¼Œåˆ™ç›¸åï¼Œä½ ä¼šè®¤ä¸ºæˆ‘æ¯”ä½ å¹´è½»ã€‚è¿™å°±æ˜¯å­ªç”Ÿå­æ‚–è®ºã€‚\n äº‹å®ä¸Šï¼Œåœ°çƒä¸Šçš„ä½ çš„çœ‹æ³•æ‰æ˜¯å¯¹çš„ï¼Œç‹­ä¹‰ç›¸å¯¹è®ºåªå¯¹æƒ¯æ€§ç³»æœ‰åŒç­‰æ„ä¹‰ï¼Œå³åªæœ‰åœ¨æƒ¯æ€§ç³»ä¸­ä¸Šé¢çš„ç­‰å¼æ‰æˆç«‹ã€‚å¯¹äºåœ¨é£èˆ¹ä¸Šçš„ä½ ï¼Œå¿…å®šç»å†äº†ä¸€æ®µåŠ é€Ÿè¿‡ç¨‹ï¼ˆä½ çš„æ—¶ç©ºåæ ‡æ˜¯éæƒ¯æ€§ç³»ï¼‰ï¼Œåœ¨è¿™æ®µæ—¶é—´é‡Œï¼Œä¸èƒ½åˆ©ç”¨ä¸Šé¢çš„å…¬å¼è®¡ç®— æˆ‘å’Œä½ çš„æ—¶é—´å·®ã€‚\nè€Œè¦æ­£ç¡®è®¡ç®—æˆ‘å’Œä½ çš„å¹´é¾„å·®ï¼Œéœ€è¦åˆ©ç”¨ç§¯åˆ†è¿›è¡Œè®¡ç®—ã€‚\nå‚è€ƒï¼š ç»´åŸºç™¾ç§‘ï¼šåŒç”Ÿå­ä½¯è°¬\nåŒæ—¶æ€§çš„ç›¸å¯¹æ€§ #  ç”±ä¸Šé¢çš„å…¬å¼ï¼ˆ6ï¼‰æˆ‘ä»¬è¿˜å¯ä»¥å¾—åˆ° åŒæ—¶çš„æ¦‚å¿µæ˜¯ç›¸å¯¹çš„ ã€‚ $$ \\Delta t' = \\frac{\\Delta t - \\Delta x u/c^2}{\\sqrt{1-u^2/c^2}} $$ ä¸¤ä¸ªå¯¹æˆ‘æ¥è¯´åŒæ—¶å‘ç”Ÿçš„æ—¶é—´ï¼Œå³ $\\Delta t = 0$ ï¼Œå¯¹äºä½ æ¥è¯´\n å¦‚æœå¯¹æˆ‘æ¥è¯´ï¼Œå®ƒä»¬ä¸åŒåœ°å‘ç”Ÿ [5] ($\\Delta x \\not = 0$) ï¼Œåˆ™ $\\Delta t' \\not= 0$ ï¼Œå³å¯¹æˆ‘åŒæ—¶å‘ç”Ÿçš„ä¸¤ä¸ªäº‹ä»¶ï¼Œå¯¹ä½ æ¥è¯´ä¸åŒæ—¶å‘ç”Ÿã€‚ è€Œå¦‚æœå¯¹æˆ‘æ¥è¯´æ˜¯åŒæ—¶åŒåœ°å‘ç”Ÿçš„äº‹ä»¶ï¼Œå¯¹ä½ æ¥è¯´ä¹Ÿæ˜¯åŒæ—¶åŒåœ°å‘ç”Ÿçš„ã€‚[6]  é€Ÿåº¦çš„å…³ç³» #  ç”±æ´›ä¼¦å…¹å˜æ¢è¿˜èƒ½æ¨å¯¼å‡ºï¼Œåœ¨æˆ‘çš„æ—¶ç©ºåæ ‡ä¸­çš„é€Ÿåº¦ä¸ä½ çš„æ—¶ç©ºåæ ‡ä¸­çš„é€Ÿåº¦ä¹‹é—´çš„å…³ç³»ã€‚ $$ v' = \\frac{\\Delta x'}{\\Delta t'} = \\frac{\\Delta x -u\\Delta t }{\\Delta t - \\Delta x u/c^2} = \\frac{\\Delta x / \\Delta t -u }{1 - \\Delta x/\\Delta t u/c^2} = \\frac{v -u }{1 - v u/c^2} $$\né™„å½• #  [1] æ ¹æ®ç›¸å¯¹æ€§åŸç†ï¼Œåœ¨æ‰€æœ‰æƒ¯æ€§ç³»ä¸­ç‰©ç†å®šå¾‹éƒ½å…·æœ‰ç›¸åŒçš„å½¢å¼ï¼Œæ•…ä¿®æ­£ç³»æ•° $\\gamma$ å¯¹ä»»ä½•äººéƒ½åº”ç›¸åŒã€‚\n[2]\n[3]\n[4]\n[5] ç”±å…¬å¼5ï¼Œå¯¹äºä½ ä¹Ÿä¸€å®šä¸åŒåœ°\n[6] å¯¹äºè¿™ä¸ªï¼Œæˆ‘ä»¬è®¾æƒ³ä¸€ä¸ªäº‹ä»¶ä¸¤è¾†è½¦ç›¸æ’ï¼Œå®ƒå¯ä»¥åˆ†è§£ä¸ºä¸¤ä¸ªäº‹ä»¶ï¼Œä¸€ä¸ªæ˜¯ä¸€è¾†è½¦åœ¨æŸä¸ªæ—¶åˆ»åˆ°æŸä¸ªåœ°ç‚¹ï¼Œå¦ä¸€ä¸ªæ˜¯å¦ä¸€è¾†è½¦åœ¨åŒä¸€æ—¶åˆ»åˆ°åŒä¸€åœ°ç‚¹ã€‚è¿™æ ·å®ƒä»¬æ‰èƒ½ç›¸æ’ã€‚æ‰€ä»¥å¦‚æœå¯¹æˆ‘æ¥è¯´åŒæ—¶åŒåœ°å‘ç”Ÿçš„è¿™ä¸¤ä¸ªäº‹ä»¶ï¼Œå¯¹ä½ æ¥è¯´ä¸æ˜¯åŒæ—¶åŒåœ°çš„ï¼Œé‚£ä¹ˆå¯¹ä½ æ¥è¯´è½¦ä¸ä¼šç›¸æ’ï¼Œè€Œè½¦ç›¸æ’æ˜¯å®¢è§‚å­˜åœ¨çš„ã€‚å› æ­¤å¯¹æˆ‘æ¥è¯´æ˜¯åŒæ—¶åŒåœ°å‘ç”Ÿçš„äº‹ä»¶ï¼Œå¯¹ä½ æ¥è¯´ä¹Ÿæ˜¯åŒæ—¶åŒåœ°å‘ç”Ÿçš„ã€‚\nå››ç»´ç©ºé—´çš„ç†è§£ #  åœ¨äºŒç»´åæ ‡ä¸­ï¼Œä¸€ä¸ªç‚¹(x,y)ï¼Œä¸è®ºæ€ä¹ˆåšå˜æ¢ï¼Œæ—‹è½¬ï¼Œå¹³ç§»ç­‰ç­‰ï¼Œéƒ½åªä¼šæ¶‰åŠx,yã€‚ç”¨ä¸¤ä¸ªåæ ‡å°±èƒ½å¤Ÿæè¿°è¿™ä¸ªäºŒç»´ä¸–ç•Œã€‚\nåœ¨ä¸‰ç»´ä¸–ç•Œï¼Œæˆ‘ä»¬æè¿°ä¸€ä¸ªäº‹ä»¶ï¼Œéœ€è¦ç”¨åˆ°ï¼ˆx,y,z,tï¼‰ï¼Œæˆ‘ä»¬æ€ä¹ˆ\u0026hellip;\nå¾…ç»­.\n"},{"id":23,"href":"/blog/posts/algorithms/Fibonacci%E6%95%B0%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95/","title":"Fibonacciæ•°çš„è¿­ä»£ç®—æ³•","section":"Posts","content":"ä½¿ç”¨è¿­ä»£ç®—æ³•æ±‚æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Œ\næ—¶é—´å¤æ‚åº¦O(n)ï¼Œç©ºé—´å¤æ‚åº¦O(1)ã€‚\nint f(int n) { int f1 = 1, f2= 0; while(--n){ f1 = f1+f2; // f(n) = f(n-1)+f(n-2)  f2 = f1-f2;\t// f(n-1) = f(n)-f(n-2)  } return f1; } "},{"id":24,"href":"/blog/posts/algorithms/%E5%BF%AB%E9%80%9F%E5%B9%82/","title":"å¿«é€Ÿå¹‚ç®—æ³•","section":"Posts","content":"åœ¨cï¼Œc++è¯­è¨€ä¸­ï¼Œå¹¶æ²¡æœ‰æä¾›æ±‚å¹‚çš„åŸºæœ¬è¿ç®—ï¼Œé€šå¸¸æˆ‘ä»¬éœ€è¦è‡ªå·±å†™å‡½æ•°æˆ–è€…è°ƒç”¨STLæä¾›çš„å‡½æ•°ã€‚\nä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å†™çš„æ±‚å¹‚å‡½æ•°åŸºæœ¬ä¸Šéƒ½æ˜¯å¾ªç¯ç´¯ä¹˜ï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(n)ã€‚è™½è¯´æ˜¯çº¿æ€§çš„æ—¶é—´å¤æ‚åº¦ï¼Œä½†æ±‚å¹‚è¿ç®—ä½œä¸ºåŸºç¡€è¿ç®—ï¼Œå¾€å¾€è°ƒç”¨é¢‘ç¹ï¼Œè¿™æ—¶å€™å³ä½¿æ˜¯çº¿æ€§çš„æ—¶é—´å¤æ‚åº¦ä¹Ÿå°†å˜å¾—éš¾ä¹Ÿæ¥å—ã€‚\nåˆ©ç”¨å¿«é€Ÿå¹‚å¯ä»¥å¿«é€Ÿè®¡ç®—åº•æ•°çš„næ¬¡å¹‚ã€‚å…¶æ—¶é—´å¤æ‚åº¦ä¸º O(logn)\nåŸç† #  ä»¥æ±‚ $a^{11}$ ä¸ºä¾‹ï¼Œå°†11å†™æˆäºŒè¿›åˆ¶å½¢å¼ 1011ã€‚\nåˆ™ $a^{11} = a^{2^0+2^1+2^3}=a^{1+2+8} =a^1\\times a^2\\times a^8$\nä½¿ç”¨ä¸€ä¸ªç´¯ä¹˜å™¨ï¼Œæ¯æ¬¡ç¿»å€ base = base*base é€æ­¥å¾—åˆ° $a^1,a^2,a^4,a^8$ ï¼Œæ ¹æ®11çš„äºŒè¿›åˆ¶ï¼Œå¦‚æœä¸º1åˆ™ä¹˜è¿›ç»“æœé‡Œã€‚\nå®ç° #  è¿­ä»£ç®—æ³•\nint power(int a,int b) { int base = 1,ans = a; while(a \u0026gt; 0){ if(a \u0026amp; 1 == 1) ans *= base;//äºŒè¿›åˆ¶ä½ä¸º1çš„æ‰è¦ä¹˜  base *= base; a \u0026gt;\u0026gt;= 1; } return ans; } é€’å½’ç®—æ³•\nint power(int a,int b) { if(b == 1) return a; int temp = power(a,b\u0026gt;\u0026gt;1) * power(a,b\u0026gt;\u0026gt;1); return (b\u0026amp;1 == 1 ? a:1) * temp * temp; // è‹¥æŒ‡æ•°ä¸ºå¶æ•°åˆ™åˆ†è§£æˆä¸€åŠï¼Œè‹¥ä¸ºå¥‡æ•°ï¼Œåˆ™è¿˜è¦å†ä¹˜a } é€’å½’ç®—æ³•ä¸è¿­ä»£ç®—æ³•çš„æ€è·¯ç•¥æœ‰ä¸åŒã€‚\n"},{"id":25,"href":"/blog/posts/algorithms/%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9A%84%E5%A6%99%E7%94%A8/","title":"ä½è¿ç®—çš„å¦™ç”¨","section":"Posts","content":"å¯¹äºä¸€äº›ç‰¹å®šé—®é¢˜ï¼Œå·§å¦™è¿ç”¨ä½è¿ç®—èƒ½ä½¿è§£æ³•å¼‚å¸¸ç®€æ´å’Œé«˜æ•ˆï¼ŒåŒæ—¶ï¼Œé€‚å½“è¿ç”¨ä½è¿ç®—ä¹Ÿèƒ½å¯¹ç¨‹åºè¿›è¡Œä¼˜åŒ–ã€‚\nè¿ç”¨çš„æ—¶å€™ï¼Œå¯èƒ½æ¶‰åŠå¤šç§ä½è¿ç®—ã€‚\nè®¡ç®—æœºä¸­ä½è¿ç®—åˆ†ä¸ºä»¥ä¸‹å…­ç§ï¼š\n ä¸ \u0026amp; æˆ– | é ï½ å¼‚æˆ– ^ å·¦ç§» \u0026laquo; å³ç§» \u0026raquo;  å¼‚æˆ– #  å¼‚æˆ–å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼ˆå¯èƒ½è¿˜æœ‰ï¼Œä¸‹åŒï¼‰ï¼š\n a^b = b^a a^a = 0 a^0 = a  å®ä¾‹ #  [leetcode 136 single number]\n Given an array of integers, every element appears twice except for one. Find that single one.\nNote: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?\n **é¢˜ç›®å¤§æ„ï¼š**æ¯ä¸ªå…ƒç´ éƒ½å‡ºç°ä¸¤æ¬¡ï¼Œä½†æœ‰ä¸€ä¸ªåªå‡ºç°ä¸€æ¬¡ï¼Œæ‰¾å‡ºå‡ºç°ä¸€æ¬¡çš„å…ƒç´ ã€‚\n è¿™é¢˜å¦‚æœç”¨æ ‡è®°æ•°ç»„åš æ—¶é—´å¤æ‚åº¦ï¼Œç©ºé—´å¤æ‚åº¦éƒ½æ˜¯ O(n) ã€‚ä½†å¦‚æœåˆ©ç”¨å¼‚æˆ–çš„å‰ä¸¤ä¸ªæ€§è´¨ï¼Œåˆ™å¯ä»¥å°†ç©ºé—´å¤æ‚åº¦å‹ç¼©åˆ°O(1)ã€‚\nclass Solution { public: int singleNumber(vector\u0026lt;int\u0026gt;\u0026amp; nums) { if(nums.size() == 0) return 0; int result = nums[0]; for(int i=1;i\u0026lt;nums.size();i++) result = result ^ nums[i]; return result; } }; ä¸æˆ– #  ä¸è¿ç®—å¯ä»¥å¿«é€Ÿå–å¾—ä¸€ä¸ªå˜é‡æŸä¸ª bitä½ çš„æ•°å€¼ã€‚\nå¦‚ï¼š 0010 \u0026amp; 1110 = 0010 0010 \u0026amp; 1101 = 0000\nå¦‚æœ 0010 \u0026amp; b = 0010 åˆ™è¡¨æ˜ b çš„ç¬¬äºŒä½æ˜¯1ï¼Œä¸ç­‰åˆ™ä¸º0ã€‚\n ä¸è¿ç®—å¯ä»¥å¿«é€Ÿå°†æŸbitä¸ºå¿«é€Ÿç½®ä¸º0ï¼Œè€Œæˆ–è¿ç®—å¯ä»¥å¿«é€Ÿå°†æŸbitä½ç½®ä¸º1ã€‚\nä¸è¿ç®—çš„å…¶ä»–ç”¨æ³•\n n \u0026amp; (n-1) å¯ä»¥æŠŠnçš„æœ€ä½ä½ç½®0  å·¦ç§»å³ç§» #  å·¦ç§»ç›¸å½“äºä¹˜2ï¼Œå³ç§»ç›¸å½“äºé™¤2ï¼Œå¹¶ä¸”å®ƒä»¬çš„é€Ÿåº¦æ¯”ä¹˜é™¤è¦å¿«ã€‚\næ‰€ä»¥å½“ç¨‹åºä¸­å‡ºç°å¤§é‡âœ–ï¸2ï¼Œæˆ–â—2è¿ç®—æ—¶ï¼Œå¯ä»¥ç”¨å·¦ç§»å’Œå³ç§»è¿›è¡Œä¼˜åŒ–ã€‚\nä¾‹å­ #   LeetCode Bit Manipulation\n"},{"id":26,"href":"/blog/posts/algorithms/%E7%AD%9B%E6%B3%95%E6%B1%82%E7%B4%A0%E6%95%B0/","title":"ç­›æ³•æ±‚ç´ æ•°","section":"Posts","content":" å‡è®¾è¦æ±‚nä»¥å†…çš„ç´ æ•°\n ç­›æ³•æ±‚ç´ æ•°æ˜¯ç”¨ä¸€ä¸ªå¤§å°ä¸ºnçš„æ•°ç»„ï¼Œä½œä¸ºæ ‡è®°æ•°ç»„ï¼Œå¦‚æœæ²¡è¢«æ ‡è®°åˆ°åˆ™ä¸ºç´ æ•°ã€‚\nå¼€å§‹å‡ä¸ºæœªæ ‡è®°ã€‚\nä»2å¼€å§‹ï¼Œ2æ²¡è¢«æ ‡è®°ï¼Œå°†2å­˜å…¥ä¸€ä¸ªå­˜ç´ æ•°çš„åœ°æ–¹ï¼Œç„¶åç­›æ‰å°äºnçš„ï¼Œ2çš„æ‰€æœ‰å€æ•°ã€‚ç„¶åæ˜¯3ï¼Œç­›æ‰3çš„æ‰€æœ‰å€æ•°ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°n-1ã€‚\nä¼˜åŒ– #  ä¸Šé¢çš„åšæ³•ï¼ŒåŒä¸€ä¸ªæ•°å¯èƒ½ä¼šè¢«ç­›æ‰å¤šæ¬¡ï¼Œæ¯”å¦‚6ä¼šè¢«3å’Œ2å„ç­›ä¸€æ¬¡ã€‚\nä¸ºäº†æé«˜æ•ˆç‡ï¼Œéœ€è¦è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å¾—æ¯ä¸ªæ•°å°½å¯èƒ½çš„è¢«å°‘ç­›ï¼Œå¦‚æœèƒ½ä¸€æ¬¡æœ€å¥½ã€‚\nè€ƒè™‘åˆ°ä»»ä½•åˆæ•°éƒ½å¯ä»¥åˆ†è§£æˆè‹¥å¹²ä¸ªç´ æ•°çš„ä¹˜ç§¯ã€‚åœ¨ç­›æ‰åˆæ•°çš„è¿‡ç¨‹ä¸­ï¼Œæœ€å¥½çš„æ˜¯è®©æ¯ä¸ªåˆæ•°åªè¢«å®ƒæœ€å°çš„å› å­ç­›æ‰ã€‚\nå¦‚24 18 éƒ½åªè¢«2ç­›æ‰\nC++å®ç° #  int countPrimes(int n) { vector\u0026lt;bool\u0026gt; vis(n,false); vector\u0026lt;int\u0026gt; prime; for(int i=2;i\u0026lt;n;i++){ if(!vis[i]) prime.push_back(i); for(int j=0;j\u0026lt;prime.size() \u0026amp;\u0026amp; i*prime[j]\u0026lt;=n;j++){ vis[i*prime[j]] = true; if(i%prime[j] == 0) break;\t//ä¼˜åŒ–  } } return prime.size(); } æœ€å¤–å±‚å¾ªç¯æ¯æ¬¡å¾ªç¯ï¼Œéƒ½èƒ½å¾—åˆ°å°äºç­‰äºiçš„æ‰€æœ‰ç´ æ•°ï¼Œå½“è¦æ±‚i+1å†…çš„ç´ æ•°æ—¶ï¼Œåªéœ€åˆ¤æ–­i+1æ˜¯å¦åœ¨ä¹‹å‰è¢«ç­›æ‰ã€‚\nä¸æ­¤åŒæ—¶ï¼Œå°†å½“å‰æ‰€æœ‰ç´ æ•°çš„i+1å€ç­›æ‰ã€‚\né‚£åé¢å‡ºç°çš„ç´ æ•°çš„i+1å€ï¼Œè®¾ä¸ºm ï¼Œä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿ\n å¦‚æœi+1æ˜¯ç´ æ•°ï¼Œmä¼šè¢« i+1 ç­›æ‰ å¦‚æœi+1ä¸æ˜¯ç´ æ•°ï¼Œåˆ™m ä¼šè¢«i+1çš„æœ€å°è´¨å› æ•°ï¼ˆä¹‹å‰å‡ºç°è¿‡çš„ç´ æ•°ä¸­çš„æŸä¸€ä¸ªï¼‰ç­›æ‰ã€‚  **ä¼˜åŒ–ç‚¹ï¼š**æ¯ä¸ªæ•°éƒ½è¢«å®ƒæœ€å°çš„å› æ•°ç­›æ‰\nå…·ä½“æ“ä½œï¼šå¦‚æœå¾ªç¯åˆ°æŸä¸ªç´ æ•° prime[j] æ˜¯ i+1çš„å€æ•°æ—¶ï¼Œåé¢çš„ç´ æ•°çš„i+1å€ prime[j+1] * (i+1) å°±ä¸ç”¨ç­›äº†ã€‚\nåŸå› åœ¨äºï¼šåé¢ç´ æ•°çš„i+1å€ä¸€å®šä¼šè¢« prime[j] ï¼ˆæ›´å°çš„ä¸€ä¸ªæ•°ï¼‰ç­›æ‰ï¼Œ\n$prime[j+1] * (i+1) = prime[j+1] * prime[j] * k$\n$k = (i+1) / prime[j]$\n"},{"id":27,"href":"/blog/docs/notes/paper/arbrcan/","title":"ArbSR: ä»»æ„å°ºåº¦è¶…åˆ† - é˜…è¯»ç¬”è®°","section":"Paper","content":"Created: August, 12, 2021 è®ºæ–‡: Learning A Single Network for Scale-Arbitrary Super-Resolution\nCode: https://github.com/LongguangWang/ArbSR\n "},{"id":28,"href":"/blog/docs/gen/introduction/","title":"Introduction","section":"AIGC","content":"Created: March, 23, 2023 å¯¹äºç”Ÿæˆå¼æ¨¡å‹æ¥è¯´ï¼Œæˆ‘ä»¬è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜å°±æ˜¯å¦‚ä½•æ„é€ ä¸€ä¸ª highly flexible families of probability distribution å»æ‹ŸåˆçœŸå®çš„æ•°æ®åˆ†å¸ƒï¼Œå¹¶ä¸”æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªåˆ†å¸ƒå®¶æ—å¯¹äºï¼Œ\n learningï¼šé€šè¿‡æ•°æ®å­¦ä¹ åˆ†å¸ƒå½¢å¼ã€‚ samplingï¼šä»åˆ†å¸ƒä¸­å–æ ·ã€‚ inferenceï¼šè®¡ç®—æ¡ä»¶æ¦‚ç‡ $p(x|y)$ã€‚ evaluationï¼šè®¡ç®—ç»™å®šæ ·æœ¬å‡ºç°çš„æ¦‚ç‡ $p(x)$ã€‚  è¿™å››ä¸ªä»»åŠ¡ï¼Œéƒ½æ˜¯æˆ–å°½å¯èƒ½éƒ½æ˜¯ analytically æˆ–è€… computationally tractable [1]_ çš„ :raw-latex:\\parencite{sohl2015deep}ã€‚\nä¸ºäº†è¿™ä¸ªç›®æ ‡ï¼Œäººä»¬æå‡ºäº†å„ç§å„æ ·çš„æ¨¡å‹ï¼Œæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•çš„æ ¸å¿ƒæ€è·¯åŸºæœ¬å°±æ˜¯ï¼Œåœ¨ä¿è¯ tractable çš„å‰æä¸‹ï¼Œå°½å¯èƒ½çš„æå‡flexibilityï¼Œä»è€Œæ›´å¥½çš„æ‹Ÿåˆå¤æ‚çš„çœŸå®æ•°æ®åˆ†å¸ƒã€‚å¸¸è§çš„æ¨¡å‹åŒ…æ‹¬ï¼šMixture Gaussian, VAE, Normalizing Flow, DDPM, GAN ç­‰ç­‰ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œä¸Šè¿°çš„å››ä¸ªä»»åŠ¡å¹¶ä¸æ˜¯éƒ½èƒ½å¤Ÿè§£çš„ï¼Œå¤§éƒ¨åˆ†éƒ½åªèƒ½è§£å†³å…¶ä¸­ä¸€ä¸ªæˆ–å‡ ä¸ªï¼ˆä¾‹å¦‚GANï¼Œæˆ‘ä»¬åªèƒ½åš learning å’Œ samplingï¼‰ã€‚\nThink from Scratch #  ä»¥ä¸Šçš„ä»‹ç»å¯èƒ½è¿‡äºæŠ½è±¡ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ä»é›¶å¼€å§‹æ€è€ƒï¼Œå¦‚æœç°åœ¨æˆ‘ä»¬æ‹¥æœ‰ä¸€å †æ•°æ®ï¼Œè¿™äº›æ•°æ®æœ‰å¾ˆå¤šæ ·æœ¬ï¼Œæ¯”å¦‚è¯´æˆ‘ä»¬æœ‰ N ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ä¸€ä¸ª D ç»´çš„å‘é‡ï¼Œæˆ‘ä»¬è¦å¦‚ä½•å­¦ä¹ è¿™äº›æ•°æ®çš„åˆ†å¸ƒå‘¢ï¼Ÿ\næƒ³è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»ç†è§£ â€åˆ†å¸ƒâ€œ æ˜¯ä»€ä¹ˆï¼Ÿåˆ†å¸ƒå…¶å®å°±æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œæ¦‚ç‡åˆ†å¸ƒå°±æ˜¯ä¸€ä¸ªéšæœºå˜é‡ä¸åŒå–å€¼çš„æ¦‚ç‡æ„æˆçš„ä¸€ä¸ªå‡½æ•°ã€‚å¯¹äºä¸€ä¸ªè¿ç»­çš„éšæœºå˜é‡ï¼Œå•ä¸ªå–å€¼çš„æ¦‚ç‡æ²¡æœ‰æ„ä¹‰ï¼Œè¿™æ—¶å€™ï¼Œåˆ†å¸ƒå…¶å®æ˜¯ä»£æŒ‡æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚\nè¿™æ—¶å€™ï¼Œæˆ‘ä»¬åº”è¯¥çŸ¥é“ï¼Œå¯¹äºå‰é¢æåˆ°çš„æ•°æ®ï¼Œå¦‚æœæˆ‘æŠŠæ•°æ®ä¸­çš„æ ·æœ¬è§†ä½œä¸€ä¸ª D ç»´çš„éšæœºå˜é‡ï¼Œæˆ‘ä»¬å®é™…ä¸Šå°±æ˜¯æƒ³è¦å­¦ä¹ è¿™ä¸ªéšæœºå˜é‡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚\nè¿™æ—¶å€™ï¼Œåˆå¼•å‡ºäº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œè¿™ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°çš„å½¢å¼æ˜¯ä»€ä¹ˆæ ·çš„å‘¢ ï¼Ÿå¦‚æœæˆ‘ä»¬çŸ¥é“æ¦‚ç‡å¯†åº¦å‡½æ•°çš„è§£æå½¢å¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæ ¹æ®ç»™å®šçš„æ•°æ®ï¼Œä¼°è®¡å‡ºè¿™ä¸ªåˆ†å¸ƒçš„æœªçŸ¥å‚æ•°äº†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾æ•°æ®æœä»é«˜æ–¯åˆ†å¸ƒï¼Œä½†æ˜¯å‡å€¼æ–¹å·®æœªçŸ¥ï¼Œé€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°è¿™ä¸ªåˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚\næ•°æ®åˆ†å¸ƒçš„å½¢å¼çš„é€‰å–æ˜¯éœ€è¦æ ¹æ®æˆ‘ä»¬çš„å…ˆéªŒçŸ¥è¯†å†³å®šçš„ï¼Œä½†å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¿™äº›å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬ä¹Ÿä¸æƒ³è‡ªå·±å†³å®šã€‚å¦ä¸€æ–¹é¢ï¼Œå³ä¾¿æˆ‘ä»¬æœ‰ä¸€äº›å…ˆéªŒçŸ¥è¯†ï¼Œä½†æ˜¯ç°å®ä¸­å¯ä¾›é€‰å–çš„ family of distribution æ˜¯æœ‰é™çš„ï¼Œè¿™äº›åˆ†å¸ƒå¹¶ä¸ä¸€å®šèƒ½å¤Ÿå»ºæ¨¡å¤æ‚çš„çœŸå®æ•°æ®ã€‚\nLatent Variable Model #  ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šå¸¸è€ƒè™‘ä½¿ç”¨ Latent Variable Model å¯¹æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå³æˆ‘ä»¬å¸Œæœ›ä»ä¸€ä¸ªknown well-defined distribution å‡ºå‘ï¼Œé€šè¿‡å­¦ä¹ ä¸€ä¸ªåˆ†å¸ƒçš„æ˜ å°„ï¼Œå°†è¿™ä¸ªå·²çŸ¥åˆ†å¸ƒæ˜ å°„åˆ°æœªçŸ¥çš„æ•°æ®åˆ†å¸ƒã€‚\nModeling #  å…·ä½“çš„ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª q ç»´çš„ latent space $\\mathbb{R}^{q}$ å’Œ n ç»´çš„ data space $\\mathbb{R}^{n}$ï¼Œæˆ‘ä»¬äº‹å®ä¸Šæ˜¯å¸Œæœ›å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå°† latent space é‡Œçš„ sampleï¼ˆæ ·æœ¬ï¼‰ æ˜ å°„åˆ° data space é‡Œçš„å‡½æ•°ï¼Œ$g: \\mathbb{R}^{q} \\rightarrow \\mathbb{R}^{n}$ã€‚\nå¦‚æœè¿™ä¸ªå‡½æ•°æ˜¯ deterministic çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¾—åˆ°äº† GAN è¿™ä¸ªæ¨¡å‹ã€‚ç‰¹åˆ«çš„ï¼Œæˆ‘ä»¬è¦æ±‚ data space é‡Œçš„æ¯ä¸ªæ ·æœ¬ï¼Œéƒ½è‡³å°‘æœ‰ä¸€ä¸ª latent space é‡Œçš„æ ·æœ¬ä¸ä¹‹å¯¹åº”ã€‚\nå¦‚æœè¿™ä¸ªå‡½æ•°æ˜¯ stochastic çš„ï¼Œé‚£ä¹ˆæˆ‘å°±å¾—åˆ°äº† VAEï¼ŒDDPM ç­‰æ¨¡å‹ã€‚è¿™æ—¶å€™ï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ çš„å‡½æ•°å®é™…ä¸Šå˜æˆäº†ä¸€ä¸ªåéªŒæ¦‚ç‡åˆ†å¸ƒ :math:p_{g}(\\mathbf{x} \\mid \\mathbf{z})\\ ã€‚æˆ‘ä»¬é€šå¸¸ä¼šé€‰å–å…·æœ‰å…·ä½“å½¢å¼çš„åˆ†å¸ƒå¯¹ :math:p_{g}(\\mathbf{x} \\mid \\mathbf{z}) è¿›è¡Œå»ºæ¨¡ï¼ˆä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œæˆ‘ä»¬å­¦ä¹ çš„å‡½æ•°å®é™…ä¸Šå°±å˜æˆé¢„æµ‹è¿™äº›åˆ†å¸ƒçš„æœªçŸ¥å‚æ•°ã€‚\nè¿›ä¸€æ­¥çš„ï¼Œåœ¨å­¦ä¹ åˆ°åéªŒæ¦‚ç‡åˆ†å¸ƒ :math:p_{g}(\\mathbf{x} \\mid \\mathbf{z})\\ ï¼Œç»“åˆå·²çŸ¥çš„å…ˆéªŒåˆ†å¸ƒ :math:p_{\\mathcal{Z}}(\\mathbf{z})\\ ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥é€šè¿‡å…¨æ¦‚ç‡å…¬å¼ï¼Œæ±‚å‡ºæœªçŸ¥çš„æ•°æ®åˆ†å¸ƒï¼Œå¦‚ä¸‹ï¼š\n$$ p_{\\mathcal{X}}(\\mathbf{x})=\\int p_{g}(\\mathbf{x} \\mid \\mathbf{z}) p_{\\mathcal{Z}}(\\mathbf{z}) d \\mathbf{z} \\label{eq-intro-px} $$\nLearning #  ç›´è§‚çš„ï¼Œå¦‚æœæˆ‘ä»¬å®šä¹‰å¥½åéªŒæ¦‚ç‡åˆ†å¸ƒ :math:p_{g}(\\mathbf{x} \\mid \\mathbf{z}) å’Œå…ˆéªŒåˆ†å¸ƒ :math:p_{\\mathcal{Z}}(\\mathbf{z}) ä¹‹åï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿé€šè¿‡å…¬å¼ [eq-intro-px] \u0026lt;#eq-intro-px\u0026gt;__ è®¡ç®—å‡º :math:p(x)\\ ï¼Œç„¶åæˆ‘ä»¬é€šè¿‡æå¤§ä¼¼ç„¶ä¼°è®¡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå³å¯ä¼°è®¡å‡ºåéªŒæ¦‚ç‡åˆ†å¸ƒä¸­æœªçŸ¥çš„å‚æ•°äº†ã€‚\n.. math::\n\\prod_{x\\in X}^X p(x) \\label{eq-intro-loglikelihood}\nä½†æ˜¯ï¼è¿™ä¹ˆåšå®é™…ä¸Šæœ‰ä¸€ä¸ªå¾ˆå¤§çš„é—®é¢˜ã€‚ç”±äºç§¯åˆ†çš„å­˜åœ¨ï¼Œå…¬å¼ [eq-intro-px] \u0026lt;#eq-intro-px\u0026gt;__ é€šå¸¸æ˜¯ï¼ˆå¤§éƒ¨åˆ†æ—¶å€™éƒ½æ˜¯ï¼‰Intractableçš„ã€‚ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿ\n  é¦–å…ˆï¼Œå¦‚æœç§¯åˆ†å¯ä»¥è§£æç®—å‡ºæ¥ï¼Œé‚£è‚¯å®šæ²¡é—®é¢˜ã€‚\n  å¦‚æœä¸è¡Œï¼Œæœ€å¸¸è§çš„åšæ³•æ˜¯è’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œå³å¯¹ :math:z éšæœºå–æ ·ï¼Œç„¶åè®¡ç®—è¢«ç§¯å‡½æ•°ï¼Œç”¨å…¶å¹³å‡å€¼æ›¿ä»£ç§¯åˆ†ã€‚å–æ ·è¶Šå¤šä¼°è®¡è¶Šå‡†ã€‚è§å…¬å¼ [eq-monte-carlo-px] \u0026lt;#eq-monte-carlo-px\u0026gt;__\\ ã€‚\n.. math::\nP(x) \\approx \\frac{1}{N} \\sum_{z_n\\sim P(Z)} P(x|z_n) \\label{eq-monte-carlo-px}    é—®é¢˜åœ¨ï¼Œå¯¹äºä¸€ä¸ªé«˜ç»´çš„ latent variable :math:z ä»¥åŠçœŸå®æ•°æ®æ¥è¯´ï¼Œå¤§éƒ¨åˆ†çš„ :math:p(x|z) éƒ½å¯èƒ½ä¼šæ˜¯é›¶ï¼Œæˆ–éå¸¸å°ï¼Œå³å¤§éƒ¨åˆ†çš„ z éƒ½ä¸å¤ªå¯èƒ½ç”Ÿæˆ çœŸå®æ•°æ®ä¸­çš„æ ·æœ¬ x [2]_ï¼Œä½†æ˜¯ï¼Œè¿˜æ˜¯æœ‰å¯èƒ½æœ‰ z æ˜¯å¯èƒ½äº§ç”ŸçœŸå®æ•°æ®ä¸­çš„æ ·æœ¬ x çš„ï¼Œå³ :math:p(x|z) æ¯”è¾ƒå¤§ã€‚å› æ­¤ï¼Œä¸ºäº†å¾—åˆ°ä¸€ä¸ªå‡†ç¡®ä¼°è®¡ï¼Œæˆ‘ä»¬å¿…é¡»è¦å–æ ·éå¸¸å¤šçš„ :math:z\\ ï¼Œè€Œè¿™å¯¹ä¸€ä¸ªé«˜ç»´çš„ :math:z æ¥è¯´ï¼Œå°†ä¼šæ˜¯ä¸€ä¸ªæˆ‘ä»¬ä¸èƒ½æ¥å—çš„æ•°å­—ã€‚\n  ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šå¸¸çš„åšæ³•æ˜¯ä¼˜åŒ–å…¬å¼ [eq-intro-loglikelihood] \u0026lt;#eq-intro-loglikelihood\u0026gt;__ çš„å¯è§£çš„ä¸€ä¸ªå˜åˆ†ä¸‹ç•Œ VLBO [3]ï¼ˆé€šå¸¸ä¹Ÿè¢«ç§°ä¸ºELBO [4]ï¼‰ï¼Œåšä¸€ä¸ªè¿‘ä¼¼çš„ä¼¼ç„¶ä¼°è®¡ï¼Œå¦‚ä¸‹ï¼š\n.. math::\n\\begin{aligned} log P(x) \u0026amp; = log\\int P(x|z)P(z) dz \\ \u0026amp; = log\\int \\frac{Q(z|x)P(x|z)P(z)}{Q(z|x)} dz \\ \u0026amp; = logE_{z\\sim Q}[\\frac{P(x|z)P(z)}{Q(z|x)}] \\ \u0026amp; \\geq E_{z\\sim Q}[log\\frac{P(x|z)P(z)}{Q(z|x)}] \\ \u0026amp; = E_{z\\sim Q}[logP_\\theta(x|z)] - D_{KL}[Q_\\theta(z|x) | P(z)] \\end{aligned} \\label{eq-vae-elbo}\næ€ä¹ˆç†è§£è¿™ä¸ªä¸‹ç•Œï¼Œä¸ºä»€ä¹ˆä¼šæƒ³åˆ°è½¬æ¢æˆè¿™ä¸ªå½¢å¼å‘¢ï¼Ÿè¿™é‡Œæä¾›ä¸€ç§è§£é‡Šã€‚å›å¿†åˆšåˆšæˆ‘ä»¬é‡åˆ°çš„é—®é¢˜ï¼Œä¸»è¦æ˜¯å…¬å¼ [eq-intro-px] \u0026lt;#eq-intro-px\u0026gt;__ ä¸­çš„ç§¯åˆ†æ²¡æ³•æœ‰æ•ˆä¼°è®¡ï¼Œå…¶åŸå› æ˜¯ä» :math:Z ä¸­é‡‡æ ·ï¼Œå‡»ä¸­è¢«ç§¯å‡½æ•° :math:P(x|z) æ¦‚ç‡å¤§çš„åœ°æ–¹çš„æ¦‚ç‡å¤ªå°ï¼Œå¾—åˆ°æœ‰æ•ˆä¼°è®¡éœ€è¦çš„é‡‡æ ·æ•°ç›®æ— æ³•æ¥å—ã€‚\næ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šæ˜¯æƒ³çŸ¥é“ :math:P(x|z) ç›¸å¯¹äº z çš„æœŸæœ›ï¼Œæˆ‘ä»¬ç°åœ¨æ˜¯ä» :math:P(z) ä¸­å–æ ·ä¼°è®¡è¿™ä¸ªæœŸæœ›ï¼Œä½†è¿™ä¹ˆåšå–æ ·å‡ºæ¥çš„ :math:z ç®—å‡ºæ¥çš„ :math:P(x|z) æ¦‚ç‡å¤ªå°äº†ã€‚\n.. math:: P(x) = \\int P(x|z)P(z) dz = E_{z\\sim Z}[P(x|z)]\næ€ä¹ˆè§£å†³å‘¢ï¼Ÿç›´è§‚çš„æ€è·¯å°±æ˜¯ï¼Œé‚£æˆ‘ä»¬å°±ä»ä¸€ä¸ªèƒ½è®© :math:P(x|z) æ¦‚ç‡å¤§çš„åˆ†å¸ƒå–ä¸€äº› :math:z å‡ºæ¥ã€‚ä»€ä¹ˆåˆ†å¸ƒèƒ½æ»¡è¶³è¿™ä¸ªè¦æ±‚å‘¢ï¼Ÿæœ€å¥½çš„å½“ç„¶æ˜¯ä» :math:P(z|x) ä¸­å–æ ·ï¼Œä½†çœŸå®çš„ :math:P(z|x) æ˜¯ä¸€ä¸ªæœªçŸ¥çš„åˆ†å¸ƒï¼Œå› æ­¤ï¼Œæˆ‘ä»¬é€€è€Œæ±‚å…¶æ¬¡ï¼Œä½¿ç”¨ä¸€ä¸ª :math:Q(z|x) å»è¿‘ä¼¼å®ƒï¼Œå³è¿™æ—¶å€™æˆ‘ä»¬å¸Œæœ›è¿™ä¹ˆç®— :math:P(x),\n.. math::\n\\begin{aligned} P(x) \u0026amp; = \\int P(x|z)P(z) dz \\ \u0026amp; = \\int \\frac{Q(z|x)P(x|z)P(z)}{Q(z|x)} dz \\ \u0026amp; = E_{z\\sim Q}[\\frac{P(x|z)P(z)}{Q(z|x)}] \\end{aligned}\nè¿™æ—¶å€™è’™ç‰¹å¡æ´›ä¼°è®¡å°±ä¼šæ›´å‡†äº†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n.. math:: P(x) \\approx \\frac{1}{N} \\sum_{z_n\\sim Q(Z|x)} P(x|z_n) P(z_n) / Q(z_n|x)\nè¿˜æ²¡å®Œï¼Œé€šå¸¸ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šç”¨ log ä¼¼ç„¶ä¼°è®¡ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥æŠŠæ¦‚ç‡çš„è¿ä¹˜å˜æˆè¿åŠ ã€‚å³è¿™æ—¶å€™ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¿™ä¸ªå‡½æ•°ï¼š\n.. math:: log P(x) = log E_{z\\sim Q}[\\frac{P(x|z)P(z)}{Q(z|x)}]\né‚£æˆ‘ä»¬è¦æ€ä¹ˆæ±‚è§£ :math:log P(x) å‘¢ï¼Ÿå¯¹äº :math:P(x)\\ ï¼Œå®ƒç­‰ä»·äºä¸€ä¸ªæœŸæœ›ï¼Œæˆ‘ä»¬ç›´æ¥è’™ç‰¹å¡æ´›ä¼°è®¡æ²¡ä»€ä¹ˆé—®é¢˜ã€‚ä½†æ˜¯å¯¹äº :math:log P(x) å‘¢ï¼Ÿç›´è§‰æ¥çœ‹ï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½è’™ç‰¹å¡æ´›ä¼°è®¡æ±‚å‡º :math:E_{z\\sim Q}[\\frac{P(x|z)P(z)}{Q(z|x)}] ç„¶åå†å–ä¸ª log å‘¢ï¼Ÿç­”æ¡ˆæ˜¯ä¸è¡Œï¼Œä½†æˆ‘è¿˜æ²¡æ‰¾åˆ°ä¸€ä¸ªç²¾ç¡®çš„è§£é‡Šå’Œåä¾‹ã€‚ç›´è§‚ä¸Šæ¥è§£é‡Šï¼Œè’™ç‰¹å¡æ´›ä¼°è®¡åªèƒ½é’ˆå¯¹ä¸€ä¸ªåˆ†å¸ƒï¼Œè€Œ :math:log P(x) ä¸æ˜¯ä¸€ä¸ªåˆ†å¸ƒï¼Œæ‰€ä»¥ä¸èƒ½è¿™ä¹ˆå¹²ã€‚\næˆ‘ä»¬çš„åšæ³•æ˜¯é€šè¿‡ Jensen ä¸ç­‰å¼ï¼Œå°† :math:log P(x) è¿›è¡Œæ”¾ç¼©è½¬æ¢æˆä¼˜åŒ–ä¸€ä¸ªæœŸæœ›ï¼Œè¿™æ—¶å€™å°±å¯ä»¥æ­£å¸¸ä½¿ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡äº†ã€‚\n.. math::\n\\begin{aligned} log P(x) \u0026amp; = logE_{z\\sim Q}[\\frac{P(x|z)P(z)}{Q(z|x)}] \\ \u0026amp; \\geq E_{z\\sim Q}[log\\frac{P(x|z)P(z)}{Q(z|x)}] \\ \u0026amp; = E_{z\\sim Q}[logP_\\theta(x|z)] - D_{KL}[Q_\\theta(z|x) | P(z)] \\end{aligned} \\label{eq-vae-elbo-jensen}\nåè®°ï¼š :math:Q(z|x) è¿™ä¸ªåˆ†å¸ƒçš„é€‰å–ç­–ç•¥ï¼Œå¡‘é€ äº†ä¸åŒçš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œåœ¨ VAE [5]_ ä¸­ï¼Œ\\ :math:Q(z|x) æ˜¯ä¸€ä¸ªè”åˆè®­ç»ƒçš„å·²çŸ¥åˆ†å¸ƒï¼ˆä¾‹å¦‚é«˜æ–¯æˆ–ä¼¯åŠªåˆ©ï¼‰ï¼Œåœ¨ DDPM ä¸­ï¼Œ\\ :math:Q(z|x) æ˜¯ä¸€ä¸ªç”±å·²çŸ¥åˆ†å¸ƒï¼ˆä¾‹å¦‚é«˜æ–¯æˆ–ä¼¯åŠªåˆ©ï¼‰ç»„æˆçš„é©¬å°”å¯å¤«é“¾ã€‚\nå¦‚æœä½ å¯¹EBLOå¯¹æ¨å¯¼è¿˜ä¸å¤ªæ˜ç™½ï¼Œå¯ä»¥å†å‚è€ƒä¸€ä¸‹è¿™ç¯‡åšå®¢ [6]_ã€‚\n.. [1] å¦‚æœä¸€ä¸ªé—®é¢˜å³èƒ½åœ¨æœ‰æ•ˆæ—¶é—´å†…è§£å†³,ä¸ç®¡æ˜¯é€šè¿‡è§£æè¿˜æ˜¯è¿‘ä¼¼çš„æ–¹å¼,æˆ‘ä»¬å°±ç§°å®ƒæ˜¯ tractable çš„ã€‚\n.. [2] å¯ä»¥è€ƒè™‘è‡ªç„¶å›¾åƒä»£è¡¨çš„æ•°æ®ç©ºé—´ï¼Œä¸€ä¸ª128_128çš„å›¾åƒï¼Œå¯ä»¥è§†ä½œä¸€ä¸ª128_128ç»´çš„éšæœºå˜é‡ï¼Œæ¯ä¸ªç»´åº¦éƒ½æœ‰256ç§å–å€¼ï¼Œä½†æ˜¯åªæœ‰å¾ˆå°çš„ä¸€éƒ¨åˆ†ä¼šå¾—åˆ°åˆç†çš„å›¾åƒï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªéšæœºå˜é‡ä»£è¡¨çš„æ•°æ®ç©ºé—´é‡Œï¼Œåªæœ‰å¾ˆå°çš„ä¸€ä¸ªå­ç©ºé—´æ¦‚ç‡å¯†åº¦æ¯”è¾ƒå¤§ã€‚å› ä¸º data space :math:X æ¯ä¸ª sample éƒ½è¦è‡³å°‘æœ‰ä¸€ä¸ª latent space :math:Z\\ \\ çš„ sample ä¸ä¹‹å¯¹åº”ï¼Œæ‰€ä»¥ä» Z é‡Œå–æ ·ï¼Œé€šè¿‡æ˜ å°„\\ \\ :math:g\\ \\ ï¼Œè¿˜æ˜¯å¾ˆéš¾è½åˆ°æ¦‚ç‡çš„å­ç©ºé—´é‡Œã€‚\\ \\ æ€è€ƒ\\ \\ ï¼šé‚£æˆ‘ä»¬å¯¹ data space åšä¸€ä¸ªå‹ç¼©ä¸å°±è¡Œäº†å—ï¼Œç”¨PCA æˆ–è€… VQVAE é™ç»´ï¼Ÿ\n.. [3] VLBO: Variantional Lower BOund\n.. [4] ELBO: Evidence Lower BOund\n.. [5] ç”±å‰é¢çš„è§£é‡Šå¯ä»¥çŸ¥é“ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ª :math:Q(z|x) å»è¿‘ä¼¼çœŸå®çš„ :math:P(z|x)\\ \\ ï¼Œè¿™ç§ç”¨ä¸€ä¸ªå‡½æ•°å»å­¦ä¹ å¦ä¸€ä¸ªå‡½æ•°çš„æ–¹æ³•ï¼Œé€šå¸¸è¢«ç§°ä¸º variational xxï¼Œè¿™ä¹Ÿæ˜¯ VAE åå­—ä¸­ V çš„æ¥æºã€‚\n.. [6] http://www.denizyuret.com/2019/11/a-simple-explanation-of-variational.html\n"},{"id":29,"href":"/blog/docs/matrix/jordan/","title":"Jordan Form","section":"Matrix","content":"Created: Decemenber, 7, 2020 Jordan Formæ˜¯çŸ©é˜µå¯¹è§’åŒ–çš„ä¸€ä¸ªæ¨å¹¿çš„äº§ç‰©ã€‚\næˆ‘ä»¬çŸ¥é“çŸ©é˜µ$A$å¯ä»¥å¯¹è§’åŒ–æˆä¸€ä¸ªå¯¹è§’çŸ©é˜µ$\\Lambda$ï¼Œè€Œä¸”è¿™ä¸ªçŸ©é˜µçš„å¯¹è§’çº¿å…ƒç´ æ˜¯$A$çš„ç‰¹å¾å€¼ï¼Œå‰ææ˜¯çŸ©é˜µ$A$å­˜åœ¨nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚å¦‚æœä¸æ»¡è¶³ï¼Œåˆ™ä¸èƒ½å¯¹è§’åŒ–ï¼Œå³æ‰¾ä¸åˆ°ä¸€ä¸ªSï¼Œä½¿å¾—ï¼š\n$$ \\Lambda = S^{-1}AS $$\nJordanåˆ™æ¨å¹¿äº†è¿™ä¸ªå¯¹è§’åŒ–çš„è¿‡ç¨‹ï¼Œä»»ä½•çŸ©é˜µï¼Œå³ä½¿å®ƒä¸å­˜åœ¨nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼Œå®ƒä¹Ÿå¯ä»¥ç›¸ä¼¼â€œå¯¹è§’åŒ–â€ä¸ºä¸€ä¸ªJordanæ ‡å‡†å‹$J$ã€‚\n$$ J = P^{-1}AP $$\nä½†è¿™æ—¶ï¼ŒJçš„å¯¹è§’çº¿ä¸åœ¨æ˜¯ä¸€ä¸ªå•ç‹¬çš„å…ƒç´ ï¼Œè€Œæ˜¯ä¸€ç³»åˆ—Jordanå—ï¼ˆè¯·æƒ³è±¡ä¸€ä¸‹åˆ†å—çŸ©é˜µï¼‰ã€‚\n$\\lambda$çŸ©é˜µ #  å¦‚æœä¸€ä¸ªçŸ©é˜µæ¯ä¸ªå…ƒç´ éƒ½æ˜¯å˜é‡$\\lambda$çš„å¤šé¡¹å¼ï¼Œåˆ™ç§°è¿™ä¸ªçŸ©é˜µä¸º$\\lambda$çŸ©é˜µï¼Œè®°åš$A(\\lambda)$ã€‚\nSmtihæ ‡å‡†å½¢\næ¯ä¸€ä¸ª$\\lambda$çŸ©é˜µéƒ½å¯ä»¥åŒ–æˆå¯¹è§’å½¢çŸ©é˜µï¼Œè¿™ä¸ªå¯¹è§’å½¢çŸ©é˜µç§°ä¸ºè¯¥$\\lambda$çŸ©é˜µçš„Smithæ ‡å‡†å½¢ã€‚\nå¯¹è§’çº¿ä¸Šçš„å…ƒç´ æŒ‰ä¸€å®šé¡ºåºæ’åˆ—ï¼š\n æ¯ä¸€ä¸ªå…ƒç´ éƒ½èƒ½æ•´é™¤åä¸€ä¸ªå…ƒç´  0åœ¨æœ€åé¢ï¼ˆè¿™å…¶å®æ˜¯ç¬¬ä¸€æ¡è§„åˆ™çš„æ¨è®ºï¼‰  ä¸å˜å› å­\nSmithæ ‡å‡†å½¢å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ç§°ä¸º$A(\\lambda)$çš„ä¸å˜å› å­ã€‚\nè¡Œåˆ—å¼å› å­\nké˜¶è¡Œåˆ—å¼å› å­å°±æ˜¯Smithæ ‡å‡†å½¢çš„å‰kå„å…ƒç´ ç›¸ä¹˜ã€‚kæœ€å¤§åªèƒ½å–åˆ°çŸ©é˜µç§©ï¼Œå°±æ˜¯è¯´è¡Œåˆ—å¼å› å­æ²¡æœ‰0ã€‚\n$$ D_k(\\lambda) = d_1(\\lambda)d_2(\\lambda)\u0026hellip;d_k(\\lambda) $$\nåˆç­‰å› å­\nä¸æ˜¯å¸¸æ•°çš„ä¸å˜å› å­çš„â€œå› æ•°â€ç§°ä¸ºåˆç­‰å› å­ã€‚\nä¾‹å¦‚ï¼Œè‹¥$\\lambda$çŸ©é˜µçš„ä¸å˜å› å­æ˜¯\n1, 1, $(\\lambda-2)^5(\\lambda-3)^3$, $(\\lambda-2)^5(\\lambda-3)^4(\\lambda+2)$\nåˆ™å®ƒçš„åˆç­‰å› å­æ˜¯ï¼Œä»”ç»†çœ‹\n$(\\lambda-2)^5$, $(\\lambda-3)^3$, $(\\lambda-2)^5$, $(\\lambda-3)^4$, $(\\lambda+2)$\n"},{"id":30,"href":"/blog/posts/algorithms/kmp/","title":"KMPç®—æ³•","section":"Posts","content":"é—®é¢˜æè¿°:\n ç»™å®šä¸€ä¸ªæ–‡æœ¬ä¸²S, å’Œä¸€ä¸ªæ¨¡å¼ä¸²P, æˆ‘ä»¬è¦æ‰¾åˆ°Påœ¨Sä¸­çš„ä½ç½®ï¼Œå³ç»™å‡ºPçš„ç¬¬ä¸€ä¸ªå­—ç¬¦åœ¨Sä¸­çš„ä½ç½®ã€‚\n æœ´ç´ ç®—æ³• #  æšä¸¾Sä¸­æ¯ä¸ªä½ç½®ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æ¨¡å¼ä¸²Pçš„èµ·ç‚¹ã€‚\nåŸºäºä¸Šè¿°æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“å¯ä»¥å†™å‡ºå¦‚ä¸‹çš„æšä¸¾ä»£ç :\nint vanilla_find(const string\u0026amp; s, const string\u0026amp; p) { int pos = -1; bool ok = true; for(int i=0; i\u0026lt;s.size(); i++) { ok = true; for(int j=0; j\u0026lt;p.size(); j++) { if(s[i+j] != p[j]) { ok = false; break; } } if(ok) { pos = i; break; } } return pos; } ä¸Šè¿°å®ç°ä½¿ç”¨1ä¸ªå¾ªç¯æšä¸¾é…å¯¹çš„èµ·ç‚¹ï¼Œå†ç”¨1ä¸ªå¾ªç¯åˆ¤æ–­ä»è¯¥èµ·ç‚¹å¼€å§‹æ˜¯å¦æœ‰æ¨¡å¼ä¸²Pã€‚\nä¸ºäº†æ–¹ä¾¿ç†è§£KMPç®—æ³•ï¼Œæˆ‘ä»¬ä¸å¦¨æ¢ä¸€ç§æ€è·¯å®ç°è¿™ä¸ªæœ´ç´ ç®—æ³•ã€‚\nint vanilla_find2(const string\u0026amp; s, const string\u0026amp; p) { int i=0, j=0; while(i \u0026lt; s.size() \u0026amp;\u0026amp; j \u0026lt; p.size()) { if(s[i] == p[j]) { i++; j++; } else { i=i-j+1; j=0; } } if(j == p.size()) return i-j; else return -1; } ä¸Šè¿°å®ç°ä½¿ç”¨ä¸¤ä¸ªæŒ‡é’ˆ, i, jï¼ŒiæŒ‡é’ˆä»£è¡¨å½“å‰Sä¸²åŒ¹é…åˆ°çš„ä½ç½®ï¼Œjè¡¨ç¤ºPä¸²åŒ¹é…åˆ°çš„ä½ç½®ï¼Œé‡åˆ°ä¸åŒ¹é…ï¼Œåˆ™å°†iæŒ‡é’ˆå›æº¯åˆ°ä¸Šä¸€æ¬¡å°è¯•çš„èµ·å§‹ç‚¹çš„åä¸€ä¸ªä½ç½®ï¼Œå³i-j+1ã€‚jæŒ‡é’ˆå›æº¯åˆ°å¼€å¤´ã€‚\nè™½ç„¶ä¸¤ç§å®ç°æœ¬è´¨æ˜¯ä¸€æ ·çš„ï¼Œä½†ç¬¬äºŒç§å®ç°ä¼šæ›´å®¹æ˜“ç†è§£KMPç®—æ³•çš„ä¼˜åŒ–ã€‚\nå¦‚å›¾1æ‰€ç¤ºï¼Œå½“æˆ‘ä»¬åŒ¹é…æˆåŠŸABCDABåï¼Œå‘ç°Dä¸åŒ¹é…ï¼Œè¿™æ—¶å¦‚æœæˆ‘ä»¬å°†iå›æº¯åˆ°i-j+1çš„ä½ç½®ï¼ŒåŒ¹é…å¿…ç„¶å¤±è´¥ï¼Œå› ä¸ºæˆ‘ä»¬å‰ä¸€æ­¥å·²ç»çŸ¥é“S[i-j+1]=Bäº†ã€‚\nå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å‰ä¸€æ­¥çš„åŒ¹é…ç»“æœï¼Œè®©iä¸å¿…å›æº¯ï¼Œåªå›æº¯jçš„è¯ï¼Œé‚£ä¹ˆåŒ¹é…æ•ˆç‡ä¼šå¤§å¹…æå‡ã€‚\n{% include double-image.html url1=\u0026quot;/assets/img/kmp1.png\u0026quot; caption1=\u0026ldquo;Step1\u0026rdquo; url2=\u0026quot;/assets/img/kmp2.png\u0026quot; caption2=\u0026ldquo;Step2\u0026rdquo; size=\u0026ldquo;70%\u0026rdquo; description=\u0026ldquo;å›¾1\u0026rdquo; %}\nKMPç®—æ³• #  å¦‚æœæˆ‘ä»¬æŠŠä¹‹å‰çš„ä¼˜åŒ–\u0008å‡è®¾å½¢å¼åŒ–ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†å¾—åˆ°å¦‚ä¸‹çš„æè¿°ã€‚\nå½“å¤±é…æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿè®©i=i,j=next[j]ï¼Œå³iä¸ç”¨å›æº¯ï¼Œjæ ¹æ®å½“å‰ä½ç½®å†³å®š\u0008æ¨¡å¼ä¸²è¦è·ŸS[i]åŒ¹é…çš„ä½ç½®ã€‚\näº‹å®ä¸Šï¼Œè¿™ä¸ªå‡è®¾æ˜¯æˆç«‹çš„ï¼Œè€ƒè™‘å¦‚å›¾2çš„æƒ…å†µï¼š\n{% include double-image.html url1=\u0026quot;/assets/img/kmp3.png\u0026quot; caption1=\u0026ldquo;Step1\u0026rdquo; url2=\u0026quot;/assets/img/kmp4.png\u0026quot; caption2=\u0026ldquo;Step2\u0026rdquo; size=\u0026ldquo;70%\u0026rdquo; description=\u0026ldquo;å›¾2\u0026rdquo; %}\nå½“Då¤±é…æ—¶ï¼Œæˆ‘ä»¬çŸ¥é“æ¨¡å¼ä¸²çš„Dä¹‹å‰æœ‰ç›¸æŠ•çš„å‰ç¼€åç¼€ABï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å°è¯•åŒ¹é…æ¨¡å¼ä¸²å‰ç¼€ABåçš„Dä»¥åŠæ–‡æœ¬ä¸²çš„ å³å¯ã€‚\næ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬çŸ¥é“è¿™æ—¶å€™next[j]=2ã€‚\nå¦‚æ­¤ï¼Œç°åœ¨çš„é—®é¢˜å°±å˜æˆå¦‚ä½•æ±‚nextæ•°ç»„äº†ã€‚\næ±‚è§£nextæ•°ç»„ #  è®©æˆ‘ä»¬å†æ¬¡è€ƒè™‘å›¾2çš„æƒ…å†µï¼Œäº‹å®ä¸Šï¼Œæˆ‘ä»¬å·²ç»èƒ½å¤Ÿå‘ç°æ±‚è§£next[j]çš„è§„å¾‹äº†ã€‚å¯¹äºPä¸²çš„ç¬¬jä¸ªå…ƒç´ ï¼Œå…¶å¤±é…åéœ€è¦å›æº¯åˆ°çš„ä½ç½®å–å†³äº - Pä¸²å‰jä¸ªå…ƒç´ æœ‰å¤šé•¿ç›¸åŒçš„å‰ç¼€åç¼€ã€‚å¦‚æœæœ‰é•¿åº¦ä¸ºkçš„ç›¸åŒå‰ç¼€åç¼€ï¼Œåˆ™next[j]=kã€‚\nåŸºäºä¸Šè¿°æ€æƒ³ï¼Œæˆ‘ä»¬å…¶å®å·²ç»å¯ä»¥ç¼–å†™ä¸€ä¸ªæš´åŠ›æ±‚nextæ•°ç»„çš„ç¨‹åºäº†ã€‚\nvector\u0026lt;int\u0026gt; get_next_vanilla(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; for(int i=1; i\u0026lt;p.size(); i++) { int n_match = 0; // æšä¸¾æ‰€æœ‰\u0008å¯èƒ½çš„ç›¸åŒå‰ç¼€åç¼€  for(int k=1; k\u0026lt;i; k++) { bool ok = true; for(int j=0; j\u0026lt;k; j++) { if(p[j] != p[i-k+j]) ok = false; } if(ok == true) n_match = k; } next[i] = n_match; } return next; } å¾ˆæ˜¾ç„¶ï¼Œè¿™ç§æš´åŠ›æ±‚è§£çš„æ•ˆç‡æ˜¯å¾ˆä½çš„($$O(n^3)$$)ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ±‚è§£nextçš„ç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚\né€’æ¨æ±‚è§£ä¼˜åŒ– #  è€ƒè™‘ä¸‹é¢è¿™ç§æƒ…å†µ\nABCDABDC 01234567 å¦‚æœæˆ‘ä»¬å·²ç»æ±‚å¾—next[6]=2ï¼Œå³Dä¹‹å‰æœ‰ç›¸åŒå‰ç¼€åç¼€ABï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨æ±‚next[7]çš„æ—¶å€™ï¼Œåªéœ€è¦åˆ¤æ–­P[next[6]] == P[7-1] (C==D)æ˜¯å¦æˆç«‹å³å¯ã€‚\n è‹¥æˆç«‹ï¼Œæ˜¾ç„¶next[7] = next[6] + 1ã€‚ è‹¥ä¸æˆç«‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦è€ƒè™‘Dè¦å’ŒABè¿™ä¸ªå‰ç¼€ä¸­çš„å“ªä¸ªå­—æ¯è¿›è¡Œé…å¯¹ç»„æˆç›¸åŒå‰ç¼€åç¼€ã€‚ï¼ˆå…¶å®è¿™æ˜¯ä¸€ä¸ªå¥—å¨ƒé—®é¢˜:ã€‹ï¼‰  æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åœ¨æ±‚next[i]çš„æ—¶å€™æˆ‘ä»¬å®é™…ä¸Šéœ€è¦çš„æ˜¯ï¼ŒP[:i-1]ä¸­æœ€é•¿çš„ç›¸åŒå‰ç¼€åç¼€ï¼ˆå¯ä»¥é€šè¿‡next[i-1]å¾—åˆ°ï¼‰ï¼Œè®¾é•¿åº¦ä¸ºk, ç„¶åå†åˆ¤æ–­P[l] == P[k]ã€‚å¦‚æœä¸ç­‰äºçš„è¯ï¼Œæˆ‘ä»¬å°±æ‰¾å‡ºP[:k-1]ä¸­ç¬¬äºŒé•¿çš„ç›¸åŒå‰ç¼€åç¼€ï¼Œç”¨åŒæ ·æ–¹æ³•å†åˆ¤æ–­ä¸€æ¬¡ã€‚\nå®é™…ä¸Šï¼Œç¬¬äºŒé•¿çš„ç›¸åŒå‰ç¼€åç¼€å¯ä»¥é€šè¿‡next[next[k-1]]å¾—åˆ°ã€‚\n ä¸ºä»€ä¹ˆï¼Ÿç¬¬ä¸€é•¿çš„ç›¸åŒå‰ç¼€åç¼€çš„ç›¸åŒå‰ç¼€åç¼€å®é™…ä¸Šå°±æ˜¯ç¬¬äºŒé•¿çš„ç›¸åŒå‰ç¼€åç¼€ã€‚ä¾‹å¦‚ ABABCDABAB, ç¬¬ä¸€é•¿æ˜¯ ABAB, ç¬¬äºŒé•¿æ˜¯ AB\n åŸºäºä¸Šè¿°æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºå¦‚ä¸‹çš„é€’æ¨æ±‚è§£çš„ä»£ç \nvector\u0026lt;int\u0026gt; get_next(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; for(int i=1;i\u0026lt;p.size();i++){ int k=i-1; while(k!=0 \u0026amp;\u0026amp; p[i-1] != p[next[k]]) k=next[k]; // å…ˆæ‰¾å‡º\u0026#34;OK\u0026#34;çš„å‰ç¼€åç¼€  next[i] = next[k] + 1; } return next; } ç»§ç»­ä¼˜åŒ– #  ä¼˜åŒ–1\næˆ‘ä»¬é¦–å…ˆå¯ä»¥é‡æ„ä¸€ä¸‹ä»£ç , ä»¥ä¸‹ä»£ç å’Œget_nextç­‰ä»·ã€‚\nvector\u0026lt;int\u0026gt; get_next2(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; int i=0, k=-1; while(i \u0026lt; p.size() - 1) { if(k == -1 || p[i] == p[k]) { i++; k++; next[i] = k; } else { k = next[k]; } } return next; } ä¼˜åŒ–2\nå†è€…ï¼Œè€ƒè™‘å¦‚ä¸‹æƒ…å†µ\nabab next: -1, 0, 0, 1 å½“bå¤±é…æ—¶ï¼Œnext=1ï¼Œä½†æ˜¯P[1]=bï¼Œæˆ‘ä»¬æ²¡å¿…è¦å†æ¯”ä¸€æ¬¡ï¼Œå› ä¸ºè¿™æ¬¡ä¸€å®šå¤±é…ï¼ŒåŸºäºæ­¤æˆ‘ä»¬å¯ä»¥å†æ¬¡ä¼˜åŒ–ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š\nvector\u0026lt;int\u0026gt; get_next2_opt(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; int i=0, k=-1; while(i \u0026lt; p.size() - 1) { if(k == -1 || p[i] == p[k]) { i++; k++; // ä»¥ä¸‹ä¸¤è¡Œä¸ºä¼˜åŒ–å†…å®¹  if(p[k] != p[i]) next[i] = k; else next[i] = next[k]; } else { k = next[k]; } } return next; } åŸæ¥çš„get_nextå½“ç„¶ä¹Ÿå¯ä»¥ç”¨åŒæ ·çš„æ–¹æ³•ä¼˜åŒ–ï¼Œä½†æ˜¯è¦æ³¨æ„å†™æ³•ã€‚ ä¸‹é¢è¿™ç§ä¼˜åŒ–æ–¹å¼å°±ä¸è¡Œã€‚äº‹å®ä¸Šï¼Œå½“ä½ æŠŠä¸‹é¢çš„ä»£ç æ”¹å¯¹ä¹‹åï¼Œä½ ä¼šå‘ç°ä½ çš„ä»£ç å°±å˜å¾—å’Œget_next2_optåŸºæœ¬ä¸€æ ·äº†ã€‚\nvector\u0026lt;int\u0026gt; get_next_opt(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; for(int i=1;i\u0026lt;p.size();i++){ int k=i-1; while(k!=0 \u0026amp;\u0026amp; p[i-1] != p[next[k]]) k=next[k]; // å…ˆæ‰¾å‡º\u0026#34;OK\u0026#34;çš„å‰ç¼€åç¼€  // ç›´æ¥è¿™æ ·ä¼˜åŒ–æ˜¯ä¸è¡Œçš„  if(p[i] != p[next[k]+1]) next[i] = next[k] + 1; else next[i] = next[next[k]+1]; } return next; } å‚è€ƒèµ„æ–™ #   ä»å¤´åˆ°å°¾å½»åº•ç†è§£KMP: cnblogs "},{"id":31,"href":"/blog/docs/notes/concept/nflow/","title":"Normalizing Flow","section":"Concept","content":"ç®€å•ä»‹ç» #  é¦–å…ˆï¼ŒNormalizing Flowæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼ˆGenerative Modelï¼‰ï¼Œå®ƒçš„æå‡ºæ˜¯ä¸ºäº†è§£å†³ç°æœ‰çš„ç”Ÿæˆçš„æ¨¡å‹çš„ä¸€äº›é—®é¢˜ï¼Œå¦‚GANè®­ç»ƒä¸ç¨³å®šï¼ŒVAEè¾¹ç¼˜æ¦‚ç‡ä¼°è®¡intractableçš„é—®é¢˜ã€‚\nNormalizing Flowæ˜¯ä¸€ç§ç‰¹æ®Šçš„inveritableç½‘ç»œï¼Œæ¯ä¸ªæ¨¡å—éƒ½æ˜¯å¯é€†çš„ï¼Œå¹¶ä¸”æœ‰tractable determinant of the Jacobianï¼Œä¸”æ±‚é€†ä¹Ÿæ˜¯tractableçš„ã€‚\nä¼˜ç‚¹ï¼š\n ç›¸æ¯”VAEï¼Œä¸å¿…å¼•å…¥å™ªå£°ï¼Œ more powerful local variance models. ç›¸æ¯”GANï¼Œè®­ç»ƒæ›´ç¨³å®šï¼Œæ›´æ˜“æ”¶æ•›ã€‚  ç¼ºç‚¹\n æ¯ä¸ªæ¨¡å—éƒ½å¿…é¡»å¯é€†ï¼Œé™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ã€‚ å¯é€†è¦æ±‚latent spaceçš„ç»´åº¦å¾ˆé«˜ã€‚  è¯¦ç»†ä»‹ç» #  å‚è€ƒèµ„æ–™ #   Github: normalizing-flows  Introduction to Normalizing Flows  Stanford CS236: Normalizing flow models  Difference between invertible NN and flow-based NN  "},{"id":32,"href":"/blog/docs/notes/paper/patchmatch/","title":"PatchMatch å¯¼è¯»","section":"Paper","content":"Created: Jan, 24, 2022 PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing\n Sample Impl ï½œ çŸ¥ä¹è§£è¯»\nå›¾åƒåŒ¹é…çš„å…¸ä¸­å…¸ï¼ŒåŸºäºåŒ¹é…å—å‘¨å›´çš„åŒ¹é…ä¹Ÿè¿‘ä¼¼åŒ¹é…çš„å…ˆéªŒçŸ¥è¯†çš„éšæœºåŒ–ç®—æ³•ã€‚\n é¦–å…ˆæ˜ç¡®è¿™ç¯‡è®ºæ–‡è§£å†³çš„é—®é¢˜æ˜¯å›¾åƒåŒ¹é…é—®é¢˜ï¼Œå³ç»™å®šä¸¤å¼ å›¾åƒAï¼ŒBï¼Œæˆ‘ä»¬éœ€è¦å°†Aï¼ŒBä¸­ç›¸åŒçš„åƒç´ æˆ–å›¾åƒå—ï¼ˆPatchï¼‰è¿›è¡ŒåŒ¹é…ï¼Œå¾—åˆ°ä¸€ä¸ªåç§»é‡fï¼Œä½¿å¾—å¯¹äºAä¸­æ¯ä¸ªç‚¹çš„åæ ‡xï¼ŒBä¸­å¯¹åº”ç‚¹çš„åæ ‡ä¸ºx+f(x)ã€‚\nè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªRandomized Correspondence Algorithmæ¥åšè¿™ä»¶äº‹æƒ…ã€‚è¦ç†è§£è¿™ä¸ªç®—æ³•ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è¿™ä¸ªç®—æ³•çš„æ ¸å¿ƒinsightï¼š\nThe key insights driving the algorithm are thatsome good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas.  ç”¨ä¸­æ–‡æ¥è¯´å°±æ˜¯ï¼Œå¯¹äºä¸€ä¸ªåŒ¹é…å¥½çš„ç‚¹ï¼Œæˆ‘ä»¬çŸ¥é“å…¶åç§»é‡ä¸ºf(x)ï¼ŒåŸºäºå›¾åƒçš„è¿ç»­æ€§ï¼Œé‚£ä¹ˆè¿™ä¸ªç‚¹å‘¨å›´çš„ç‚¹çš„åç§»é‡åº”è¯¥ä¹Ÿè¿‘ä¼¼æ˜¯f(x)ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å¿«é€ŸæŠŠæ­£ç¡®ç‚¹çš„åç§»é‡å‘å…¶å‘¨å›´ä¼ æ’­ï¼Œä½¿å…¶å‘¨å›´ç‚¹çš„ä¼°è®¡åç§»é‡è¶Šæ¥è¶Šå‡†ç¡®ã€‚\nå›¾åƒçš„è¿ç»­æ€§ï¼šä¸€ä¸ªè‹¹æœåœ¨ä¸¤å¼ å›¾åƒä¸Šæ”¾åœ¨ä¸åŒä½ç½®ï¼Œä½†æ˜¯è‹¹æœä¸ŠæŸä¸¤ä¸ªç‚¹çš„ç›¸å¯¹ä½ç½®å…³ç³»è¿‘ä¼¼ä¸å˜ã€‚  ç®—æ³• #  å‰ææ¡ä»¶ å‡è®¾æˆ‘ä»¬æœ‰ä¸¤å¼ å›¾åƒAï¼ŒBï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªåç§»é‡fï¼Œä½¿å¾—å¯¹äºAä¸­æ¯ä¸ªç‚¹çš„åæ ‡xï¼ŒBä¸­å¯¹åº”ç‚¹çš„åæ ‡ä¸ºx+f(x)ã€‚\nPatchMatchçš„ç®—æ³•ä¸»è¦åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š\n Initializationï¼š éšæœºåˆå§‹åŒ–åŒ¹é…å…³ç³»ï¼Œå³Aä¸­æ¯ä¸ªç‚¹çš„åç§»é‡ã€‚ Propagationï¼š æ ¹æ®åŒ¹é…ç¨‹åº¦ï¼Œå°†æ¯ä¸ªç‚¹çš„åç§»é‡ä¼ æ’­åˆ°å‘¨å›´çš„ç‚¹ã€‚å®é™…å®ç°æ˜¯ï¼Œå¯¹äºæŸä¸ªç‚¹(x,y)ï¼Œä½¿ç”¨å…¶å‘¨å›´ç‚¹çš„åç§»é‡è¿˜æ˜¯è‡ªå·±çš„åç§»é‡å¾—åˆ°åŒ¹é…ç‚¹ï¼Œè®¡ç®—åŒ¹é…ç¨‹åº¦ï¼Œå–åŒ¹é…ç¨‹åº¦æœ€å¥½çš„åç§»é‡ä½œä¸ºå½“å‰ç‚¹çš„åç§»é‡ã€‚ Random searchï¼š ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åŒ¹é…ç»“æœï¼Œæˆ‘ä»¬åœ¨å‰ä¸€æ­¥å¾—åˆ°çš„åç§»é‡åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å†éšæœºçš„åšä¸€äº›å˜åŒ–ï¼Œæ£€æŸ¥åŒ¹é…ç‚¹å‘¨å›´æ˜¯å¦æœ‰æ›´å¥½çš„åŒ¹é…ç‚¹ã€‚   è§£è¯»ï¼š\n éšæœºåˆå§‹åŒ–åœ¨ç‚¹å¾ˆå¤šçš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ‰å¾ˆå¤§å‡ ç‡ä¼šå¾—åˆ°ä¸å°‘è¿˜ä¸é”™çš„åŒ¹é…ï¼Œè¿™äº›æ­£ç¡®åŒ¹é…æ˜¯åç»­Propagationçš„åŸºç¡€ã€‚ ä¸åŠ random searchæ€§èƒ½å¦‚ä½•ï¼ŸTODO  åº”ç”¨ #   Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution PatchmatchNet: Learned Multi-View Patchmatch Stereo  "},{"id":33,"href":"/blog/docs/notes/paper/raft/","title":"Raft é˜…è¯»ç¬”è®°","section":"Paper","content":"Created: August, 6, 2021 è®ºæ–‡: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow\nCode: https://github.com/princeton-vl/RAFT\n è¿™ç¯‡æ–‡ç« åšçš„æ˜¯å…‰æµä¼°è®¡ï¼Œç®€å•æ¥è¯´ï¼Œä»»åŠ¡å°±æ˜¯è¾“å…¥ä¸¤å¼ å›¾åƒ$I_1,I_2$ï¼Œæˆ‘ä»¬éœ€è¦ä¼°è®¡å‡ºç¬¬äºŒå¼ å›¾åƒ$I_1$æ¯ä¸ªåƒç´ ç‚¹$(u,v)$ç›¸å¯¹äºç¬¬ä¸€å¼ å›¾åƒ$I_1$çš„åç§»é‡$(f^1(u), f^2(v))$ã€‚\n$$ I_1 = I_2 + f $$\nè¿™ç¯‡æ–‡ç« çš„æ€è·¯æ˜¯è¿™æ ·çš„ï¼š\n é¦–å…ˆä½¿ç”¨ä¸€ä¸ªfeature encoder  æ±‚å‡ºflow featureï¼šåŒæ—¶è¾“å…¥img1ï¼Œimg2ï¼Œå¾—åˆ°fmap1ï¼Œfmap2 æ±‚å‡ºcontext featureï¼šåªè¾“å…¥img1ï¼Œå¾—åˆ°inp   æ±‚å‡ºfmap1å’Œfmap2çš„ç›¸ä¼¼åº¦corr ä½¿ç”¨ä¸€ä¸ªRNNè¿­ä»£çš„æ±‚å‡ºflowã€‚  RNNçš„è¾“å…¥åŒ…æ‹¬ä¸€ä¸ªéšè—çŠ¶æ€netï¼Œcontext feature inpï¼Œç›¸ä¼¼åº¦corrï¼Œä¸Šä¸€æ­¥çš„flow     è¿™é‡Œçš„ä¸»è¦é—®é¢˜æ˜¯æ¯è¿­ä»£ä¸€æ¬¡ï¼Œæˆ‘ä»¬éƒ½å¾—åˆ°äº†ä¸€ä¸ªæ›´æ¥è¿‘img1çš„img2ï¼Œè¿™æ—¶å€™ç›¸ä¼¼åº¦éœ€è¦é‡æ–°è®¡ç®—ã€‚\n RAFTè®¾è®¡äº†ä¸€ä¸ªæŸ¥è¡¨çš„æ€è·¯ï¼Œåªéœ€è¦åœ¨æœ€å¼€å§‹ç®—ä¸€éå³å¯ã€‚å®ƒçš„æ–¹æ³•æ˜¯ç”¨æ–°åæ ‡å‘¨å›´çš„ç‚¹çš„flow featureæ‹‰æˆä¸€ä¸ªå‘é‡ä½œä¸ºæ–°åæ ‡ç‚¹çš„flowç‰¹å¾ã€‚\n"},{"id":34,"href":"/blog/docs/gen/reverse_diffusion/","title":"Reverse Time Stochastic Differential Equations for Generative Modelling","section":"AIGC","content":"Created: March, 23, 2023 Originally posted at Reverse Time Stochastic Differential Equations for generative modelling.\n What follows is a derivation of the main result of â€˜Reverse-Time Diffusion Equation Modelsâ€™ by Brian D.O. Anderson (1982). Earlier on this blog we learned that a stochastic differential equation of the form $$ \\begin{align} dX_t = \\mu(X_t, t) dt + \\sigma(X_t, t) dW_t \\end{align} $$\nwith the derivative of Wiener process $W_t$ admits two types of equations, called the forward Kolmogorov or Fokker-Planck equation and the backward Kolmogorov equation. The details of the derivation of the forward and backward Kolmogorov equations via the Kramers-Moyal expansion can be found in the previous blog post. For notational brevity we will use the term $\\mu(x_t)$ for the drift and $\\sigma(x_t)$ as the diffusion parameter and omit the explicit time dependency.\nThe Kolmogorov forward equation is identical to the Fokker Planck equation and states\n $$ \\begin{align} \\partial_t p(x_t) = -\\partial_{x_t} \\left[ \\mu(x_t) p(x_t) \\right] + \\frac{1}{2} \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right]. \\end{align} $$  It describes the evolution of a probability distribution $p(x_t)$ forward in time. We can quite frankly think of it as, for example, a Normal distribution being slowly transformed into an arbitrary complex distribution according to the drift and diffusion parameters $\\mu(x_t)$ and $\\sigma(x_t)$.\nThe Kolmogorov backward equation for $s \\geq t$ is defined as\n $$ \\begin{align} - \\partial_t p(x_s | x_t) = \\mu(x_t) \\ \\partial_{x_t} p(x_s|x_t) + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\end{align} $$  and it basically answers the question how the probability of $x_s$ at a later point in time changes as we change $x_t$ at an earlier point in time. The Kolmogorov backward equation is somewhat confounding with respect to time as weâ€™re taking the partial derivative with respect to the earlier time step $t$ on which we are also coniditoning. But we can think of it as asking â€˜How does the probability of $x_s$ at the later point in time $s$ change, as we slowly evolve the probability distribution backwards through time and condition on $x_t$â€™.\nTaking inspiration from our crude example earlier, the backward equation offers a partial differential equation which we can solve backward in time, which would correspond to evolving the arbitrarily complex distribution backwards to our original Normal distribution. Unfortunately there is no corresponding stochastic differential equation with a drift and diffusion term that describes the evolution of a random variable backwards through time in terms of a stochastic differential equation.\nThis is where the remarkable result from Anderson (1982) comes into play.\nThe granddaddy of all probabilistic equations, Bayes theorem, tells us that a joint distribution can be factorized by conditioning: $p(x_s , x_t) = p(x_s|x_t) p(x_t)$ with the time ordering $t \\leq s$. Why do we invoke the joint probability $p(x_s, x_t)$ we might ask? What weâ€™re trying to achieve is to derive a stochastic differential equation that tells us from what values of $x_t$ we can arrive at $x_s$. We can ask ourselves what the partial differential equation would be that describes the evolution of the joint distribution over time. First multiplying both sides of Bayes theorem with minus one and taking the derivative with respect to time $t$, we obtain via the product rule\n $$ \\begin{align} - \\partial_t p(x_s, x_t) \u0026= - \\partial_t \\left[ p(x_s| x_t) p(x_t) \\right] \\\\ \u0026= \\underbrace{-\\partial_t p(x_s|x_t)}_{\\text{KBE}} p(x_t) - p(x_s | x_t) \\underbrace{\\partial_t p(x_t)}_{\\text{KFE}} \\end{align} $$  into which we can plug in the Kolmogorov forward (KFE) and Kolmogorov backward (KBE) equations,\n $$ \\begin{align} \u0026 -\\partial_t p(x_s|x_t) p(x_t) - p(x_s | x_t) \\partial_t p(x_t) \\\\ \u0026= \\left( \\mu(x_t) \\ \\partial_{x_t} p(x_s|x_t) + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\right) p(x_t) \\\\ \u0026 + p(x_s| x_t) \\left( \\partial_{x_t} \\left[ \\mu(x_t) p(x_t) \\right] - \\frac{1}{2} \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\right) \\end{align} $$  The derivative occuring in the backward Kolmogorov equation are\n $$ \\begin{align} \\partial_{x_t} p(x_s|x_t) \u0026= \\partial_{x_t} \\left[ \\frac{p(x_s, x_t)}{p(x_t)} \\right] \\\\ \u0026 = \\frac{\\partial_{x_t} p(x_s, x_t) p(x_t) - p(x_s, x_t) \\partial_{x_t} p(x_t)}{p^2(x_t)} \\\\ \u0026 = \\frac{\\partial_{x_t} p(x_s, x_t)}{p(x_t)} - \\frac{p(x_s, x_t) \\partial_{x_t} p(x_t)}{p^2(x_t)} \\end{align} $$  The next step is to evaluate the derivative of the products in the forward Kolmogorov equation.\n $$ \\begin{align} \\partial_{x_t} \\left[ \\mu(x_t) p(x_t) \\right] \u0026 = \\partial_{x_t} \\mu(x_t) \\ p(x_t) + \\mu(x_t) \\ \\partial_{x_t} p(x_t) \\\\ \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \u0026 = \\partial_{x_t}^2 \\sigma^2(x_t) \\ p(x_t) + 2 \\ \\partial_{x_t} \\sigma^2(x_t) \\ \\partial_{x_t} p(x_t) + \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_t) \\end{align} $$  Substituting the derivatives of the probability distributions accordingly we obtain\n $$ \\begin{align} - \\partial_t p(x_s, x_t) = \u0026 - \\partial_t \\left[ p(x_s| x_t) p(x_t) \\right] \\\\ = \u0026 -\\partial_t p(x_s|x_t) p(x_t) - p(x_s | x_t) \\partial_t p(x_t) \\\\ = \u0026 \\left( \\mu(x_t) \\ \\partial_{x_t} p(x_s|x_t) + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\right) p(x_t) \\\\ \u0026 + p(x_s| x_t) \\left( \\partial_{x_t} \\left[ \\mu(x_t) p(x_t) \\right] - \\frac{1}{2} \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\right) \\\\ = \u0026 \\mu(x_t) \\ \\partial_{x_t} p(x_s|x_t) \\ p(x_t) + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t) \\\\ \u0026 + p(x_s| x_t) \\partial_{x_t} \\mu(x_t) \\ p(x_t) + p(x_s| x_t) \\mu(x_t) \\ \\partial_{x_t} p(x_t) \\\\ \u0026 - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\\\ = \u0026 \\mu(x_t) \\ \\left(\\frac{\\partial_{x_t} p(x_s, x_t)}{\\cancel{p(x_t)}} - \\frac{p(x_s, x_t) \\partial_{x_t} p(x_t)}{p^{\\cancel{2}}(x_t)} \\right) \\ \\cancel{p(x_t)} \\\\ \u0026 + p(x_s| x_t) \\partial_{x_t} \\mu(x_t) \\ p(x_t) + p(x_s| x_t) \\mu(x_t) \\ \\partial_{x_t} p(x_t) \\\\ \u0026 + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t) - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\\\ = \u0026 \\mu(x_t) \\ \\left(\\partial_{x_t} p(x_s, x_t) - \\frac{p(x_s, x_t) \\partial_{x_t} p(x_t)}{p(x_t)} \\right) \\\\ \u0026 + p(x_s| x_t) \\partial_{x_t} \\mu(x_t) \\ p(x_t) + p(x_s| x_t) \\mu(x_t) \\ \\partial_{x_t} p(x_t) \\\\ \u0026 + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t) - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\\\ = \u0026 \\mu(x_t) \\ \\left(\\partial_{x_t} p(x_s, x_t) - \\cancel{p(x_s| x_t) \\partial_{x_t} p(x_t)} \\right) \\\\ \u0026 + p(x_s, x_t) \\partial_{x_t} \\mu(x_t) + \\cancel{p(x_s| x_t) \\mu(x_t) \\ \\partial_{x_t} p(x_t)} \\\\ \u0026 + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t) - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\\\ = \u0026 \\underbrace{\\mu(x_t) \\ \\partial_{x_t} p(x_s, x_t) + p(x_s, x_t) \\partial_{x_t} \\mu(x_t)}_{\\text{product rule}} \\\\ \u0026 + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t) - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\\\ = \u0026 \\partial_{x_t} \\left[ \\mu(x_t) \\ p(x_s, x_t) \\right] \\\\ \u0026 + \\underbrace{\\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t)}_{(1)} - \\underbrace{\\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right]}_{(2)} \\end{align} $$  In order to transform the partial differential equation above into a form from which we can deduce an equivalent stochastic differential equation, we match the terms of the second order derivatives with the following identity,\n $$ \\begin{align} \u0026 \\frac{1}{2} \\partial_{x_t}^2 \\left[ p(x_s, x_t) \\sigma^2(x_t) \\right] \\\\ = \u0026 \\frac{1}{2} \\partial_{x_t}^2 \\left[ p(x_s | x_t) p(x_t) \\sigma^2(x_t) \\right] \\\\ = \u0026 \\frac{1}{2} \\partial_{x_t}^2 p(x_s | x_t) p(x_t) \\sigma^2(x_t) + \\partial_{x_t} \\left[ p(x_t) \\sigma^2(x_t) \\right] \\partial_{x_t} p(x_s| x_t) + \\frac{1}{2} \\partial_{x_t}^2 \\left[ p(x_t) \\sigma^2(x_t) \\right] p(x_s| x_t) \\\\ = \u0026 \\underbrace{\\frac{1}{2} \\sigma^2(x_t) \\partial_{x_t}^2 p(x_s | x_t) p(x_t)}_{(1)} + \\partial_{x_t} \\left[ p(x_t) \\sigma^2(x_t) \\right] \\partial_{x_t} p(x_s| x_t) + \\underbrace{\\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ p(x_t) \\sigma^2(x_t) \\right]}_{(2)} \\end{align} $$  by observing that the terms (1) and (2) occur in both equations. We can see from the expansion of the derivative above that we can combine the terms in our derivation if we expand the â€œcenter termâ€. Furthermore we can employ the identity $-\\frac{1}{2} X = -X + \\frac{1}{2} X$ to obtain\n $$ \\begin{align} -\\partial_t p(x_s, x_t) = \u0026 \\partial_{x_t} \\left[ \\mu(x_t) \\ p(x_s, x_t) \\right] \\\\ \u0026 + \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t) - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\\\ = \u0026 \\partial_{x_t} \\left[ \\mu(x_t) \\ p(x_s, x_t) \\right] \\\\ \u0026 + \\frac{1}{2} \\ \\sigma^2(x_t) \\ p(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\underbrace{ - \\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] }_{-\\frac{1}{2} X = -X + \\frac{1}{2} X} \\\\ \u0026 \\underbrace{\\pm \\partial_{x_t} p(x_s | x_t) \\partial_{x_t} \\left[ p(x_t) \\sigma^2(x_t) \\right]}_{\\text{complete the square}} \\\\ = \u0026 \\partial_{x_t} \\left[ \\mu(x_t) \\ p(x_s, x_t) \\right] \\textcolor{red}{+ \\frac{1}{2} \\ \\sigma^2(x_t) \\ \\partial_{x_t}^2 p(x_s | x_t) \\ p(x_t)} \\\\ \u0026 \\underbrace{ - p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] + \\textcolor{red}{\\frac{1}{2} p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right]} }_{-\\frac{1}{2} X = -X + \\frac{1}{2} X} \\\\ \u0026 \\textcolor{red}{\\pm \\partial_{x_t} p(x_s | x_t) \\partial_{x_t} \\left[ p(x_t) \\sigma^2(x_t) \\right]} \\\\ = \u0026 \\partial_{x_t} \\left[ \\mu(x_t) \\ p(x_s, x_t) \\right] + \\textcolor{red}{\\frac{1}{2} \\partial_{x_t}^2 \\left[ p( x_s | x_t) p(x_t) \\sigma^2(x_t) \\right]} \\\\ \u0026 \\underbrace{- p(x_s| x_t) \\partial_{x_t}^2 \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] - \\partial_{x_t} p(x_s | x_t) \\partial_{x_t} \\left[ p(x_t) \\sigma^2(x_t) \\right]}_{ - \\partial_{x_t} \\left[ p(x_s| x_t) \\partial_{x_t} \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\right] \\text{ (product rule) } } \\\\ = \u0026 \\partial_{x_t} \\left[ \\mu(x_t) \\ p(x_s, x_t) \\right] + \\frac{1}{2} \\partial_{x_t}^2 \\left[ p( x_s , x_t) \\sigma^2(x_t) \\right] \\\\ \u0026 - \\partial_{x_t} \\left[ p(x_s| x_t) \\partial_{x_t} \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\right]. \\end{align} $$  the result of which is in the form of a Kolmogorov forward equation, although using the joint probability distribution $p(x_s, x_t)$. For the time ordering of $t \\leq s$, we can observe that the term $-\\partial_t p(x_s, x_t)$ describes the change of the probability distribution as we move backward in time. In accordance with Leibnizâ€™ rule we can marginalize over $x_s$ without interferring with the partial derivative $\\partial_t$, to obtain\n $$ \\begin{align} -\\partial_t p(x_t) = \u0026 -\\partial_{x_t} \\left[ p(x_t) \\left( -\\mu(x_t) + \\frac{1}{p(x_t)} \\partial_{x_t} \\left[ \\sigma^2(x_t) \\ p(x_t) \\right] \\right) \\right] \\\\ \u0026 + \\frac{1}{2} \\partial_{x_t}^2 \\left[ p(x_t) \\sigma^2(x_t) \\right] \\\\ \\end{align} $$  and introduce the time reversal $\\tau \\doteq 1 - t$ which, with respect to the integration with respect to the flow of time, yields\n $$ \\begin{align} - \\partial_t p(x_t) = \u0026 \\partial_\\tau p(x_{1-\\tau}) \\\\ = \u0026 -\\partial_{x_{1-\\tau}} \\left[ p(x_{1-\\tau}) \\left( -\\mu(x_{1-\\tau}) + \\frac{1}{p(x_{1-\\tau})} \\partial_{x_{1-\\tau}} \\left[ \\sigma^2(x_{1-\\tau}) \\ p(x_{1-\\tau}) \\right] \\right) \\right] \\\\ \u0026 + \\frac{1}{2} \\partial_{x_{1-\\tau}}^2 \\left[ p(x_{1-\\tau}) \\sigma^2(x_{1-\\tau}) \\right] \\end{align} $$  which finally gives us a stochastic differential equation analogous to the Fokker-Planck/forward Kolmogorov equation that we can solve backward in time:\n $$ \\begin{align} dX_\\tau = \\left(-\\mu(x_{1-\\tau}) + \\frac{1}{p(x_{1-\\tau})} \\partial_{x_{1-\\tau}} \\left[ \\sigma^2(x_{1-\\tau}) \\ p(x_{1-\\tau}) \\right] \\right) d\\tau + \\sigma(x_{1-\\tau}) dW_\\tau \\end{align} $$  where $\\tilde{W}_t$ is a Wiener process that flows backward in time.\nBy keeping the $\\sigma^2(x_t)$ constant and independent of $x_t$ and applying the log-derivative trick, the drift simplifies to\n $$ \\begin{align} dX_\\tau \u0026 = \\Big(-\\mu(x_{1-\\tau}) + \\frac{1}{p(x_{1-\\tau})} \\partial_{x_{1-\\tau}} \\big[ \\overbrace{\\sigma^2(x_{1-\\tau})}^{=\\sigma^2} \\ p(x_{1-\\tau}) \\big] \\Big) d\\tau + \\sigma(x_{1-\\tau}) dW_\\tau \\\\ \u0026 =\\left(-\\mu(x_{1-\\tau}) + \\frac{\\sigma^2}{p(x_{1-\\tau})} \\partial_{x_{1-\\tau}} \\ p(x_{1-\\tau}) \\right) d\\tau + \\sigma(x_{1-\\tau}) dW_\\tau \\\\ \u0026= \\Big(-\\mu(x_{1-\\tau}) + \\sigma^2 \\partial_{x_{1-\\tau}} \\ \\log p(x_{1-\\tau}) \\Big) dt + \\sigma(x_{1-\\tau}) d\\tilde{W}_\\tau \\end{align} $$  "},{"id":35,"href":"/blog/docs/gen/sde/","title":"Stochastic Differential Equations","section":"AIGC","content":"Created: March, 23, 2023  A Wiener twist to differential equations\n Non-Differential Equations #  Most of us are quite familiar with linear and non-linear equations from our 101 math classes and lectures. These equations define an equality between the two terms left and right of the equal sign: $$ \\begin{align} y = f(x) \\end{align} $$\nThese functions assert an equality between $y$ and $x$ through the function $f(\\cdot)$ and describe a \u0026ldquo;static\u0026rdquo; relationship between a value $x$ and its corresponding value $y$. Examples of these functions are numerous and we can list a couple of them here:\n Linear equations: $y = A x + b$ Exponential equations: $y = e^x$ Polynomials: $y = \\sum_{k=0}^n a_k x^k$ Trigonometric equations: $y = \\sin(x)$ and the list goes on and on \u0026hellip;  All the equations above share the characteristic that they equate two separate values $y$ and $f(x)$.\nDifferential Equations #  As you can guess from the title there is another important class of equations: differential equations. These equations relate one or more functions to their derivatives. Mathematically this looks like the following: $$ \\begin{align} \\underbrace{\\frac{d y}{dx}}_{\\text{derivative}} = f(x) \\end{align} $$\nAs we can see from above the one thing that changed to our earlier, non-differential equation is the derivative. Instead of telling us what the value $y$ is given the function $f(x)$ as in the case of non-differential equations, the differential equation above tells us the change of $y$ with respect to $x$. In plain English, it tells us how much $y$ changes if we change $x$ by simply evaluating the function $f(x)$.\nNaturally, the question arises where we ask ourselves what the heck do these equations tell us. In non-differential equations, the relationship between input to a function and output is quite straight forward.\nI struggled for quite some time to arrive at an intuitive interpretation of what differential equations actually represent. Fortunately, one field where differential equations pop up en masse is physics (which apart from quantum physics tends to be quite intuitive for humans). So we\u0026rsquo;ll make a detour through physics to keep the intuition alive while diving into differential equations.\nDifferential equations are often employed in physics when a physical system is most accurately described through its instantaneous change in time. It should be noted that the differential could be defined with respect to any argument of the function $f(\\cdot)$, but in physics the time differential $d / dt$ is often the differential of interest as we want to predict things into the future. In the simplest case, a physical object $x(t)$ moves through time and space according to some function:\n $$ \\begin{align} \\underbrace{\\frac{d}{dt} \\ x(t)}_{\\text{change over time}} = \\underbrace{f(t, x(t))}_{\\text{value of change}} \\quad \\cong \\quad f(x(t)) \\end{align} $$  The equation above simply states that the change over time, $d x(t) / dt$ is equivalent to the function $f(t, x(t))$. Mathematically, we require the time $t$ to appear in the function $f(t, x(t))$ since otherwise the time derivative wouldn\u0026rsquo;t exist. For a more intuitive notation we can drop it and equate the change $d/dt x(t)$ with the function $f( \\cdot )$ with the \u0026lsquo;essentially the same\u0026rsquo; symbol $\\cong$.\nWe can write the differential equation in a shorter way by using the infinitesimal differential by pulling $dt$ over to the other side: $$ \\begin{align} dx(t) = f(t, x(t)) dt \\end{align} $$\nwhich simply states that a \u0026ldquo;super small\u0026rdquo; change $dx(t)$ in $x(t)$ corresponds to function $f(t, x(t))$ \u0026ldquo;scaled\u0026rdquo; by the \u0026ldquo;super small\u0026rdquo; time difference $dt$.\nBelow is an image juxtaposing what we refer to as non-differential equations and a differential equations with respect to time:\n {: .align=\u0026ldquo;center\u0026rdquo; height=\u0026ldquo;50%\u0026rdquo; width=\u0026ldquo;100%\u0026quot;}\nInstead of working with a \u0026ldquo;absolute\u0026rdquo; equation as shown on the left side, the differential equation on the right gives us the change $dx(t)$ for any point $x(t)$ at any point in time $t$ (which is mathematically a vector field). Each arrow in the right plot is an evaluation of the differential equation $dx(t)$ at a specific point $x(t)$ at a specific point in time $t$.\nA more intuitive example of the right hand plot above is the temperature of a hot coffee mug. The hotter the coffee mug, the larger the temperature gradient between coffee mug and the surrounding. So the larger the gradient the more temperature (thermal energy) is passed off into the environment of the hot coffee mug, ergo the temperature decrease is faster for coffee mugs with high temperatures. (To be frank, this is not the most physically correct way of how energy behaves, but this is just for an intuitive visualization.)\nThe grey lines in in the right plot model the changing temperatures over time of three coffee mugs with different temperatures. We model the thermal energy dissipation through a (ordinary) differential equation and would like to know what the temperature of the three coffee mugs will be at a later point in time. Computing the later temperature amounts to \u0026ldquo;little more\u0026rdquo; than following the arrows. These arrows are computed through the differential equation and tell us what the temperature change $dx(t)$ is for a mug with a specific temperature $x(t)$ at time $t$.\nOn a side note: Notice how the arrows don\u0026rsquo;t change in their direction and magnitude for a specific value $x(t)$ while we progress in time. This signals that $dx(t)$ doesn\u0026rsquo;t actually use $t$ to compute the change in temperature.\nThe way we solve differential equations is to start at some initial point $x(0)$ and add up all the temperature changes $dx(t)$ that the hot coffee mug is exposed to over time. Mathematically, this amounts to little more than: $$ \\begin{align} x(T) = x(0) + \\underbrace{\\int_{t=0}^T dx(t)}_{\\text{sum up all the changes}} \\end{align} $$\nthe solution of which is shown as the grey line in the right plot.\nAnother analogy would be kicking a soccer ball over a soccer field. The ball starts somewhere $x(0)$ and you kick it repeatedly in some direction (adding $dx(t)$ repeatedly). Each kick changes the location of the soccer ball and results in the ball lying in a new position $x(t)$. After we kicked the soccer ball about the soccer field enough, we\u0026rsquo;ll finally leave it at $x(T)$.\nUnfortunately, computers can\u0026rsquo;t really work with infinitesimal small number like $dx(t)$ or $dt$ since numbers in computers are stored with a finite amount of bits. As so often, the (approximate) solution is to discretize the changes to very small, yet still representable values of $\\Delta x(t)$ and $\\Delta t$: $$ \\begin{align} x(T) \u0026amp;= x(0) + \\underbrace{\\int_{t=0}^T dx(t)}{\\text{sum up all the changes}} \\ \u0026amp; \\underbrace{\\approx}{\\text{discretize}} x(0) + \\sum_{t=0}^T \\Delta x(t) \\end{align} $$\nwhere $t$ is some finite partition of time into discrete values.\nIt turns out that the integral above (and its respective discrete approximation) is all we need to solve (ordinary) differential equations. More importantly it\u0026rsquo;s all we need to get a basic understanding of stochastic differential equations. But before we can proceed to stochastic differential equations, we have to talk above stochasticity over time.\nEnter Wiener processes \u0026hellip;\nWiener Process #  In order to understand Wiener processes we need to think about the position of a particle in an Euclidean space that moves purely randomly. The question is how we could model such a particle.\nThe first idea would be to determine that at any point in time the particle has the tendency to move randomly in space. Therefore it does not jiggle and bounce at discrete time steps but will always move an infinitesimally small distance $dx(t)$ in a random direction $\\epsilon$ for any infinitesimally short period of time $dt$.\nVisually we want the random moving particle looking something like this in two dimensions:\n {: .align=\u0026ldquo;center\u0026rdquo; height=\u0026ldquo;50%\u0026rdquo; width=\u0026ldquo;50%\u0026quot;}\nWe can thus proclaim the following, somewhat un-mathematical property of this rambunctious little particle: $$ \\begin{align} \\underbrace{dx_t}{\\text{change in space}} = \\overbrace{\\epsilon}^{\\text{random move}} \\underbrace{\u0026ldquo;dt\u0026rdquo;}{\\text{some change in time}} \\end{align} $$\nThere are a couple of things to observe here:\n First we introduced a random variable $\\epsilon$ which follows some probability distribution. Secondly, through the infinitesimal differentials on both sides we equated the random move in space with the duration of the movement just like in a differential equation. Thirdly, the infinitesimal movements of $x_t$ through time are completely independent since $\\epsilon$ is sampled uncorrelated through time.  It turns out that if we choose the random movements $\\epsilon$ and the change in time $\u0026ldquo;dt\u0026rdquo;$ smartly, we can derive convenient theoretical properties about the movement of the little random particle $x_t$.\nSince $\\epsilon$ is a random variable at any point in time, the position of $x_t$ will never be predictable with absolute certainty. Instead we have to treat the position of the particle $x_t$ itself as a random variable, the behavior of which is governed by the differential equation above.\nFirst up is the choice of $\\epsilon$. The usage of the Normal distribution $\\mathcal{N}(\\mu, \\sigma)$ is prevalent in a lot of modelling approaches due to the convergence of sequences of random variables and it furthermore has nice theoretical properties. For that reason we will model the probability of the random movement $\\epsilon$ with a standard normal distribution, namely $\\epsilon \\sim \\mathcal{N}(0,1)$.\nSecondly we will chose the \u0026ldquo;amount of time $dt$\u0026rdquo; to actually be $\\sqrt{dt}$, the reason of which will be clear in an instant.\nThirdly, we want to particle to start at zero, so $x_0 = 0$.\nGiven these modelling assumptions, we are interested where the particle could turn up at a later point in time, so we want to know what $x_T$ is: $$ \\begin{align} x_T \u0026amp;= x_0 + \\int_{t=0}^T dx_t \\ \u0026amp;= \\underbrace{x_0}{\\text{$=0$}} + \\int{t=0}^T \\epsilon \\sqrt{dt} \\ \u0026amp;= \\int_{t=0}^T \\epsilon \\sqrt{dt} \\end{align} $$\nBut since $\\epsilon$ is a random variable we actually have to treat the position of the particle at $x_T$ as a random variable. The most that we can do is thus to treat $x_T$ as a probability distribution for which we can compute the first two moments, the mean and the variance: $$ \\begin{align} \\mathbb{E}\\left[ x_T \\right] \u0026amp;= \\mathbb{E}\\left[\\int_{t=0}^T dx_t \\right] \\ \u0026amp;= \\int_{t=0}^T \\underbrace{\\mathbb{E}\\left[ \\epsilon \\right]}_{\\mathcal{N}(0,1)} \\sqrt{dt} \\ \u0026amp;= 0 \\end{align} $$\nand $$ \\begin{align} \\mathbb{V}\\left[ x_T \\right] \u0026amp;= \\mathbb{V}\\left[\\int_{t=0}^T dx_t \\right] \\ \u0026amp;= \\int_{t=0}^T \\underbrace{\\mathbb{V}\\left[ \\epsilon \\sqrt{dt} \\right]}{\\mathbb{V}[a X] = a^2 \\mathbb{V}[X]} \\ \u0026amp;= \\int{t=0}^T dt \\underbrace{\\mathbb{V}\\left[ \\epsilon \\right]}{\\epsilon \\sim \\mathcal{N}(0,1)} \\ \u0026amp;= \\int{t=0}^T dt \\ \u0026amp;= T \\end{align} $$\nWe can validate the properties of the Wiener process experimentally through what a good friend of mine calls \u0026ldquo;computational evidence\u0026rdquo;:\n {: .align=\u0026ldquo;center\u0026rdquo; height=\u0026ldquo;50%\u0026rdquo; width=\u0026ldquo;50%\u0026quot;}\nwith the following code\nT = 1000 dt = 1 MC = 200 mu = 0.0 sigma = 0.1 def drift(x, mu, dt): return mu*dt def diffusion(x, sigma, dt): return sigma * dt**0.5*np.random.randn(*x.shape) # start at zero x = [np.zeros(MC)] # simulate Brownian Motion in parallel for _ in range(T): x.append(x[-1]+drift(x[-1], mu, dt) + diffusion(x[-1], sigma, dt)) x = np.array(x) fig = plt.figure(1) ax = fig.add_subplot(111) # plot all the trajectories _ = ax.plot(x, color=\u0026#39;red\u0026#39;, alpha=0.1) # compute the analytical means and variances of the Brownian Motion analytic_var = np.arange(0,T)*dt*sigma**2 analytic_std = analytic_var**0.5 analytic_mean = np.arange(0,T)*dt*mu # compute the empirical mean and variance from the sampled Brownian Motion empiric_var = x.var(axis=1) empiric_std = empiric_var**0.5 empiric_mean = x.mean(axis=1) ax.plot(empiric_mean + 3*empiric_std, ls=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;) ax.plot(empiric_mean - 3*empiric_std, ls=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;) ax.plot(x.mean(axis=1), ls=\u0026#39;-.\u0026#39;, color=\u0026#39;red\u0026#39;) ax.fill_between(x=np.arange(0,T),y1=analytic_mean+3*analytic_std, y2=analytic_mean+-3*analytic_std, color=\u0026#39;gray\u0026#39;, alpha=0.1) plt.plot(analytic_mean, color=\u0026#39;gray\u0026#39;, ls=\u0026#39;-.\u0026#39;) ax.grid(True) ax.set_xlim(-20,100) ax.set_ylim(-4,4) ax.set_xticklabels([],[]) ax.set_yticklabels([],[]) plt.show() We can observe both the sampled mean and the variance (in red) of our 100 Wiener processes match the expected mean and variance (in gray) up to the noise that we introduce through sampling. Basically all the paths stay around zero where they start and they spread out according to our analytical computed variance of $\\mathbb{V}[x_t] = t$ over time.\nSince we chose $\\epsilon$ and $\u0026ldquo;dt\u0026rdquo;$ smartly, we arrive at quite succinct definitions for the mean and variance of this random variable $x_T$. In fact, this specific kind of stochastic process has a specific name. By choosing the random movement $\\epsilon \\sim \\mathcal{N}(0,1)$, the starting value $x_0=0$ and the time differential $\\sqrt{dt}$ we have defined our little, rambunctious particle to follow a Wiener process which is a specific kind of stochastic process. The defining properties of a Wiener process $W_t$ ( $W_t$ being the common notation of a Wiener process) that describes the infinitesimal movement of a particle through $dx_t = \\epsilon \\sqrt{dt}$ are the following:\n  $W_0 = 0$ \\ This means that the Wiener process always starts at zero.\n  Independent increments: $\\mathbb{C}[W_{t+u} - W_s, W_s] =0$ for $u \\geq 0$ and $s \\leq t$ \\ Increments (the movement of the Wiener process) are independent from the past movements. $\\mathbb{C}[\\cdot , \\cdot ]$ is the covariance between two random variables.\n  Gaussian increments: $W_{t+u} - W_t \\sim \\mathcal{N}(0, u)$ \\ The difference between any two realizations is Gaussian distributed accordingly to the time difference between these two realizations.\n  Continuous paths in time $t$.\\ We can basically zoom infinitely far into the movements on the time axis and we will never find a discontinuous jump. Yet, due to it being a stochastic process it turns out that the Wiener process is not differentiable.\n  In order to be all set up for the final chapter of this post, we will define an infinitesimal version of the Gaussian increment property of the Wiener process: $$ \\begin{align} W_{t+u} - W_t \\sim \\mathcal{N}(0, u) \\quad \\Leftrightarrow \\quad dW_t \\sim \\mathcal{N}(0,dt) \\end{align} $$\nStochastic Differential Equations (= Differential Equations + Wiener Processes) #  Once we understood differential equations and Wiener processes, we\u0026rsquo;ll realize that (basic) stochastic differential equations are just the combination of the two. We can thus define a stochastic differential equation as $$ \\begin{align} \\underbrace{dX_t}{\\text{total change}} = \\underbrace{\\mu_t dt}{\\text{deterministic}} + \\underbrace{\\sigma_t dW_t}_{\\text{stochastic}} \\end{align} $$\nwhich defines the infinitesimal change in the random variable $X_t$ at time $t$ as the combination of a deterministic change $\\mu_t dt$ and a scaled Wiener process $\\sigma_t dW_t \\sim \\mathcal{N}(0,\\sigma_t^2 dt)$.\nWith a constant drift, this looks something like this:\n {: .align=\u0026ldquo;center\u0026rdquo; height=\u0026ldquo;50%\u0026rdquo; width=\u0026ldquo;50%\u0026quot;}\nthrough\nT = 1000 dt = 1 MC = 200 mu = 0.1 sigma = 0.1 def drift(x, mu, dt): return mu*dt def diffusion(x, sigma, dt): return sigma * dt**0.5*np.random.randn(*x.shape) # start at zero x = [np.zeros(MC)] # simulate Brownian Motion in parallel for _ in range(T): x.append(x[-1]+drift(x[-1], mu, dt) + diffusion(x[-1], sigma, dt)) x = np.array(x) fig = plt.figure(1) ax = fig.add_subplot(111) # plot all the trajectories _ = ax.plot(x, color=\u0026#39;red\u0026#39;, alpha=0.1) # compute the analytical means and variances of the Brownian Motion analytic_var = np.arange(0,T)*dt*sigma**2 analytic_std = analytic_var**0.5 analytic_mean = np.arange(0,T)*dt*mu # compute the empirical mean and variance from the sampled Brownian Motion empiric_var = x.var(axis=1) empiric_std = empiric_var**0.5 empiric_mean = x.mean(axis=1) ax.plot(empiric_mean + 3*empiric_std, ls=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;) ax.plot(empiric_mean - 3*empiric_std, ls=\u0026#39;--\u0026#39;, color=\u0026#39;red\u0026#39;) ax.plot(x.mean(axis=1), ls=\u0026#39;-.\u0026#39;, color=\u0026#39;red\u0026#39;) ax.fill_between(x=np.arange(0,T),y1=analytic_mean+3*analytic_std, y2=analytic_mean+-3*analytic_std, color=\u0026#39;gray\u0026#39;, alpha=0.1) plt.plot(analytic_mean, color=\u0026#39;gray\u0026#39;, ls=\u0026#39;-.\u0026#39;) ax.grid(True) ax.set_xlim(-20,100) ax.set_ylim(-4,4) ax.set_xticklabels([],[]) ax.set_yticklabels([],[]) plt.show() For such drift-diffusion processes, or more specifically Ito drift-diffusion processes, we can compute the analytical mean and variance of how $X_t$ will be distributed in the future. To keep things simple, we will work with a constant mean $\\mu = \\mu_t$ and diffusion $\\sigma = \\sigma_t$. Solving the SDE amounts to: $$ \\begin{align} X_T \u0026amp;= \\int_{t=0}^T dX_t \\ \u0026amp;= \\int_{t=0}^T \\mu dt + \\sigma dW_t \\ \u0026amp;= \\mu \\int_{t=0}^T dt + \\sigma \\int_{t=0}^T dW_t \\ \u0026amp;= \\mu T + \\sigma \\int_{t=0}^T dW_t \\ \u0026amp;= \\mu T + \\sigma W_T \\ \\end{align} $$\nSimilarly to earlier, the Brownian motion $W_T \\sim \\mathcal{N}(0,T)$ is a random variable, which entices us to compute the mean and variance of the term above: $$ \\begin{align} \\mathbb{E}[X_T] \u0026amp;= \\mathbb{E}[\\mu T + \\sigma W_T] \\ \u0026amp;= \\mu T + \\sigma \\mathbb{E}[W_T] \\ \u0026amp;= \\mu T \\end{align} $$ and $$ \\begin{align} \\mathbb{V}[X_T] \u0026amp;= \\mathbb{V}[\\mu T + \\sigma W_T] \\ \u0026amp;= \\underbrace{\\mathbb{V}[\\mu T]}_{=0} + \\mathbb{V}[\\sigma W_T] \\ \u0026amp;= \\sigma^2 \\mathbb{V}[ W_T] \\ \u0026amp;= \\sigma^2 T \\ \\mathbb{Std}[X_T] \u0026amp;= \\sigma \\sqrt{T} \\end{align} $$\nwhich we were able to validate with our \u0026ldquo;computational evidence\u0026rdquo; in the plot above. The mean increases constantly as time progresses and the standard deviation above and below the mean increases asymptotically due to the $\\sqrt{T}$.\nThis is a fairly simple SDE since we assume that $\\mu_t$ and $\\sigma_t$ are constant in time and do not depend on the value of $X_t$. Things get significantly more interesting when both $\\mu_t$ and $\\sigma_t$ change over time depending on the value of $X_t$ such that we are working with $$ \\begin{align} dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t \\end{align} $$\nThe drift $\\mu(t, X_t)$ and $\\sigma(t, X_t)$ can now be potentially highly non-linear and complex functions which could even take in other stochastic processes as additional input. But Ito\u0026rsquo;s lemma, Ornstein-Uhlenbeck processes and Geometric Brownian Motion are topics for another time \u0026hellip;\n"},{"id":36,"href":"/blog/docs/matrix/types/","title":"å„ç§çŸ©é˜µ","section":"Matrix","content":"Created: Decemenber, 1, 2020 å¯¹ç§°çŸ©é˜µ #    å®šä¹‰ï¼šæ»¡è¶³$A^T=A$çš„çŸ©é˜µã€‚ æ˜¾ç„¶ï¼Œå¯¹ç§°çŸ©é˜µæ˜¯æ–¹é˜µï¼Œå¦åˆ™è½¬ä¸€ä¸‹å½¢çŠ¶éƒ½å˜äº†ã€‚  å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å€¼/å‘é‡æœ‰ç€å¾ˆå¥½çš„æ€§è´¨ï¼š\n å®å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚ å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ã€‚æ²¡éªŒè¯ï¼Œä¸è¿‡åº”è¯¥æ˜¯ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ä¸€å®šæ˜¯æ­£äº¤çš„ï¼ŒåŒä¸€ä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å¯èƒ½éœ€è¦Schmidtæ­£äº¤åŒ–ã€‚  è¯æ˜ 1. å®å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°\nå·²çŸ¥$Ax = \\lambda x$ï¼Œå·¦å³ä¸¤è¾¹åŒæ—¶å–å…±è½­ï¼Œæœ‰\n$$ \\bar{A}\\bar{x}=\\bar{\\lambda}\\bar{x} $$\nå› ä¸ºAæ˜¯å®å¯¹ç§°çŸ©é˜µï¼Œ$\\bar{A}=A$ï¼Œå› æ­¤æœ‰ $$ A\\bar{x}=\\bar{\\lambda}\\bar{x} $$\nå·¦å³ä¸¤è¾¹åŒæ—¶å–è½¬ç½®ï¼Œæœ‰\n$$ \\bar{x}^TA=\\bar{x}^T\\bar{\\lambda} $$\nå·¦å³ä¸¤è¾¹åŒæ—¶å³ä¹˜xï¼Œå¾— $$ \\bar{x}^TAx=\\bar{x}^T\\bar{\\lambda}x $$\nå¯¹$Ax = \\lambda x$ï¼Œå·¦å³ä¸¤è¾¹åŒæ—¶å·¦ä¹˜$\\bar{x}^T$ï¼Œå¾— $$ \\bar{x}^TAx=\\bar{x}^T\\lambda x $$\næ¯”è¾ƒä¸¤ä¸ªå¼å­ï¼Œå¯ä»¥å¾—åˆ°\n$$ \\bar{\\lambda} = \\lambda $$\nä¸€ä¸ªæ•°çš„å…±è½­ç­‰äºå…¶æœ¬èº«ï¼Œè¿™ä¸ªæ•°æ˜¯å®æ•°ã€‚\n2. å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„\n  æ­£å®šçŸ©é˜µ #   æ­£å®šçŸ©é˜µæ˜¯å¯¹ç§°çŸ©é˜µçš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå› æ­¤å®ƒä¹Ÿæ˜¯å¯¹ç§°çš„ã€‚\nå®šä¹‰ #  æ­£å®šçŸ©é˜µæœ‰å„ç§å„æ ·çš„å®šä¹‰ï¼Œå®ƒä»¬äº’ç›¸ä¹‹é—´æ˜¯ç­‰ä»·çš„ã€‚ä¸‹é¢æ˜¯å‡ ç§å¸¸è§å®šä¹‰/åˆ¤å®šæ–¹æ³•ï¼š\n æ‰€æœ‰ç‰¹å¾å€¼å¤§äº0ã€‚ æ‰€æœ‰é¡ºåºä¸»å­å¼éƒ½å¤§äº0ã€‚ æ‰€æœ‰Pivotséƒ½å¤§äº0ã€‚ $x^TAx\u0026gt;0$ ï¼š $x=[x_1,x_2, \u0026hellip;, x_n]$, å¯¹äºæ‰€æœ‰çš„éé›¶xæ’å¤§äº0  ç±»ä¼¼çš„å¯ä»¥å®šä¹‰åŠæ­£å®š(å¤§äºç­‰äº0)ï¼Œè´Ÿå®šï¼ˆå°äº0ï¼‰ï¼ŒåŠè´Ÿå®šï¼ˆå°äºç­‰äº0ï¼‰ã€‚\nä¸Šè¿°å››ä¸ªå®šä¹‰ç›¸å…³æ€§çš„ä¸ä¸¥æ ¼è¯æ˜ è€ƒè™‘ç¬¬å››ä¸ªå®šä¹‰ï¼Œæˆ‘ä»¬çŸ¥é“æ­£å®šçŸ©é˜µå°±æ˜¯äºŒæ¬¡å‹çš„ç³»æ•°çŸ©é˜µï¼Œå½“äºŒæ¬¡å‹è¦æ’å¤§äº0ï¼Œåˆ™å®ƒå¿…å®šå¯ä»¥åŒ–æˆä¸€ç³»åˆ—å¹³æ–¹çš„å’Œï¼Œä¸”æ¯ä¸€é¡¹çš„ç³»æ•°éƒ½å¤§äº0ï¼ˆå¯ä»¥ç­‰äº0å—ï¼Ÿä¸è¡Œï¼Œå¹³æ–¹é¡¹æ˜¯å¯ä»¥ç­‰äº0çš„ï¼Œå¦‚æœæœ‰ä¸€é¡¹ç³»æ•°æ˜¯0ï¼Œå°±å¤šäº†ä¸€ä¸ªè‡ªç”±å˜é‡ï¼Œå°±å¯ä»¥æ„é€ å‡ºç­‰äº0çš„xï¼‰ï¼Œè€ŒåŒ–ä¸ºå¹³æ–¹å’Œå½¢å¼çš„è¿‡ç¨‹ï¼Œç­‰ä»·äºå¯¹Aåšeliminationï¼Œå¹³æ–¹é¡¹çš„ç³»æ•°å°±æ˜¯eliminationä¹‹åçš„pivotsï¼Œé‡Œé¢çš„ç³»æ•°æ˜¯eliminationè¿‡ç¨‹çš„LçŸ©é˜µã€‚\nä¾‹ï¼š\n  ç‰¹æ®Šæƒ…å†µ #   $A^TA$å¿…å®šæ˜¯åŠæ­£å®šçŸ©é˜µï¼Œå¦‚æœA(m*nçŸ©é˜µ)çš„ç§©ä¸ºnï¼Œåˆ™å¿…å®šæ˜¯æ­£å®šçŸ©é˜µã€‚  è¯æ˜ ç›´è§‚çš„ï¼Œè¿™ä¸ªç›¸å½“äºçŸ©é˜µå½¢å¼çš„å¹³æ–¹ã€‚æˆ‘ä»¬çŸ¥é“æ ‡é‡ï¼Œ$a^2\u0026gt;0$ï¼Œå¯¹åº”çš„ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡$A^TA$æ˜¯åº”è¯¥æ˜¯æ­£å®šçš„ã€‚\nå›å¿†æ­£å®šçŸ©é˜µçš„å››ç§å®šä¹‰ï¼šç‰¹å¾å€¼ä¸çŸ¥é“ï¼Œpivotsä¸çŸ¥é“ï¼Œé¡ºåºä¸»å­å¼ä¸çŸ¥é“ï¼Œå› æ­¤è€ƒè™‘æœ€åä¸€ç§å½¢å¼ã€‚\nä¸¥æ ¼è¯æ˜ï¼š\nè€ƒè™‘çŸ©é˜µ$A\\in R^{m\\times n}$\n$$ x^TA^TAx = (Ax)^TAx = ||Ax||^2 \\geq 0 $$\nä¸Šå¼ï¼Œåªæœ‰åœ¨Ax=0çš„æ—¶å€™æ‰å–0ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›$A^TA$æ˜¯æ­£å®šçš„ï¼Œè¿™æ„å‘³ç€Ax=0ä¸èƒ½æœ‰è§£ï¼Œè€Œæ— è§£çš„æ¡ä»¶æ˜¯Açš„åˆ—å‘é‡çº¿æ€§æ— å…³ï¼Œå³Açš„ç§©ä¸ºnã€‚\n  æ­£äº¤çŸ©é˜µ #   æ­£äº¤çŸ©é˜µï¼ˆè‹±è¯­ï¼šorthogonal1 matrixï¼‰æ˜¯ä¸€ä¸ªæ–¹å—çŸ©é˜µQï¼Œå…¶å…ƒç´ ä¸ºå®æ•°ï¼Œè€Œä¸”è¡Œå‘é‡ä¸åˆ—å‘é‡çš†ä¸ºæ­£äº¤çš„å•ä½å‘é‡ã€‚\n ç›¸åŒè¡Œ/åˆ—å‘é‡å†…ç§¯ä¸º1 ä¸åŒè¡Œ/åˆ—å‘é‡å†…ç§¯ä¸º0  æ˜¾ç„¶ï¼Œæ¯ä¸ªè¡Œ/åˆ—å‘é‡éƒ½ä¸èƒ½ç”±å…¶ä»–è¡Œ/åˆ—å‘é‡çº¿æ€§è¡¨å‡ºï¼Œå› æ­¤æ­£äº¤çŸ©é˜µå¿…å®šæ˜¯æ»¡ç§©çŸ©é˜µã€‚\nè¡Œå‘é‡æ­£äº¤èƒ½å¦æ¨å‡ºåˆ—å‘é‡æ­£äº¤? å¯ä»¥ï¼Œæ¨å¯¼å¦‚ä¸‹ï¼š\nè¡Œå‘é‡æ­£äº¤ï¼Œå› æ­¤æœ‰$AA^T = I$, å¯¹äºæ–¹é˜µï¼Œæœ‰$AB = I \\simeq BA=I$ ( è¯æ˜)ï¼Œ åˆ™æœ‰$A^TA=I$ï¼Œåˆ™æœ‰åˆ—å‘é‡æ­£äº¤ã€‚\n é‡è¦æ€§è´¨ #   æ­£äº¤çŸ©é˜µçš„è½¬ç½®çŸ©é˜µç­‰äºå…¶é€†çŸ©é˜µ: $A^T = A^{-1}$ è¡Œåˆ—å¼å€¼å¿…å®šä¸º+1æˆ–-1 $$ 1=\\operatorname{det}(I)=\\operatorname{det}\\left(Q^{T} Q\\right)=\\operatorname{det}\\left(Q^{T}\\right) \\operatorname{det}(Q)=(\\operatorname{det}(Q))^{2} \\Rightarrow \\operatorname{det}(Q)=\\pm 1 $$    è™½ç„¶æ›´å®Œæ•´çš„è¯´æ³•æ˜¯orthonormal matrix(æ ‡å‡†æ­£äº¤çŸ©é˜µ)ï¼Œä½†æˆ‘ä»¬è¯´æ­£äº¤çŸ©é˜µçš„æ—¶å€™ä¸€èˆ¬éƒ½æ˜¯æŒ‡æ ‡å‡†æ­£äº¤ã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":37,"href":"/blog/docs/matrix/svd/","title":"å¥‡å¼‚å€¼åˆ†è§£","section":"Matrix","content":"Created: Decemenber, 1, 2020 æ‰€æœ‰çŸ©é˜µéƒ½å¯ä»¥åˆ†è§£æˆï¼šæ­£äº¤çŸ©é˜µ * å¯¹è§’çŸ©é˜µ * æ­£äº¤çŸ©é˜µï¼Œè¿™ä¸ªåˆ†è§£å«å¥‡å¼‚å€¼åˆ†è§£ã€‚\n$$ A = U\\Sigma V^T $$\nå…¶ä¸­Aæ˜¯ä¸€ä¸ªm*nçš„çŸ©é˜µï¼ŒVæ˜¯ä¸€ä¸ªn*nçš„çŸ©é˜µï¼ŒUæ˜¯ä¸€ä¸ªm*mçš„çŸ©é˜µï¼Œ$\\Sigma$æ˜¯ä¸€ä¸ªm*nçš„çŸ©é˜µã€‚\nåœ¨ä¸€äº›ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œå¦‚Aæ˜¯å¯¹ç§°çŸ©é˜µï¼Œå®ƒçš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ï¼Œå¥‡å¼‚å€¼åˆ†è§£é€€åŒ–ä¸ºçŸ©é˜µå¯¹è§’åŒ–ï¼Œè®¾Qæ˜¯æ˜¯Açš„ç‰¹å¾å‘é‡ï¼ˆåˆ—å‘é‡ï¼‰æ„æˆçš„çŸ©é˜µã€‚åˆ™æœ‰\n$$ A = U\\Sigma V^T = Q \\Lambda Q^T $$\nIntuition - å‡ ä½•å«ä¹‰ #   å½“æˆ‘ä»¬åœ¨åšå¥‡å¼‚å€¼åˆ†è§£çš„æ—¶å€™ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨åšä»€ä¹ˆï¼Ÿ\n åœ¨MIT Linear Algebra ç¬¬29è®²ï¼ŒStrangæ•™æˆç»™äº†æˆ‘ä»¬ä¸€ä¸ªå¾ˆç”ŸåŠ¨çš„è§£é‡Šã€‚  è¯¦ç»†è§£é‡Š Revision needed\nè€ƒè™‘ä¸¤ä¸ªçº¿æ€§ç©ºé—´$R^n$å’Œ$R^m$, å…¶ä¸­$V=[v_1,v_2,\u0026hellip;,v_n]$æ˜¯$R^n$çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹è¿™ä¸ªåŸºåº•åšä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°$R^m$ç©ºé—´çš„ä¸€ç»„åŸºåº•ï¼Œè¿™ä¸ªåŸºåº•ä¹Ÿæ˜¯æ­£äº¤çš„ã€‚\nè¿™ä¸ªæ–°åŸºåº•æ˜¯$R^m$ç©ºé—´ä¸‹æŸä¸ªæ ‡å‡†æ­£äº¤åŸºä¹˜ä¸ŠæŸä¸ªç³»æ•°çŸ©é˜µï¼Œç”¨çŸ©é˜µçš„è¯­è¨€æ¥è¯´ï¼Œæˆ‘ä»¬å®é™…æœ‰è¿™ä¸ªç­‰å¼ï¼š\n$$ AV = U\\Sigma $$\nå…¶ä¸­Aè¡¨ç¤ºçº¿æ€§å˜æ¢ï¼ŒVæ˜¯$R^n$çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºï¼ŒUæ˜¯$R^m$çš„ä¸€ç»„æ ‡å‡†æ­£äº¤åŸºï¼Œ$\\Sigma$æ˜¯ç³»æ•°å¯¹è§’é˜µã€‚\næ¢å¥è¯è¯´ï¼Œå¯¹æ¯ä¸€ä¸ªm*nçš„çŸ©é˜µï¼Œå®ƒå…¶å®éƒ½æ˜¯å…¶è¡Œç©ºé—´$R^n$å’Œåˆ—ç©ºé—´$R^m$æŸä¸¤ä¸ªæ­£äº¤åŸºåº•çš„å˜æ¢çŸ©é˜µï¼Œå¥‡å¼‚å€¼åˆ†è§£å…¶å®å°±è¦æ‰¾åˆ°è¿™ä¸¤ä¸ªåŸºåº•ã€‚\n  æ±‚è§£ #   ç®€å•æ¥è¯´ï¼š\n Væ˜¯$A^TA$çš„ç‰¹å¾å‘é‡æ„æˆçš„çŸ©é˜µã€‚ Uæ˜¯$AA^T$çš„ç‰¹å¾å‘é‡æ„æˆçš„çŸ©é˜µã€‚ $\\Sigma$æ˜¯$A^TA$å’Œ$AA^T$çš„ç‰¹å¾å€¼ï¼ˆäºŒè€…ç‰¹å¾å€¼ä¸€è‡´ï¼‰ã€‚  è¯¦ç»†æ¨å¯¼ ç›´æ¥æ±‚è§£ä¸‹å¼ï¼Œå¹¶ä¸å¥½æ±‚ï¼Œå› ä¸ºå®ƒåŒæ—¶åŒ…å«å¾ˆå¤šä¸ªæœªçŸ¥é‡ã€‚\n$$ A = U\\Sigma V^T $$\næˆ‘ä»¬çŸ¥é“$A^TA$å…·æœ‰å¾ˆå¥½çš„æ€§è´¨ï¼Œè€ƒè™‘\n$$ A^TA = V\\Sigma^T U^TU\\Sigma V^T = V\\Sigma^T\\Sigma V^T $$\næˆ‘ä»¬çŸ¥é“$A^TA$æ˜¯å¯¹ç§°çŸ©é˜µï¼Œè€Œå¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ï¼Œè€Œæˆ‘ä»¬æ­£å¥½è¦æ±‚Væ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼å› æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“$A^TA$çš„ç‰¹å¾å‘é‡æ„æˆçš„çŸ©é˜µå°±æ˜¯ï¼ˆæ»¡è¶³è¦æ±‚ï¼Œä½†å”¯ä¸€æ€§éœ€è¦è¯æ˜ï¼‰æˆ‘ä»¬è¦æ±‚çš„Vã€‚$\\Sigma^T\\Sigma$åˆ™æ˜¯$A^TA$çš„ç‰¹å¾å€¼ã€‚\nç±»ä¼¼çš„ï¼Œè€ƒè™‘$AA^T$ï¼Œæˆ‘ä»¬æœ‰\n$$ AA^T = U\\Sigma^T V^TV\\Sigma U^T = U\\Sigma^T\\Sigma U^T $$\nå³Uæ˜¯$AA^T$çš„ç‰¹å¾å‘é‡æ„æˆçš„çŸ©é˜µï¼Œ$\\Sigma^T\\Sigma$åˆ™æ˜¯$AA^T$çš„ç‰¹å¾å€¼ã€‚\næˆ‘ä»¬è¿™æ—¶å€™å…¶å®å¾—åˆ°äº†ä¸€ä¸ªé™„å±ç»“è®ºï¼Œé‚£å°±æ˜¯$AA^T$å’Œ$A^TA$çš„ç‰¹å¾å€¼æ˜¯ç›¸åŒçš„ã€‚     å¯¹äºå¤çŸ©é˜µï¼ˆé…‰çŸ©é˜µï¼‰ï¼Œåªéœ€è¦å°†è½¬ç½®Tæ”¹æˆå…±è½­è½¬ç½®Hå³å¯ã€‚  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶ä¸æ˜¯ä»»å–nä¸ª$A^TA$çš„ä¸¤ä¸¤æ­£äº¤å•ä½é•¿çš„ç‰¹å¾å‘é‡éƒ½å¯ä»¥ä½œä¸ºVçš„åˆ—å‘é‡ã€‚$V_1$å’Œ$U_1$å¿…é¡»è¦æ»¡è¶³ä¸‹é¢çš„å…³ç³»æ‰è¡Œ:\n$$ V_1 = A^HU_1\\Delta^{-1} $$\nå…¶ä¸­$U_1$, $V_1$æ˜¯éé›¶ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ç»„ã€‚\n"},{"id":38,"href":"/blog/docs/matrix/road/","title":"å¦‚ä½•å°†å„ä¸ªçŸ¥è¯†ç‚¹ä¸²èµ·æ¥ï¼Ÿ","section":"Matrix","content":"Created: Decemenber, 3, 2020 å¼•å…¥ ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“çš„æ¨å¯¼å‡º çŸ©é˜µå¯¹è§’åŒ–çš„å…¬å¼ã€‚\nçŸ©é˜µå¯¹è§’åŒ–æ˜¯é’ˆå¯¹ä¸€èˆ¬æ–¹é˜µçš„ï¼Œå¯¹äºç‰¹æ®Šæ–¹é˜µï¼Œå¦‚ å¯¹ç§°çŸ©é˜µï¼ŒçŸ©é˜µå¯¹è§’åŒ–å¯ä»¥è¿›ä¸€æ­¥ç‰¹åŒ–ã€‚\nå¯¹ç§°çŸ©é˜µæœ‰å¾ˆå¤šå¾ˆå¥½çš„æ€§è´¨ï¼Œå®ƒæ˜¯å®ƒçš„è¿›ä¸€æ­¥ç‰¹ä¾‹ï¼Œ æ­£å®šçŸ©é˜µæœ‰ç€æ›´å¥½çš„æ€§è´¨ã€‚\n#  "},{"id":39,"href":"/blog/docs/matrix/misc/","title":"æ‚ä¸ƒæ‚å…«","section":"Matrix","content":"Created: Decemenber, 1, 2020 Quick Look #   é…‰çŸ©é˜µ: $A^HA=E$ æ­£è§„çŸ©é˜µ: $A^HA=AA^H$, åŒ…æ‹¬å¯¹ç§°çŸ©é˜µï¼Œåå¯¹ç§°çŸ©é˜µï¼ŒHermiteçŸ©é˜µï¼ŒåHermiteçŸ©é˜µï¼Œæ­£äº¤çŸ©é˜µï¼Œé…‰çŸ©é˜µç­‰ã€‚ é…‰å¯¹è§’åŒ–ï¼š $A=Q\\Lambda Q^H$, æ™®é€šå¯¹è§’åŒ–: $A=Q\\Lambda Q^{-1}$ å•çº¯çŸ©é˜µï¼šå¯ä»¥å¯¹è§’åŒ–çš„çŸ©é˜µï¼Œå³æœ‰nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚  çº¿æ€§ä»£æ•°ä¸­å¸¸è§çš„ç­‰ä»·å…³ç³» #    singular matrix = å¥‡å¼‚çŸ©é˜µ = ä¸å¯é€†çŸ©é˜µ\n  Aä¸å¯é€† \u0026lt;-\u0026gt;1 Ax = 0 æœ‰éé›¶è§£ \u0026lt;-\u0026gt;2 ç‰¹å¾å€¼=0\n  Spectrum #  çŸ©é˜µä¸­æœ‰è°±ï¼ˆspectrumï¼‰è¿™ä¸ªæ¦‚å¿µï¼Œä¾‹å¦‚è°±åˆ†è§£ï¼Œè°±èŒƒæ•°ç­‰ç­‰ã€‚\nè¿™ä¸ªè°±å…¶å®è¯´çš„å°±æ˜¯ç‰¹å¾å€¼ï¼Œç‰¹å¾å‘é‡ï¼Œå®ƒæ˜¯ä»å…‰å­¦é‡Œå€Ÿè¿‡æ¥çš„è¯´æ³•ã€‚å…‰å­¦ä¸­å…‰æ˜¯ç”±å„ç§pure lightæ„æˆçš„ï¼Œå…‰è°±å°±æ˜¯å„ä¸ªæˆåˆ†çš„å æ¯”ã€‚è€Œåœ¨çŸ©é˜µä¸­ï¼ŒçŸ©é˜µç”±å„ä¸ªpureçš„componentï¼Œç‰¹å¾å€¼ç‰¹å¾å‘é‡æ„æˆã€‚\nè¿›ä¸€æ­¥æ¥è¯´ï¼Œæˆ‘ä»¬çœ‹çŸ©é˜µå¯¹è§’åŒ–ï¼Œæˆ‘ä»¬çŸ¥é“ä»»ä½•ä¸€ä¸ªå¯¹ç§°çŸ©é˜µAéƒ½å¯ä»¥å¯¹è§’åŒ–æˆ$Q\\Lambda Q^{T}$ï¼Œå…¶ä¸­Qä¸ºç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µã€‚\næˆ‘ä»¬è¿›ä¸€æ­¥æŠŠè¿™ä¸ªå¼å­æ‹†å¼€ï¼Œæˆ‘ä»¬è®¾Açš„ç‰¹å¾å‘é‡ä¸º\u0008$q_i$, åˆ™\n$$ A = Q\\Lambda Q^{T} = \\lambda_1 q_1 q_1^T + \\lambda_2 q_2 q_2^T + \u0026hellip; + \\lambda_n q_n q_n^T $$\n$q_i$æ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œ$q_1 q_1^T$æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå› æ­¤Aå®é™…ä¸Šå°±åŒ–æˆäº†nä¸ªçŸ©é˜µçš„çº¿æ€§ç»„åˆï¼Œç¬¬iä¸ªçŸ©é˜µæ˜¯$q_i q_i^T$ï¼Œç³»æ•°æ˜¯ç‰¹å¾å€¼$\\lambda_i$ã€‚\n  Aå¯é€†çš„è¯ï¼Œæ„å‘³ç€ç®€åŒ–é˜¶æ¢¯å½¢æœ€åä¸€è¡Œä¸å…¨ä¸º0ï¼Œä¸ºäº†æ»¡è¶³Ax=0ï¼Œxå¿…é¡»ç­‰äº0ã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n TODO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":40,"href":"/blog/docs/matrix/eigen/","title":"ç‰¹å¾å€¼\u0026\u0026ç‰¹å¾å‘é‡","section":"Matrix","content":"Created: Decemenber, 1, 2020 é¦–å…ˆæ˜ç¡®ä¸€ç‚¹ï¼Œç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ˜¯é’ˆå¯¹æ–¹é˜µæ¥è¯´çš„ï¼Œæˆ‘ä»¬è¯´æŸä¸ªçŸ©é˜µAæœ‰ä»€ä¹ˆç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæš—ç¤ºäº†çŸ©é˜µAæ˜¯ä¸€ä¸ªæ–¹é˜µã€‚\nå®šä¹‰ #  ä»»ä½•æ»¡è¶³ä¸‹åˆ—æ¡ä»¶çš„æ ‡é‡å’Œå‘é‡åˆ†åˆ«ä¸ºçŸ©é˜µAçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚1\n$$ Ax = \\lambda x $$\nIntuition #  å°†ä¸€ä¸ªçŸ©é˜µAä¸ä¸€ä¸ªå‘é‡xç›¸ä¹˜ï¼Œæˆ‘ä»¬å®é™…ä¸Šåœ¨å¯¹å‘é‡xåšä¸€ä¸ª çº¿æ€§å˜æ¢ï¼Œç‰¹å¾å‘é‡å®é™…ä¸Šå°±æ˜¯ç»è¿‡è¿™ä¸ªå˜æ¢ä¸åŸå‘é‡ä»ç„¶å¹³è¡Œçš„é‚£äº›å‘é‡ï¼Œç‰¹å¾å€¼å°±æ˜¯å‰åä¸¤ä¸ªå‘é‡é•¿åº¦çš„ä¸€ä¸ªæ¯”å€¼ã€‚\n æ˜¾ç„¶ï¼Œé›¶å‘é‡æ»¡è¶³æ¡ä»¶ï¼Œé‚£å®ƒæ˜¯ä»»ä½•çŸ©é˜µçš„ç‰¹å¾å‘é‡å—ï¼Ÿ\nä¸æ˜¯ï¼Œæˆ‘ä»¬æ˜¾ç¤ºå°†ç‰¹å¾å‘é‡å®šä¹‰ä¸ºéé›¶å‘é‡ã€‚\n æ±‚è§£ #  æ ¹æ®å®šä¹‰ï¼Œæœ‰\n$$ (A-\\lambda I)x = 0 $$\nè¿™ä¸ªç­‰å¼è¦æœ‰éé›¶è§£ï¼Œ$(A-\\lambda I)$å¿…é¡»æ˜¯singularï¼ˆä¸å¯é€†ï¼‰çš„ï¼Œç‰¹å¾å€¼è¦ç­‰äº0ï¼š\n$$ Det(A-\\lambda I) = 0 $$\näºæ˜¯ï¼š\n æ±‚è¡Œåˆ—å¼å¹¶è§£æ–¹ç¨‹å³å¯æ±‚å‡ºç‰¹å¾å€¼ã€‚ å¯¹äºç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å›ä»£ç‰¹å¾å€¼ï¼Œè§£$(A-\\lambda I)x=0$è¿™ä¸ªæ–¹ç¨‹ã€‚  å› ä¸º$(A-\\lambda I)$ä¸å¯é€†ï¼Œæ‰€ä»¥è§£æœ‰æ— ç©·å¤šä¸ªï¼Œå³ç‰¹å¾å‘é‡æœ‰æ— ç©·å¤šä¸ªï¼Œè¿™æ—¶å€™æˆ‘ä»¬åªéœ€è¦ç»™å‡ºä¸€ä¸ªå°±è¡Œäº†ã€‚    æ€§è´¨ #   $\\sum \\lambda = trace(A)$ : çŸ©é˜µAçš„æ‰€æœ‰ç‰¹å¾å€¼ä¹‹å’Œç­‰äºçŸ©é˜µAçš„ traceï¼ˆå¯¹è§’çº¿å…ƒç´ ä¹‹å’Œï¼‰ã€‚ $\\prod \\lambda = det(A)$ : çŸ©é˜µAçš„æ‰€æœ‰ç‰¹å¾å€¼çš„ä¹˜ç§¯ç­‰äºçŸ©é˜µAçš„è¡Œåˆ—å¼ã€‚ ä¸‰è§’çŸ©é˜µçš„ç‰¹å¾å€¼å°±æ˜¯å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ã€‚ å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ã€‚  è¯æ˜ï¼šå¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ To be done     ç”±ç‰¹å¾å‘é‡çš„å®šä¹‰ï¼Œæ˜¾ç„¶Aå¿…é¡»è¦ä¸ªæ–¹é˜µï¼Œè‹¥ä¸æ˜¯ï¼ŒAä¸xç›¸ä¹˜ï¼Œxç»´æ•°å°±å˜äº†ã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":41,"href":"/blog/docs/matrix/similar/","title":"ç›¸ä¼¼çŸ©é˜µ","section":"Matrix","content":"Created: Decemenber, 7, 2020 æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„ä¸¤ä¸ªçŸ©é˜µï¼Œæˆ‘ä»¬ç§°å®ƒä»¬æ˜¯ç›¸ä¼¼çš„ã€‚\n$$ B = M^{-1}AM $$\næ¢å¥è¯è¯´ï¼Œå¦‚æœæŸä¸ªçŸ©é˜µå¯ä»¥ç”±å¦ä¸€ä¸ªçŸ©é˜µå·¦ä¹˜ä¸€ä¸ªçŸ©é˜µå†å³ä¹˜è¿™ä¸ªçŸ©é˜µçš„é€†ï¼Œåˆ™ç§°å®ƒä»¬æ˜¯ç›¸ä¼¼çš„ã€‚\nIntuition #  ç›¸ä¼¼çš„çŸ©é˜µå…·æœ‰å¾ˆå¤šç›¸ä¼¼çš„å±æ€§ã€‚\næˆ‘ä»¬å¯ä»¥æŠŠæ¯ä¸€ä¸ªçŸ©é˜µå’Œå…¶ç›¸ä¼¼çš„çŸ©é˜µï¼ˆä»¬ï¼‰éƒ½æƒ³æˆä¸€ä¸ªfamilyï¼Œè¿™ä¸ªfamilyé‡Œå¯èƒ½æœ‰ä¸€äº›çŸ©é˜µå¾ˆç‰¹æ®Šï¼Œé€šè¿‡ç ”ç©¶è¿™äº›ç‰¹æ®Šçš„çŸ©é˜µï¼ˆé€šå¸¸æ›´ä¸ºå®¹æ˜“ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åæ¨å‡ºåŸçŸ©é˜µçš„ä¸€äº›å±æ€§ã€‚\n çŸ©é˜µå¯¹è§’åŒ–$\\Lambda = S^{-1}AS$çš„å¯¹è§’çŸ©é˜µå°±æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç›¸ä¼¼çŸ©é˜µã€‚   æ€§è´¨ #   ç›¸ä¼¼çŸ©é˜µçš„ç‰¹å¾å€¼æ˜¯ç›¸åŒçš„ï¼ˆç‰¹å¾å‘é‡å¯èƒ½ä¸åŒ1,ä½†ç›¸äº’ç‹¬ç«‹çš„æ•°é‡æ˜¯ç›¸åŒçš„ï¼‰  è¯æ˜ è¯æ˜è¦åˆ†ä¸¤ä¸ªæ–¹å‘ï¼ŒAçš„ç‰¹å¾å€¼æ˜¯Bçš„ç‰¹å¾å€¼ï¼ŒBçš„ç‰¹å¾å€¼ä¹Ÿæ˜¯Açš„ï¼Œç”±è¿™ä¸¤è€…å³å¯å¾—åˆ°äºŒè€…ç‰¹å¾å€¼ç›¸ç­‰ã€‚\nå¯¹äºåŸçŸ©é˜µA, å…¶ç‰¹å¾å€¼æ»¡è¶³ï¼š\n$$ Ax = \\lambda x $$\nå¯¹äºBï¼š\n$$ (M^{-1}AM)M^{-1}x = M^{-1} \\lambda x = \\lambda M^{-1} x $$ $$ BM^{-1}x = \\lambda M^{-1} x $$\näºæ˜¯ï¼Œæˆ‘ä»¬çŸ¥é“Açš„ç‰¹å¾å€¼ä¹Ÿæ˜¯Bçš„ç‰¹å¾å€¼ï¼Œä¸”\u0008Bçš„ç‰¹å¾å‘é‡æ˜¯Açš„ç‰¹å¾å‘é‡ä¹˜ä»¥$M^{-1}$ã€‚\nåå‘å¾ˆå®¹æ˜“ï¼ŒæŠŠMä¹˜åˆ°Bé‚£è¾¹å¾—åˆ°$MBM^{-1} = A$ï¼Œç”¨ç±»ä¼¼çš„æ–¹æ³•å³å¯è¯æ˜ã€‚\n     è¿˜æ˜¯ä¸€å®šä¸åŒï¼ŸTODO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  "},{"id":42,"href":"/blog/docs/matrix/diagonalization/","title":"çŸ©é˜µå¯¹è§’åŒ–","section":"Matrix","content":"Created: Decemenber, 1, 2020  To be continue.  Keypoint: ä¸€äº›çŸ©é˜µå¯ä»¥å¯¹è§’åŒ– $S^{-1}AS = \\Lambda$ã€‚å…¶ä¸­Sä¸ºç‰¹å¾å‘é‡(åˆ—å‘é‡)æ„æˆçš„çŸ©é˜µï¼Œ$\\Lambda$ä¸ºç‰¹å¾å€¼æ„æˆçš„å¯¹è§’çŸ©é˜µã€‚\n ä»€ä¹ˆæ ·çš„çŸ©é˜µå¯ä»¥ï¼Ÿ å› ä¸ºå®šä¹‰é‡ŒSè¦å¯é€†ï¼Œæ‰€ä»¥Aå¿…é¡»æ‹¥æœ‰nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚   å…³äºçŸ©é˜µå¯¹è§’åŒ–ï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥ä»ä¸¤ä¸ªè§’åº¦ç†è§£ï¼š\n ä¸€äº›çŸ©é˜µå¯ä»¥å¯¹è§’åŒ–$S^{-1}AS = \\Lambda$ã€‚ æˆ–ä¸€äº›çŸ©é˜µå¯ä»¥åŒ–æˆä¸‰ä¸ªçŸ©é˜µçš„ä¹˜ç§¯ï¼š$A = S\\Lambda S^{-1}$  æ¨å¯¼ #   $$ \\begin{equation} \\begin{aligned} AS \u0026= A[x_1, x_2, ..., x_n] = [Ax_1, Ax_2, ..., Ax_n] = [\\lambda_1 x_1, \\lambda_1 x_2, ..., \\lambda_1 x_n] \\\\\\\\\\\\ \u0026= [x_1, x_2, ..., x_n] \\begin{bmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 \\lambda_n\\\\ \\end{bmatrix} = S\\Lambda \\end{aligned} \\end{equation} $$  å·¦å³ä¸¤è¾¹åŒæ—¶ä¹˜Sçš„é€†ï¼Œå³å¯å¾—åˆ°ï¼š\n$$ S^{-1}AS = \\Lambda $$\nç‰¹æ®Šæƒ…å†µ #  å½“çŸ©é˜µAæ˜¯å¯¹ç§°çŸ©é˜µæ—¶ï¼Œå…¶ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„1ã€‚å› æ­¤ï¼Œå¯¹è§’åŒ–å¯ä»¥å†™æˆ\n$$ Q^{T}AQ = \\Lambda $$\n   è¯æ˜\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":43,"href":"/blog/docs/matrix/space/","title":"çº¿æ€§ç©ºé—´å’Œçº¿æ€§å˜æ¢","section":"Matrix","content":"Created: Decemenber, 9, 2020 æŠ½è±¡æ¥è¯´ï¼Œçº¿æ€§ç©ºé—´å°±æ˜¯é‡Œé¢å…ƒç´ æ»¡è¶³ä¸€å®šè¿ç®—è§„å¾‹çš„ä¸€ä¸ªç©ºé—´ã€‚\nå…·ä½“æ¥è¯´ï¼Œç©ºé—´çš„æ¦‚å¿µåœ¨æ•°å­¦ä¸Šå°±æ˜¯ä¸€ä¸ªé›†åˆï¼Œç©ºé—´é‡Œçš„å…ƒç´ å°±æ˜¯é›†åˆé‡Œçš„å…ƒç´ ï¼Œçº¿æ€§ç©ºé—´å°±æ˜¯é‡Œé¢å…ƒç´ æ»¡è¶³ä¸€å®šè¿ç®—è§„å¾‹çš„ä¸€ä¸ªé›†åˆã€‚\nçº¿æ€§ç©ºé—´ #    ç¬¦å·è¯´æ˜ï¼šå®æ•°åŸŸRï¼Œå¤æ•°åŸŸCï¼Œç»Ÿç§°æ•°åŸŸFã€‚\n çº¿æ€§ç©ºé—´æ˜¯æ•°åŸŸFä¸Šæ»¡è¶³åŠ æ³•å’Œæ•°ä¹˜è¿ç®—è§„å¾‹çš„ä¸€ä¸ªé›†åˆã€‚å…·ä½“å®šä¹‰å¦‚ä¸‹ï¼š\n TODO  æ ¸(é›¶)ç©ºé—´ #   æ ¸ç©ºé—´ï¼Œæˆ–è€…è¯´é›¶ç©ºé—´æ˜¯é’ˆå¯¹ä¸€ä¸ªçŸ©é˜µæ¥è¯´çš„ã€‚å¯¹ä¸€ä¸ªçŸ©é˜µAï¼Œå®ƒçš„æ ¸ç©ºé—´å°±æ˜¯Ax=0çš„è§£ç©ºé—´ã€‚\nå¸¸ç”¨ N(A) æ¥è¡¨ç¤ºï¼ŒNå…¶å®å°±æ˜¯è‹±æ–‡é‡ŒNull Spaceçš„é¦–å­—æ¯ã€‚\nåˆ—ç©ºé—´/å€¼åŸŸ #   åŒæ ·æ˜¯é’ˆå¯¹ä¸€ä¸ªçŸ©é˜µæ¥è¯´çš„ï¼Œå…·ä½“ä¸è§£é‡Šã€‚å¸¸ç”¨ R(A) è¡¨ç¤ºã€‚\nå‘é‡ #   æˆ‘ä»¬æŠŠçº¿æ€§ç©ºé—´é‡Œçš„å…ƒç´ ç§°ä¸ºå‘é‡ã€‚\næ³¨æ„ï¼Œè¿™é‡Œçš„å‘é‡æ¯”çº¿æ€§ä»£æ•°é‡Œçš„nå…ƒå‘é‡å«ä¹‰æ›´å¹¿ã€‚  æ¦‚å¿µï¼š\n çº¿æ€§ç»„åˆ/çº¿æ€§è¡¨ç¤º çº¿æ€§ç›¸å…³/æ— å…³  åŸº #   ä¸€ä¸ªçº¿æ€§ç©ºé—´çš„åŸºï¼Œæ˜¯å¯ä»¥çº¿æ€§è¡¨å‡ºè¿™ä¸ªçº¿æ€§ç©ºé—´ä¸­å…¶å®ƒæ‰€æœ‰å‘é‡çš„nä¸ªçº¿æ€§æ— å…³çš„å‘é‡ã€‚\n$$ \\alpha = k_1a_1 + k_2a_2 + \u0026hellip; + k_na_n $$\nå…¶ä¸­ç³»æ•°ç§°ä¸ºå‘é‡$\\alpha$åœ¨è¿™ä¸ªåŸºä¸‹çš„åæ ‡ã€‚\nåŸºå˜æ¢ #   ä¸åŒåŸºä¹‹é—´å¯ä»¥è¿›è¡Œå˜æ¢ï¼Œå…¶ä¸­Pç§°ä¸ºè¿‡æ¸¡çŸ©é˜µã€‚\n$$ B = AP $$\n$$ [b_1,b_2,\u0026hellip;,b_n] = [a_1,a_2,\u0026hellip;,a_n]P $$\nåæ ‡å˜æ¢ #   åæ ‡å˜æ¢æ˜¯å·¦ä¹˜$P^{-1}$ã€‚æ¨å¯¼å¦‚ä¸‹\n$$ B\\beta^T = A\\alpha^T $$\n$$ AP\\beta^T = A\\alpha^T $$\n$$ \\beta^T = P^{-1}\\alpha^T $$\nçº¿æ€§å­ç©ºé—´ #   çº¿æ€§å­ç©ºé—´æ˜¯çº¿æ€§ç©ºé—´ä¸­ä»æ»¡è¶³çº¿æ€§ç©ºé—´è¦æ±‚çš„ä¸€ä¸ªå­ç©ºé—´ã€‚\nç‰¹æ®Šçš„ä¸¤ä¸ªå­ç©ºé—´ï¼šä¹Ÿç§°è¿™ä¸¤ä¸ªå­ç©ºé—´ä¸ºå¹³å‡¡å­ç©ºé—´\n çº¿æ€§ç©ºé—´æœ¬èº« é›¶ç©ºé—´  ç”Ÿæˆå­ç©ºé—´ï¼šå†™åš span{$a_1, a_2, \u0026hellip;, a_n$}\näº¤ç©ºé—´ï¼šä¸¤ä¸ªå­ç©ºé—´çš„äº¤é›†ï¼ŒåŒæ—¶å­˜åœ¨äºä¸¤ä¸ªå­ç©ºé—´ã€‚\n $$ V_{1} \\cap V_{2}=\\left\\{\\boldsymbol{\\alpha} \\mid \\boldsymbol{\\alpha} \\in V_{1} \\text { ä¸” } \\boldsymbol{\\alpha} \\in V_{2}\\right\\} $$  å’Œç©ºé—´ï¼šè¿™ä¸ªç©ºé—´çš„å‘é‡æ˜¯ä¸¤ä¸ªç©ºé—´å‘é‡çš„å’Œã€‚è¿™ä¸¤ä¸ªå­ç©ºé—´æœ‰ç‚¹åƒæ˜¯å’Œç©ºé—´çš„åŸºåº•ã€‚\n $$ V_{1}+V_{2}=\\left\\{\\boldsymbol{\\alpha}=\\boldsymbol{\\alpha}_{1}+\\boldsymbol{\\alpha}_{2} \\mid \\boldsymbol{\\alpha}_{1} \\in V_{1} \\text { ä¸” } \\boldsymbol{\\alpha}_{2} \\in V_{2}\\right\\} $$  ç»´æ•°å…¬å¼ï¼š\n$$ \\operatorname{dim} V_{1}+\\operatorname{dim} V_{2}=\\operatorname{dim}\\left(V_{1}+V_{2}\\right)+\\operatorname{dim}\\left(V_{1} \\cap V_{2}\\right) $$\nç›´å’Œç©ºé—´ï¼šè‹¥V1ï¼ŒV2ä¸¤ä¸ªå­ç©ºé—´æ²¡æœ‰äº¤é›†ï¼Œåˆ™å®ƒä»¬çš„å’Œç©ºé—´ç§°ä¸ºV1ï¼ŒV2çš„ç›´å’Œç©ºé—´ã€‚\nè¡¥å­ç©ºé—´ï¼šç›´å’Œç©ºé—´ä¸ºå…¨é›†çš„ä¸¤ä¸ªå­ç©ºé—´äº’ä¸ºè‡ªå·±ä»£æ•°è¡¥ã€‚\nçº¿æ€§æ˜ å°„ #    å®šä¹‰ï¼šæ»¡è¶³åŠ æ³•å’Œæ•°ä¹˜çš„ä¸¤ä¸ªçº¿æ€§ç©ºé—´çš„æ˜ å°„å…³ç³»ç§°ä¸ºçº¿æ€§æ˜ å°„ã€‚\n çº¿æ€§æ˜ å°„å‰çš„ç©ºé—´å«åŸåƒï¼Œåçš„å«â€œåƒâ€ã€‚\nè¯æ˜çº¿æ€§æ˜ å°„ï¼Œåªéœ€è¦è¯æ˜ä¸‹é¢ä¸¤ä¸ªå¼å­ï¼š\n $$ \\begin{array}{l} \\mathcal{A} \\left( \\alpha _{1}+ \\alpha _{2}\\right)= \\mathcal{A} \\left( \\alpha _{1}\\right)+ \\mathcal{A} \\left( \\alpha _{2}\\right) \\\\ \\mathcal{A} \\left(\\lambda \\alpha _{1}\\right)=\\lambda \\mathcal{A} \\left( \\alpha _{1}\\right) \\end{array} $$  æ€§è´¨ #   é›¶å…ƒç´ ç»è¿‡çº¿æ€§æ˜ å°„è¿˜æ˜¯é›¶å…ƒç´ ã€‚ ä¸€ç»„å…ƒç´ ç»è¿‡çº¿æ€§æ˜ å°„ï¼Œå¦‚æœåŸæ¥çº¿æ€§ç›¸å…³ï¼Œåˆ™ä¹‹åä¹Ÿçº¿æ€§ç›¸å…³ã€‚ä½†æ˜¯ï¼Œå¦‚æœåŸæ¥çº¿æ€§æ— å…³ï¼Œåˆ™ä¹‹åä¸ä¸€å®šçº¿æ€§æ— å…³ã€‚  çŸ©é˜µè¡¨ç¤º #  çº¿æ€§æ˜ å°„å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤ºï¼Œä½†è¿™ä¹‹å‰æˆ‘ä»¬éœ€è¦å°†ä¸¤ä¸ªçº¿æ€§ç©ºé—´çš„å…ƒç´ ç”¨å„è‡ªçš„åŸºåº•è¿›è¡Œè¡¨ç¤ºã€‚\né€‰å–ä¸åŒçš„åŸºåº•ï¼ŒåŒä¸€ä¸ªçº¿æ€§æ˜ å°„çš„çŸ©é˜µè¡¨ç¤ºå°±ä¸åŒã€‚\næ³¨æ„åŒºåˆ†ä¸¤ä¸ªæ¦‚å¿µï¼š\n åŸºåæ ‡å˜æ¢ï¼š$\\alpha=\\beta A$ å‘é‡åæ ‡å˜æ¢ï¼š$y=Ax$  ç»™å®šåŸºçš„æƒ…å†µä¸‹ç»™ä¸ªçŸ©é˜µéƒ½è¡¨ç¤ºä¸€ä¸ªçº¿æ€§æ˜ å°„ã€‚\nä¸åŒåŸºåº•ä¸‹çŸ©é˜µè¡¨ç¤ºä¹‹é—´çš„å…³ç³»ï¼š\nè®¾æœ‰ä¸¤ä¸ªç©ºé—´$X,Y$, ä¸¤ç»„åŸºåº•$x_1,y_1$å’Œ$x_2,y_2$ï¼Œ\n $Q$æ˜¯$y_1$åˆ°$y_2$çš„è¿‡æ¸¡çŸ©é˜µï¼Œ $P$æ˜¯$x_1$åˆ°$x_2$ä¹‹é—´çš„è¿‡æ¸¡çŸ©é˜µï¼Œ  åˆ™\u0008åœ¨$x_1,y_1$ä¸‹çš„$A$ä¸åœ¨$x_2,y_2$çš„$B$æœ‰å¦‚ä¸‹å…³ç³»ã€‚\n$$ B = Q^{-1}AP $$\nå€¼åŸŸ\u0026amp;\u0026amp;æ ¸ #  å€¼åŸŸ\næ ¸å­ç©ºé—´ï¼šçº¿æ€§æ˜ å°„çš„â€œé›¶ç©ºé—´â€œï¼Œæ˜ å°„ä¹‹åä¸º0çš„å…ƒç´ æ„æˆçš„ç©ºé—´ã€‚\nå¯¹äºä¸€ä¸ªä»nç»´çº¿æ€§ç©ºé—´v1ï¼Œæ˜ å°„åˆ°mç»´çº¿æ€§ç©ºé—´v2çš„çº¿æ€§æ˜ å°„ã€‚\n æ ¸å­ç©ºé—´æ ¸å€¼åŸŸçš„ç»´æ•°ä¹‹å’Œä¸ºn  çº¿æ€§å˜æ¢ #    åŒä¸€ä¸ªç©ºé—´çš„çº¿æ€§æ˜ å°„ç§°ä¹‹ä¸ºçº¿æ€§å˜æ¢ã€‚\n çŸ©é˜µç›¸ä¼¼, å­˜åœ¨çŸ©é˜µPï¼Œæ»¡è¶³ï¼š\n$$ B=P^{-1}AP $$\nåˆ™ç§°Aï¼ŒBç›¸ä¼¼ã€‚\nä¸¤ä¸ªçŸ©é˜µç›¸ä¼¼ï¼Œæ„å‘³ç€å®ƒä»¬æ˜¯ä¸åŒåŸºåº•ä¸‹åŒä¸€ä¸ªçº¿æ€§å˜æ¢çš„ä¸åŒçŸ©é˜µè¡¨ç¤ºã€‚  çº¿æ€§å˜æ¢æœ¬èº«ä¹Ÿå¯ä»¥å®šä¹‰è¿ç®—, è¿™äº›è¿ç®—å’Œå…¶çŸ©é˜µè¡¨ç¤ºæœ‰ç€å¯¹åº”å…³ç³»:\n ä¹˜æ³•: AB åŠ æ³•: A+B æ•°ä¹˜: kA  ç‰¹å¾å€¼\u0026amp;\u0026amp;ç‰¹å¾å‘é‡ #  çŸ©é˜µç‰¹å¾å€¼çš„æ¨å¹¿ï¼Œæ±‚è¿˜æ˜¯ç”¨çº¿æ€§å˜æ¢çš„çŸ©é˜µè¡¨ç¤ºæ¥æ±‚ã€‚\n ç‰¹å¾å­ç©ºé—´ï¼šä¸€ä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å’Œé›¶å‘é‡æ„æˆçš„ä¸€ä¸ªå­ç©ºé—´ï¼Œç§°ä¸ºè¿™ä¸ªç‰¹å¾å€¼çš„ç‰¹å¾å­ç©ºé—´ã€‚ ä»£æ•°é‡å¤åº¦ï¼šæ¯ä¸ªä¸åŒç‰¹å¾å€¼çš„é‡å¤æ•°ç›®ç§°ä¸ºè¿™ä¸ªç‰¹å¾å€¼çš„ä»£æ•°é‡å¤åº¦ã€‚ å‡ ä½•é‡å¤åº¦ï¼šä¸€ä¸ªç‰¹å¾å€¼ç‰¹å¾å­ç©ºé—´çš„ç»´æ•°ç§°ä¸ºè¿™ä¸ªç‰¹å¾å€¼çš„å‡ ä½•é‡å¤åº¦ã€‚  ç›¸ä¼¼å¯¹è§’åŒ– #   né˜¶çŸ©é˜µAå¯å¯¹è§’åŒ–çš„å……è¦æ¡ä»¶æ˜¯Aæœ‰nä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ã€‚  æ³¨æ„ï¼Œå¹¶ä¸æ˜¯ä»€ä¹ˆçº¿æ€§å˜æ¢å­˜åœ¨ä¸€ä¸ªåŸºï¼Œä½¿å¾—åœ¨è¿™ä¸ªåŸºä¸‹çš„çŸ©é˜µè¡¨ç¤ºï¼Œå‘ˆç°å¯¹è§’å½¢ã€‚ "},{"id":44,"href":"/blog/posts/algorithms/%E8%B4%AA%E5%BF%83%E7%94%9F%E6%88%90%E6%9C%80%E4%BC%98%E7%BC%96%E7%A0%81%E7%9A%84%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/","title":"è´ªå¿ƒç”Ÿæˆæœ€ä¼˜ç¼–ç çš„æ€è·¯åˆ†æ","section":"Posts","content":"è´ªå¿ƒç”Ÿæˆæœ€ä¼˜ç¼–ç çš„æ€è·¯åˆ†æ #  ç›®æ ‡ï¼šæ±‚å­—ç¬¦ç¼–ç \né¦–å…ˆå¾—å…ˆæƒ³åˆ°ç”¨äºŒå‰æ ‘è¡¨ç¤ºç¼–ç ï¼ŒèŠ‚ç‚¹å³ä¸ºå­—ç¬¦ï¼Œè¾¹ä¸ºç¼–ç ã€‚\nç„¶åä¼˜åŒ–ç›®æ ‡ï¼ˆç›®æ ‡å‡½æ•°ï¼‰å³ä¸ºï¼š f(x) = w(x)*l(x)\n w(x) ä¸º å­—ç¬¦xçš„é¢‘ç‡ l(x) ä¸º å­—ç¬¦ç¼–ç çš„é•¿åº¦  å†³ç­–ç›®æ ‡ï¼šå·§å¦™å®‰æ’äºŒå‰æ ‘çš„ç»“æ„ï¼Œä½¿å¾—ç›®æ ‡å‡½æ•°å€¼æœ€å°ã€‚\næœ€ä¼˜ç¼–ç æ ‘æ€§è´¨ä¸€ï¼šå¯¹äºæœ€ä¼˜ç¼–ç æ ‘ï¼Œé¢‘ç‡æœ€å°çš„ä¸¤ä¸ªèŠ‚ç‚¹æ·±åº¦æœ€å¤§ï¼Œä¸”ä¸ºå…„å¼Ÿã€‚\nè¯æ˜ï¼š\n  æ·±åº¦æœ€å¤§æ˜¾ç„¶ï¼Œå¦åˆ™æ·±åº¦æœ€å¤§çš„é¢‘ç‡æ›´å¤§ï¼Œä¼šä½¿å¾—ç›®æ ‡å‡½æ•°å€¼å¢å¤§ã€‚\n  è‹¥æœ€å°çš„èŠ‚ç‚¹æœ‰å…„å¼Ÿï¼Œä¸”å…¶å…„å¼Ÿä¸æ˜¯æ¬¡å°çš„ï¼Œåˆ™å¯ä»¥è®©æ¬¡å°çš„èŠ‚ç‚¹ä¸è¯¥èŠ‚ç‚¹äº¤æ¢ï¼Œä½¿å¾—ç›®æ ‡å‡½æ•°æ›´å°ã€‚\n  è‹¥æœ€å°çš„èŠ‚ç‚¹æ²¡æœ‰å…„å¼Ÿï¼Œåˆ™å¯ä»¥å°†è¯¥èŠ‚ç‚¹ä¸å…¶çˆ¶äº²äº¤æ¢ï¼Œå»æ‰â€œçˆ¶äº²â€ï¼Œå¯ä»¥ä½¿å¾—ç›®æ ‡å‡½æ•°æ›´å°ã€‚\n  æœ‰äº†è¿™ä¸ªæ€§è´¨ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥å…ˆå°†ä¸¤ä¸ªé¢‘ç‡æœ€å°çš„ç‚¹è¿èµ·æ¥ä½œå…„å¼Ÿã€‚ä½†ä¸ºäº†è¿›ä¸€æ­¥æ„é€ æœ€ä¼˜ç¼–ç æ ‘ï¼Œè¿˜éœ€è¦ç¬¬äºŒä¸ªåŸåˆ™ã€‚\næœ€ä¼˜ç¼–ç æ ‘æ€§è´¨äºŒï¼š å¯¹äºæœ€ä¼˜ç¼–ç æ ‘ï¼Œåˆ æ‰é¢‘ç‡æœ€å°çš„èŠ‚ç‚¹ï¼ˆä¸¤ä¸ªå¶å­èŠ‚ç‚¹ï¼‰ï¼Œåˆ™å…¶çˆ¶äº²å˜æˆäº†å¶å­ç»“ç‚¹ï¼Œå¦å…¶é¢‘ç‡ä¸ºä¸¤å­©å­é¢‘ç‡ä¹‹å’Œï¼Œåˆ™æ–°çš„ç¼–ç æ ‘ä»ç„¶æ˜¯æœ€ä¼˜ç¼–ç æ ‘ã€‚\nè¯æ˜ï¼šf1ä¸ºåˆ ä¹‹å‰ï¼Œf2ä¸ºåˆ ä¹‹åç›®æ ‡å‡½æ•°å€¼ï¼Œæœ€å°ä¸¤ä¸ªèŠ‚ç‚¹ä¸ºx1,x2, åˆ™æœ‰\nf1 = f2 + x1 + x2ã€‚\nå·²çŸ¥f1æœ€å°ï¼Œåˆ™f2ä¹Ÿæ˜¯æœ€å°çš„ï¼Œå¦åˆ™è°ƒæ•´æ–°çš„ç¼–ç æ ‘ä½¿å…¶å˜ä¸ºæœ€ä¼˜ç¼–ç æ ‘ï¼Œåˆ™f1æ›´å°ï¼Œä¸å·²çŸ¥f1æœ€å°çŸ›ç›¾ã€‚æ•…æ–°ç¼–ç æ ‘ä¹Ÿä¸ºæœ€ä¼˜ç¼–ç æ ‘ã€‚\næœ‰äº†è¿™ç¬¬äºŒä¸ªæ€§è´¨ï¼Œæˆ‘ä»¬å·²çŸ¥æ–°çš„ç¼–ç æ ‘ä¸ºæœ€ä¼˜ç¼–ç æ ‘ï¼Œåˆ™æ ¹æ®æ€§è´¨ä¸€ï¼Œ æ–°ç¼–ç æ ‘é¢‘ç‡æœ€å°çš„èŠ‚ç‚¹ä¸ºå…„å¼Ÿã€‚é€’å½’è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æ‰€æœ‰èŠ‚ç‚¹éƒ½çº³å…¥ç¼–ç æ ‘ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰ã€‚\nä¾‹: å°†ä¸¤ä¸ªé¢‘ç‡æœ€å°çš„ç‚¹è¿èµ·æ¥ä½œå…„å¼Ÿï¼Œç„¶åç”¨è¿™ä¸¤ä¸ªç‚¹çš„çˆ¶äº²ä»£æ›¿è¿™ä¸¤ä¸ªç‚¹ï¼Œæ±‚å‰©ä½™ç‚¹çš„æœ€ä¼˜ç¼–ç æ ‘ã€‚\n"}]