<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on Ze&#39;s Blog</title>
    <link>https://zeqiang-lai.github.io/blog/docs/notes/paper/</link>
    <description>Recent content in Paper on Ze&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language><atom:link href="https://zeqiang-lai.github.io/blog/docs/notes/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ArbSR: 任意尺度超分 - 阅读笔记</title>
      <link>https://zeqiang-lai.github.io/blog/docs/notes/paper/arbrcan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/docs/notes/paper/arbrcan/</guid>
      <description>Created: August, 12, 2021 论文: Learning A Single Network for Scale-Arbitrary Super-Resolution
Code: https://github.com/LongguangWang/ArbSR
 </description>
    </item>
    
    <item>
      <title>PatchMatch 导读</title>
      <link>https://zeqiang-lai.github.io/blog/docs/notes/paper/patchmatch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/docs/notes/paper/patchmatch/</guid>
      <description>Created: Jan, 24, 2022 PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing
 Sample Impl ｜ 知乎解读
图像匹配的典中典，基于匹配块周围的匹配也近似匹配的先验知识的随机化算法。
 首先明确这篇论文解决的问题是图像匹配问题，即给定两张图像A，B，我们需要将A，B中相同的像素或图像块（Patch）进行匹配，得到一个偏移量f，使得对于A中每个点的坐标x，B中对应点的坐标为x+f(x)。
这篇论文提出了一个Randomized Correspondence Algorithm来做这件事情。要理解这个算法，我们首先需要这个算法的核心insight：
The key insights driving the algorithm are thatsome good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas.  用中文来说就是，对于一个匹配好的点，我们知道其偏移量为f(x)，基于图像的连续性，那么这个点周围的点的偏移量应该也近似是f(x)，这样我们就可以快速把正确点的偏移量向其周围传播，使其周围点的估计偏移量越来越准确。
图像的连续性：一个苹果在两张图像上放在不同位置，但是苹果上某两个点的相对位置关系近似不变。  算法 #  前提条件 假设我们有两张图像A，B，我们需要找到一个偏移量f，使得对于A中每个点的坐标x，B中对应点的坐标为x+f(x)。
PatchMatch的算法主要分为三个步骤：
 Initialization： 随机初始化匹配关系，即A中每个点的偏移量。 Propagation： 根据匹配程度，将每个点的偏移量传播到周围的点。实际实现是，对于某个点(x,y)，使用其周围点的偏移量还是自己的偏移量得到匹配点，计算匹配程度，取匹配程度最好的偏移量作为当前点的偏移量。 Random search： 为了进一步优化匹配结果，我们在前一步得到的偏移量基础上，我们再随机的做一些变化，检查匹配点周围是否有更好的匹配点。   解读：</description>
    </item>
    
    <item>
      <title>Raft 阅读笔记</title>
      <link>https://zeqiang-lai.github.io/blog/docs/notes/paper/raft/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/docs/notes/paper/raft/</guid>
      <description>Created: August, 6, 2021 论文: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow
Code: https://github.com/princeton-vl/RAFT
 这篇文章做的是光流估计，简单来说，任务就是输入两张图像$I_1,I_2$，我们需要估计出第二张图像$I_1$每个像素点$(u,v)$相对于第一张图像$I_1$的偏移量$(f^1(u), f^2(v))$。
$$ I_1 = I_2 + f $$
这篇文章的思路是这样的：
 首先使用一个feature encoder  求出flow feature：同时输入img1，img2，得到fmap1，fmap2 求出context feature：只输入img1，得到inp   求出fmap1和fmap2的相似度corr 使用一个RNN迭代的求出flow。  RNN的输入包括一个隐藏状态net，context feature inp，相似度corr，上一步的flow     这里的主要问题是每迭代一次，我们都得到了一个更接近img1的img2，这时候相似度需要重新计算。
 RAFT设计了一个查表的思路，只需要在最开始算一遍即可。它的方法是用新坐标周围的点的flow feature拉成一个向量作为新坐标点的flow特征。</description>
    </item>
    
  </channel>
</rss>
