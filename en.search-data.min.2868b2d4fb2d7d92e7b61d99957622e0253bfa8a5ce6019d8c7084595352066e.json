[{"id":0,"href":"/blog/en/posts/drag_gan/","title":"DragGAN: Early Access and Local Deployment Tutorial","section":"Posts","content":"DragGAN, which has recently become popular across the internet, does not have its official code released yet. However, it is now available for early access experience\nProject Pages\n  Zeqiang-Lai/DragGAN: Code and Checkpoints, Local Deployment, Colab Demo.  OpenGVLab/InternGPT: Free Online Demo Integration.  Online Demo #   InternGPT  Google Colab   ‚ö†Ô∏è Note for Colab, remember to select a GPU via Runtime/Change runtime type\n Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!  Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!  Local Deployment - Pip Install Approach #  The following image demonstration uses Windows deployment as an example, but the deployment process is the same for Linux as well.\nCurrently, the implementation of Zeqiang-Lai/DragGAN has been uploaded to the PyPI repository. Therefore, there is no need to download the code; you can simply install it using pip install.\nInstalling Conda #  To avoid dependency conflicts, we first create a virtual environment using Conda. If you haven\u0026rsquo;t installed Conda yet, you can download Miniconda from here.\n ‰∏ãËΩΩÂÆåÊàêÂêéÔºåÁÇπÂáªÂÆâË£ÖÂåÖ‰∏ÄÁõ¥‰∏ã‰∏ÄÊ≠•Â∞±ÂèØ‰ª•‰∫Ü„ÄÇ\n Creating Conda Virtual Environment #  Next, select \u0026ldquo;Anaconda Powershell Prompt (miniconda3)\u0026rdquo; from the Windows menu bar to access the Conda command line.\n Once inside, enter the following command to create an environment named draggan with Python version 3.7. When prompted to proceed, enter y to continue.\nconda create -n draggan python=3.7  Since I already have an environment named \u0026ldquo;draggan,\u0026rdquo; the image shows \u0026ldquo;draggan2\u0026rdquo; instead.\n  Installing PyTorch #  Let\u0026rsquo;s activate the newly created environment by entering the following command:\nconda activate draggan  Next, refer to the official PyTorch installation guide.\n You can install PyTorch using either of the following commands (choose one), depending on your preferred download speed:\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia For users without a GPU, use the following command for installation:\npip3 install torch torchvision torchaudio  When you see \u0026ldquo;Successfully installed,\u0026rdquo; it means the installation was successful. You can disregard any other warnings.\nInstalling DragGAN #  After the installation is complete, we can install DragGAN using the following command:\npip install draggan Due to some unknown reasons, the \u0026ldquo;draggan\u0026rdquo; package is not synchronized with the Tsinghua pip source. If your pip is configured to use the Tsinghua or other domestic pip sources, you may encounter a \u0026ldquo;package not found\u0026rdquo; issue.\n In such a case, you can use the following command to temporarily install from the official source:\npip install draggan -i https://pypi.org/simple/  Similar to PyTorch installation, when you see \u0026ldquo;Successfully installed,\u0026rdquo; it means the installation was successful. You can disregard any other warnings.\n At this point, all dependencies have been installed, and you can proceed to run DragGAN.\nRunning DragGAN Demo #  You can run the DragGAN demo using the following command:\npython -m draggan.web If you accidentally close the command line, there\u0026rsquo;s no need to reinstall. Simply reopen the \u0026ldquo;Anaconda Powershell Prompt (miniconda3)\u0026rdquo; and enter the Conda command line, activate the environment, and run the command.\nconda activate draggan python -m draggan.web For users without a GPU, use the following command:\npython -m draggan.web --device cpu When you see the following URL: http://127.0.0.1:7860, it means the program has run successfully.\n Enter this URL into your browser to access the DragGAN demo.\n "},{"id":1,"href":"/blog/en/posts/sd_cg/","title":"Steepest Descent \u0026\u0026 Conjugate Gradient","section":"Posts","content":"This is a reading note on:\n An Introduction to the Conjugate Gradient Method Without the Agonizing Pain\nIt is definitely the best paper to understand the steepest descent conjugate gradient method, which save me a lot of time, thanks god. And it is written in 1994, pretty cool, right? üßê\nIntroduction #  Well, let\u0026rsquo;s begin with a short summary of this paper:\n"},{"id":2,"href":"/blog/en/posts/least_square/","title":"Derivation of Least Square","section":"Posts","content":"Least square problem solve the following equation:\n$$ argmin_x |Ax-y|_2^2 $$\nIt can be solve in closed-form by the following equation:\n$$ \\hat{x} = (A^TA)^{-1}A^Ty $$\nDerivation #  The objective can be unfolded via:\n$$ |Ax-y|_2^2 = (Ax-y)^T(Ax-y) = x^TA^TAx - x^TA^Ty -y^TAx +y^Ty $$\nTake the derivative of the objective with respect to x, we have:\n$$ \\frac{\\partial |Ax-y|_2^2}{\\partial x} = 2A^TAx - 2A^Ty $$\nThe minimum can be founded through setting the derivative to zero, then we have:\n$$ A^TAx - A^Ty = 0 \\Rightarrow A^TAx = A^Ty $$\n$$ x = (A^TA)^{-1}A^Ty $$\n\\The derviation of the above derivatives relies on the following equations:\n$$ \\frac{\\partial x^Tb}{\\partial x} = \\frac{\\partial b^Tx}{\\partial x} = b $$ This is obvious by looking at the scalar form of $x^Tb$.\n$$ \\frac{\\partial x^TAx}{\\partial x} = (A+A^T)x $$\nThe detailed derivation of this equality can be founded in stackexchange.\n "},{"id":3,"href":"/blog/en/posts/admm-2d-tv/","title":"2D TV Denosing with ADMM -- Mathematics \u0026\u0026 Implementation","section":"Posts","content":"For 2D Total variation denosing, we have the following objective, where $x$ is the clean image we want to optimize, $y$ is the origin noisy image, and $D_r $and $D_c$ are doubly block circulant matrices for two 2D convolution.\nIn detail, both of $x$ and $y$ are vectorized into 1D vectors, and the total variation terms are expressed as two convolutions in Matrix-vector form.\n $$ \\operatorname{minimize} \\enspace \\frac{1}{2} \\|x-y\\|_{2}^{2} + \\lambda\\|D_r x\\|_{1} + \\lambda\\|D_c x\\|_{1} $$  With ADMM, we substitute$D_* x$ with$ z_*$, and add two constraints.\n $$ \\begin{array}{ll} \\operatorname{minimize} \u0026 \\frac{1}{2} \\|x-y\\|_{2}^{2} + \\lambda\\|z_r\\|_{1} + \\lambda\\|z_c\\|_{1} \\\\ \\text { subject to } \u0026 D_r x-z_r=0 \\\\ \\text { subject to } \u0026 D_c x-z_c=0 \\end{array} $$  Then, we use augmented lagrangian method to remove constraintsÔºö\n $$ \\begin{aligned} L_{\\rho}( x , z_r , \\nu_r, z_c, \\nu_c ) = \\frac{1}{2}\\|x-y\\|_{2}^{2} \u0026 + \\lambda\\|z_r\\|_{1}+ \\nu_r ^{ T }(D_r x-z_r)+\\frac{\\rho}{2}\\|D_r x -z_r\\|_{2}^{2} \\\\ \u0026 + \\lambda\\|z_c\\|_{1}+ \\nu_c ^{ T }(D_c x-z_c)+\\frac{\\rho}{2}\\|D_c x -z_c\\|_{2}^{2} \\end{aligned} $$  Let $\\mu_r = \\nu_r / \\rho$Ôºå$\\mu_c = \\nu_c / \\rho$Ôºåtransform the above equation into the following one:\n $$ \\begin{aligned} L_{\\rho}( x , z_r , \\nu_r, z_c, \\nu_c )= \\frac{1}{2}\\|x-y\\|_{2}^{2} \u0026 + \\lambda\\|z_r\\|_{1}+ \\frac{\\rho}{2}\\|D_r x-z_r+ \\mu_r \\|_{2}^{2}-\\frac{\\rho}{2}\\| \\mu_r \\|_{2}^{2} \\\\ \u0026 + \\lambda\\|z_c\\|_{1}+ \\frac{\\rho}{2}\\|D_c x-z_c+ \\mu_c \\|_{2}^{2}-\\frac{\\rho}{2}\\| \\mu_c \\|_{2}^{2} \\end{aligned} $$  X subproblem #   For x subproblem, we need to optimize the following equation:\n $$ \\begin{aligned} x ^{(k+1)} =\\arg \\min _{ x } \\enspace \\|x^{(k)}-y\\|_{2}^{2} \u0026 + \\left\\|\\sqrt{\\rho}D_r x^{(k)}- \\sqrt{\\rho}(z_r ^{(k)}- \\mu_r ^{(k)})\\right\\|_{2}^{2} \\\\ \u0026 + \\left\\|\\sqrt{\\rho}D_c x^{(k)}- \\sqrt{\\rho}(z_c ^{(k)}- \\mu_c ^{(k)})\\right\\|_{2}^{2} \\end{aligned} $$  This is a least square problem\n $$ \\min _{x}\\left\\|\\left[\\begin{array}{c} I \\\\ \\sqrt{\\rho} D_r \\\\ \\sqrt{\\rho} D_c \\\\ \\end{array}\\right] x^{(k)} -\\left[\\begin{array}{c} y \\\\ \\sqrt{\\rho}\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right) \\\\ \\sqrt{\\rho}\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right) \\\\ \\end{array}\\right]\\right\\|_{2}^{2} $$  And we can solve it with the solution of least square $(X^TX)^{-1}X^TY$Ôºö\n $$ \\begin{aligned} x ^{(k+1)} \u0026=\\left( I +\\rho (D_r^TD_r + D_c^TD_c) \\right)^{-1}\\left[ I, \\sqrt{\\rho} D_r^T, \\sqrt{\\rho} D_c^T \\right]\\left[\\begin{array}{c} y \\\\ \\left.\\sqrt{\\rho}\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right)\\right] \\\\ \\left.\\sqrt{\\rho}\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right)\\right] \\\\ \\end{array}\\right] \\\\ \u0026=\\left( I +\\rho (D_r^TD_r + D_c^TF_c) \\right)^{-1} \\left( y + \\rho \\left[ D_r^T\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right) + D_c^T\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right) \\right] \\right) \\end{aligned} $$  DFT Speedup #  The inverse of matrix $I +\\rho (D_r^TD_r + D_c^TD_c)$ is computationally expensive. To see why, suppose there is a 200 * 300 image, then the shape of $D_r$ and $D_c$are both (200*300, 200*300), and it means we have to take inverse of a giant  (200*300, 200*300) matrix, which is impractical.\nThanks to convolution theorem, we could solve this equation purely in frequency domain.\nFor detail, we start with moving $(I +\\rho (D_r^TD_r + D_c^TD_c))^{-1}$ to the left hand side:\n $$ \\left( I +\\rho (D_r^TD_r + D_c^TF_c) \\right) x ^{(k+1)} = \\left( y + \\rho \\left[ D_r^T\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right) + D_c^T\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right) \\right] \\right) $$  Suppose $\\mathcal{F}$ is the Fourier matrix and $\\mathcal{F}^{-1}$ is the inverse Fourier matrix. Perform fourier transform on the both sides of above equation gives us the following result (using the linearity of Fourier transform):\n $$ \\left( \\mathcal{F}I +\\rho (\\mathcal{F}D_r^TD_r + \\mathcal{F}D_c^TF_c) \\right) x ^{(k+1)} = \\left( \\mathcal{F}y + \\rho \\left[ \\mathcal{F}D_r^T\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right) + \\mathcal{F}D_c^T\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right) \\right] \\right) $$  If we use the property of $\\mathcal{F}^{-1}F = \\mathcal{F}^H\\mathcal{F} = I$, we could get:\n $$ \\begin{aligned} \\left( \\mathcal{F}I +\\rho (\\mathcal{F}D_r^TD_r + \\mathcal{F}D_c^TF_c) \\right) \\mathcal{F}^H\\mathcal{F} x ^{(k+1)} = LHS \\\\ \\text{=} \\left( \\mathcal{F}I\\mathcal{F}^H +\\rho (\\mathcal{F}D_r^TD_r\\mathcal{F}^H + \\mathcal{F}D_c^TF_c\\mathcal{F}^H) \\right) \\mathcal{F} x ^{(k+1)} = LHS \\end{aligned} $$  From the facts below:\n $\\mathcal{F}I\\mathcal{F}^H = I$ Fourier matrix can diagonize any circulant matrix in the way of $\\mathcal{F}D\\mathcal{F}^H$, See [Link1], [Link2].  We could get:\n $$ \\left( I +\\rho (\\Lambda_r + \\Lambda_c) \\right) \\mathcal{F} x ^{(k+1)} = \\left( \\mathcal{F}y + \\rho \\left[ \\mathcal{F}D_r^T\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right) + \\mathcal{F}D_c^T\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right) \\right] \\right) $$  In frequency domain, convolution is element-wise multiplication, so the final result is:\n $$ x ^{(k+1)} = \\mathcal{F}^{-1} \\frac{\\left( \\mathcal{F}y + \\rho \\left[ \\mathcal{F}D_r^T\\left( z_r ^{(k)}- \\mu_r ^{(k)}\\right) + \\mathcal{F}D_c^T\\left( z_c ^{(k)}- \\mu_c ^{(k)}\\right) \\right] \\right)} {\\left( I +\\rho (\\Lambda_r + \\Lambda_c) \\right)} $$  Implementation #  Recall that any doubly block circulant matrices $D$ can be diagonized by Fourier matrix. The column of $F^H$ are the eigen vectors and the corresponding eigen values are the DFT values of the signal generating the circulant matrix.\nAnd with the following equations, we know the eigen value of $D^TD$ are just the multiplication of the eigen matrix of $D$ and its conjugate.\n $$ H = \\mathcal{F}^{H} D \\mathcal{F} \\\\ H^H = \\mathcal{F}^{H} D^T \\mathcal{F} \\\\ H^HH = \\mathcal{F}^{H} D^T \\mathcal{F}\\mathcal{F}^{H} D \\mathcal{F} = \\mathcal{F}^{H} D^T D \\mathcal{F} $$  Summarize all the fact above, we could caluclate $\\Lambda_r + \\Lambda_c$ with the following Python snippet.\neigDtD = np.abs(np.fft.fft2(np.array([[1, -1]]), (row, col))) ** 2 + \\ np.abs(np.fft.fft2(np.array([[1, -1]]).transpose(), (row, col))) ** 2 Note that we use the fact that $A^HA = ||A||_2^2$\nSince $\\Lambda_r + \\Lambda_c$ is diagonal, addition and multiplication of $I +\\rho (\\Lambda_r + \\Lambda_c)$ could be done in element-wise way.\nlhs = 1 + rho * eigDtD "}]