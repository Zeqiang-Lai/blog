<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction on Ze&#39;s Blog</title>
    <link>https://zeqiang-lai.github.io/blog/en/</link>
    <description>Recent content in Introduction on Ze&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://zeqiang-lai.github.io/blog/en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Steepest Descent &amp;&amp; Conjugate Gradient</title>
      <link>https://zeqiang-lai.github.io/blog/en/posts/sd_cg/</link>
      <pubDate>Mon, 25 Apr 2022 14:32:05 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/en/posts/sd_cg/</guid>
      <description>&lt;p&gt;This is a reading note on:&lt;/p&gt;
&lt;p&gt;
  &lt;a href=&#34;https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf&#34;&gt;An Introduction to the Conjugate Gradient Method Without the Agonizing Pain&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is definitely the best paper to understand the steepest descent conjugate gradient method, which save me a lot of time, thanks god.
And it is written in 1994, pretty cool, right? üßê&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Derivation of Least Square</title>
      <link>https://zeqiang-lai.github.io/blog/en/posts/least_square/</link>
      <pubDate>Thu, 21 Apr 2022 14:32:05 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/en/posts/least_square/</guid>
      <description>&lt;p&gt;Least square problem solve the following equation:&lt;/p&gt;
&lt;p&gt;$$
argmin_x |Ax-y|_2^2
$$&lt;/p&gt;
&lt;p&gt;It can be solve in closed-form by the following equation:&lt;/p&gt;
&lt;p&gt;$$
\hat{x} = (A^TA)^{-1}A^Ty
$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>2D TV Denosing with ADMM -- Mathematics &amp;&amp; Implementation</title>
      <link>https://zeqiang-lai.github.io/blog/en/posts/admm-2d-tv/</link>
      <pubDate>Fri, 25 Dec 2020 14:32:04 +0000</pubDate>
      
      <guid>https://zeqiang-lai.github.io/blog/en/posts/admm-2d-tv/</guid>
      <description>&lt;p&gt;For 2D Total variation denosing, we have the following objective, where $x$ is the clean image we want to optimize, $y$ is the origin noisy image, and $D_r $and $D_c$ are doubly block circulant matrices for two 2D convolution.&lt;/p&gt;
&lt;p&gt;In detail, both of $x$ and $y$ are vectorized into 1D vectors, and the total variation terms are expressed as two convolutions in Matrix-vector form.&lt;/p&gt;
&lt;div&gt;
$$
\operatorname{minimize} \enspace \frac{1}{2} \|x-y\|_{2}^{2} + \lambda\|D_r x\|_{1} + \lambda\|D_c x\|_{1}
$$
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
