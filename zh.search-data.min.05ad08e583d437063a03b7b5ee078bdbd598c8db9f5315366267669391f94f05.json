[{"id":0,"href":"/blog/docs/notes/concept/","title":"Concept","section":"Notes","content":"Concepts #  Deep Generative Models #   VAE #   再看变分自动编码器VAE: [ Note], [ Slide]  Flow-based #   Invertible Neural Network Normalizing Flow (Flow Neural Network) [Note] Denoising Normalizing Flow  Illustration   EM #    推导高斯混合模型的EM算法：[ Note] 从最小化自由能的角度推导高斯混合模型的EM算法：[ Note]  ADMM #    ADMM Tutorial: [ Note] ADMM各个任务推导: [ Note]  Others #    Denoising Autoencoder  朴素贝叶斯 #  朴素贝叶斯 用于求解分类问题，给定数据，每个数据点包含多列特征和一个类别。 任务是估计给定特征，各个类别出现的概率，即求： $$ p\\left(C \\mid F_{1}, \\ldots, F_{n}\\right) $$\n我们可以用频率估计概率的方法直接从数据中估计这个概率，但是当特征F的种类很多，取值也很多的情况下，我们需要很大量的数据才能做出准确估计。这在现实生活中往往是做不到的。\n因此，朴素贝叶斯首先用贝叶斯公式改写求解目标： $$ p\\left(C \\mid F_{1}, \\ldots, F_{n}\\right)=\\frac{p(C) p\\left(F_{1}, \\ldots, F_{n} \\mid C\\right)}{p\\left(F_{1}, \\ldots, F_{n}\\right)} $$\n分母是每个特征出现的联合概率，与类别无关，可以看作某个未知的定值。对于分子，$p(C)$是比较容易通过频率估计的，而$p\\left(F_{1}, \\ldots, F_{n} \\mid C\\right)$则不好估计，因为当特征F很多的时候，数据一般比较稀疏，给定$C$，$F_{1}, \\ldots, F_{n}$出现的概率可能为0，因为数据集中不存在这种情况（但不代表真的出现概率为0）。\n因此，朴素贝叶斯做了个朴素的假设，即各个特征相互独立。基于此，则有：\n $$ \\begin{equation} \\begin{aligned} p\\left(F_{1}, \\ldots, F_{n} \\mid C\\right) \u0026 \\propto p\\left(F_{1} \\mid C\\right) p\\left(F_{2} \\mid C\\right) p\\left(F_{3} \\mid C\\right) \\ldots \\\\ \u0026 \\propto \\prod_{i=1}^{n} p\\left(F_{i} \\mid C\\right) \\end{aligned} \\end{equation} $$  显然，基于频率统计概率$p\\left(F_{i} \\mid C\\right)$会更加容易得多。\n  "},{"id":1,"href":"/blog/docs/notes/paper/","title":"Paper","section":"Notes","content":"Paper Notes #  一些论文阅读笔记\n  [PDF] Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems  [PDF] Effective Snapshot Compressive-spectral Imaging via Deep Denoising and Total Varia-tion Priors  [Draft] RAFT: Recurrent All-Pairs Field Transforms for Optical Flow  [Draft] PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing  Simple Review #   Vision Transformer for Small-Size Datasets #   提出了一个针对小数据集的Transformer。 主要模块：Shifted Patch Tokenization, Locality Self-Attention。细节没看。  Functional Neural Networks for Parametric Image Restoration Problems #   对于超分，去噪，JPEG Decompression，有不同的parameter（scale factor，noise level等），现有方法针对不同parameter单独训练模型，或不考虑不同parameter的差异，一起训练一个统一的模型。 作者提出学习一个函数，输入paramter，输出针对这个parameter的模型参数。主要用点是不用单独训练每个parameter的模型，而是训练一个函数，输入parameter，输出模型参数。  Variational Image Restoration Network #   用了Generative Model和Variational inference做Image Restoration。太长公式太多，没细看。  Designing a Practical Degradation Model for Deep Blind Image Super-Resolution #   Github\n 提出了一个超分的Degradation Model用于真实图像超分。 其实就是总结了常见的Blur，Downsample，Noise的种类，然后degradation就是这些的排列组合。  Revisiting RCAN: Improved Training for Image Super-Resolution #   提出了一些训练策略，让RCAN的性能进一步提升。 策略简单说：多GPU大Batch Size，SiLu替换ReLu，训练更长时间，Lamb Optimizer，48*48 patch训练，64*64 finetune，FP16加速训练，不要regularization，先训练x2，然后用x2参数训练x3，以此类推。  Enhancing Low-Light Images in Real World via Cross-Image Disentanglement #  arxiv 2022 | CIDN\n 不需要配对数据训练的暗光增强网络 核心思想，将一张图分成 content feature 和 brightness feature。使用低光图像的content feature，和正常光图像的brightness feature，然后使用normal-light decoder重构出来。正常光参考图像不必与低光图像对齐。  结构图   训练数据是参考图像粗对齐的正常光图，测试数据参考图像可以完全不一样。 几个Loss：（1）低光和正常光的content feature应该类似。(2) 低光的content feature和brightness feature应该能够重构出低光图像。（3）正常光图像没有gt。（4）令光照特征符合正态分布，与结构无关。（5）VGG perceptual loss。    Disentangling Noise from Images: A Flow-Based Image Denoising Neural Network #  arxiv 2021 | Github\n Invertible Denoising Network: A Light Solution for Real Noise Removal的期刊扩展。  Denoising Normalizing Flow #  NeurIPS 2021\n 本文并不是用来去噪的Normalizing Flow。类似Denoising Autoencoder。  DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows #  CVPR 2021 | Github | Blog:Normalizing Flow\n 提出了一个不需要配对数据训练的conditional flow网络用于合成训练数据。 具体做法简介：需要先了解Normalizing Flow。TODO  Invertible Denoising Network: A Light Solution for Real Noise Removal #   Github | Blog:Invertible Network\n 提出使用Invertible Network做真实噪声去噪。 网络将noisy image编码成low resolution clean image和一个high frequency和noise信息的latent vector。 逆过程把latent vector换成一个从标准正态分布取样的vector，然后逆回去，得到clean image。  问题（比较疑惑的地方）：为什么是随机取样，取出来的latent vector起的作用是什么，不同的vector，恢复出来的clean image不是是不一样的？\nAdaDM: Enabling Normalization for Image Super-Resolution #   传统超分网络一般不加Normalization，因为这会使得特征方差变小，对性能有很大影响。 这篇文章文章提出了 Adaptive Deviation Modulator (AdaDM)，可以放大方差，使得Normalization又能加了。并且有性能提升。  IDR: Self-Supervised Image Denoising via Iterative Data Refinement #   Github\n 提出了一个unsupervised denoising method，不用配对数据训练。 基于在带噪图像上加noise model的合成噪声训练的网络，能够一定程度地对带噪图像降噪。noiser-noisy数据集和noisy-clean的domain gap越小，网络降噪性能越好。 本文提出用noiser-noisy数据集训练一个网络，然后用网络去噪得到更干净的noisy图像，再合成noiser-nosiy数据集。迭代进行训练，不断缩小noiser-noisy数据集和noisy-clean的domain gap。  Rethinking Noise Synthesis and Modeling in Raw Denoising #   Github\n 物理噪声模型对于真实场景去噪比DNN的好使，但是需要对不同相机，光照条件单独建模或者标定，毕竟麻烦。 本文提出从相机噪声中随机取样作为合成噪声，因此不需要噪声模型，只需要拍摄多张带噪图像作为database即可。 作者将噪声分为信号相关和信号无关的，信号无关采用随机取样，信号相关部分只考虑photo shot noise，从泊松分布采样使用，仍然需要标定。  On Efficient Transformer and Image Pre-training for Low-level Vision #   Github\n 提出了一个Encoder-Decoder结构的Transformer。比IPT小10倍，只用200k（15.6%的ImageNet）数据预训练。GFLOPS只有SwinIR的8.4%, 38 vs 451。 研究了预训练策略，发现多任务预训练比单任务和单纯增加数据量好。simply increasing the data scale may not be the op- timal option, and in contrast, multi-related-task pre-training is more effective and data-efficient.  TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions #   Github\n Transformer结构，不同任务相同encoder，decoder，但是decoder，把任务类型作为query传进decoder。  Quick Review   All in One Bad Weather Removal using Architectural Search #   用多个Encoder，一个Decoder做Image Restoration，具体是Bad Weather Removal。 Encoder后面接了一个NAS搜的网络，其他没有特别的设计。Feature Space也没什么约束。在去噪等传统领域效果感觉不一定好，很可能不好。  Quick Review   PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing #   Sample Impl ｜ 知乎解读 | 本站 🌟\n 图像匹配的典中典，基于匹配块周围的匹配也近似匹配的先验知识的随机化算法。 原论文挺好读的，但是建议先看解读有一个insight之后再读。  Quick Review  随机初始化匹配关系。 基于正确的匹配关系进行修正，即正确匹配的Patch Pair周围的patch应该是匹配的。 加入扰动，在修正后匹配的匹配对周围搜索是否有更加匹配的patch。     A ConvNet for the 2020s #   Offical Impl\n CNN的大型Ablation Study，研究各种已有模块和训练技巧的有效性，基于此构建了一个超越现有Transformer的纯CNN模型。  Masked Autoencoders Are Scalable Vision Learners #   Offical Impl\n 基于Transformer的（高层）视觉预训练模型，只用重构任务。  "},{"id":2,"href":"/blog/docs/matrix/","title":"Matrix","section":"Docs","content":"Matrix #  正在努力编写中✍️  矩阵分析速查表: [ PDF]\n  如何将各个知识点串起来？  杂七杂八: 一些无法分类的小知识  线性空间  各种矩阵\n  对称矩阵  正定矩阵  正交矩阵  相似矩阵 : ⚠️ 相似矩阵是指一系列互相相似的矩阵。  特征值/特征向量\n  基础知识  矩阵对角化 : 利用特征值，特征向量进行。  Jordan Form  矩阵分解\n  特征值分解(矩阵对角化) : 对角化也算是一种分解。  奇异值分解 : 特征值分解(eigen decomposition)的扩展  谱分解 : 特征值分解的另一种表达形式。  "},{"id":3,"href":"/blog/posts/ai/sd_cg/","title":"Steepest Descent \u0026\u0026 Conjugate Gradient","section":"Posts","content":"这是一篇主要介绍Conjugate Gradient (CG)的笔记，当然为了引入CG，也会一并介绍其“前身” Steepest Descent。\n本文主要参考这篇论文： An Introduction to the Conjugate Gradient Method Without the Agonizing Pain\n It is definitely the best paper to understand the steepest descent conjugate gradient method, which save me a lot of time, thanks god. And it is written in 1994, pretty cool, right? 🧐\n 类似的知乎： https://zhuanlan.zhihu.com/p/64227658\n Introduction #  Well, let\u0026rsquo;s begin with a short outline of this paper:\n首先，我们需要了解CG要求解的问题，CG是一个迭代求解大规模线性方程组的方法，即我们要求:\n$$ Ax = b $$\n其中x是我们要求的一个向量，b是一个已知向量，A是一个已知的 square, symmetric, positive-definite 的矩阵。\nThe Quadratic Form #  二次型是一个标量二次函数，即输入是一个向量x，输出一个标量，它的形式如下：\n$$ f(x) = \\frac{1}{2}x^T A x - b^T x + c $$\n对$f(x)$求导，我们有：\n$$ f'(x) = \\frac{1}{2}A^Tx + \\frac{1}{2}Ax -b $$\n当A是对称矩阵时，我们有:\n$$ f'(x) = Ax - b $$\n不难得出，当A是对称矩阵时，原问题$Ax=b$的解就是二次型导数为0的点，也即极值点。因此，我们可以通过求解二次型的极值点找到$Ax=b$的解。\n进一步的，如果A是正定（postive definite）矩阵，二次型的极值点是它的最小值点，因此我们可以通过最小化 $f(x)$ 找到 $Ax=b$ 的解。\n当A矩阵是正定矩阵时，二次型有最小值，如a所示，如果是负定矩阵，则如b所示。 原论文 Figure 5。 Steepest Descent #  回顾，我们现在需要做的是找出二次型的最小值。\n首先，基于迭代的求最小值的方法思路都是从一个随机解 $x_0$开始，不断更新，得到 $x_1, x_2, \u0026hellip;$，直到与真实值足够接近。（用$r=Ax-b$判断）\nSteepest Descent 也是基于迭代的方法，它是梯度下降的一个改进，即从初始点出发，每次都朝负梯度方向更新。\n$$ -f'(x_i) = b - Ax_i $$\n我们定义 residual 和 error $$ r_i = b - Ax_i $$ $$ e_i = x_i -x $$\n则每次更新公式为：\n$$ x_{i+1} = x_i + \\alpha r_i $$\n现在一个问题是，我们知道更新的方向，但我们要走多远呢？即 $\\alpha$ 要如何选取。\nSteepest Descent的思路是，每次更新都取从负梯度方向出发能够的到达的最低点。如下图a,b所示。\n 从数学上，我们的目标就是找到一个 $\\alpha$ 使得 $f(x_{i+1})$ 最小。回忆，因为A是正定的，所以 $f$ 的极值点就是最值点，因此我们可以通过导数 $\\frac{\\partial f(x_{i+1})}{ \\partial \\alpha}$ 为0，求得 $\\alpha$。\n应用复合函数求导： $$ \\frac{\\partial f(x_{i+1})}{ \\partial \\alpha} = \\frac{\\partial f(x_{i+1})}{ \\partial x_{i+1}} \\frac{\\partial x_{i+1}}{ \\partial \\alpha} $$\n其中 $\\frac{\\partial f(x_{i+1})}{ \\partial x_{i+1}} = f'(x_{i+1}) = - r_{i+1}$， $ \\frac{\\partial x_{i+1}}{ \\partial \\alpha} = r_i$\n令导数为0，我们有（为了简化表达式，这里用1表示i+1，0表示i）：\n $$ \\begin{aligned} r_{(1)}^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A x_{(1)}\\right)^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A\\left(x_{(0)}+\\alpha r_{(0)}\\right)\\right)^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A x_{(0)}\\right)^{T} r_{(0)}-\\alpha\\left(A r_{(0)}\\right)^{T} r_{(0)} \u0026=0 \\\\ \\left(b-A x_{(0)}\\right)^{T} r_{(0)} \u0026=\\alpha\\left(A r_{(0)}\\right)^{T} r_{(0)} \\\\ r_{(0)}^{T} r_{(0)} \u0026=\\alpha r_{(0)}^{T}\\left(A r_{(0)}\\right) \\\\ \\alpha \u0026=\\frac{r_{(0)}^{T} r_{(0)}}{r_{(0)}^{T} A r_{(0)}} . \\end{aligned} $$  事实上 Steepest Descent 每次更新的方向都与上一步的方向正交的。\n这是因为每一步更新的时候都是从负梯度方向走到最低点，如果停下来的位置梯度不与更新梯度正交，那么这个位置就不是最低点，因为这个位置在更新梯度上还有一个不为0的分量。\n  "},{"id":4,"href":"/blog/posts/algorithms/%E6%9C%80%E7%9F%AD%E8%B7%AF/","title":"最短路算法","section":"Posts","content":"Dijkstra用于求没有负权边的单源最短路。\nDijkstra #  从源点出发，每次选择与访问过的点距离最近的点，更新距离。这个距离就是从源点到该点的最短距离。\nWhy？\nBellman-Ford #  SPFA # "},{"id":5,"href":"/blog/posts/algorithms/gcd/","title":"快速求最大公约数gcd","section":"Posts","content":"快速求两个数的最大公约数(公因数)有两个办法：\n 更相减损法 辗转相除法  更相减损法 #  原理：两个正整数a和b（a\u0026gt;b），它们的最大公约数等于a-b（大减小）和b（小）的最大公约数。\nint gcd(int a,int b) { if(a==b) return a; if(a\u0026gt;b) return gcd(a-b,b); if(a\u0026lt;b) return gcd(b-a,a); } 辗转相除法 #  原理：两个正整数a和b（a\u0026gt;b），它们的最大公约数等于a%b（大模小）和b（小）之间的最大公约数。\nwhy？\n 假设最大公约数是x, 显然，x也是a%b因数。 现在证明x是a%b和b的最大公因数：假设存在更大的公因数y，因为a=kb+a%b, 则y肯定也是a的公因数。因此x=y，得证。  int gcd(int a,int b) { if(b==0) return a; else return gcd(b,a%b); } // 一行写法 int gcd(int a,int b) { return b ? gcd(b,a%b):a; } 比较 #  更相减损法避免了大整数取模导致效率低下的问题，但是运算次数要比辗转相除多得多。\nReference #    https://www.cnblogs.com/fusiwei/p/11301436.html "},{"id":6,"href":"/blog/posts/ai/monte_carlo/","title":"蒙特卡洛法","section":"Posts","content":"蒙特卡洛(MonteCarlo)是一大类随机算法(RandomizedAlgorithms)的总称，它们通过随机样本来估算真实值。\n估计圆周率 #  假设有一个半径为1，圆心在原点的圆，我们知道它的面积是$\\pi$。现在我们随机在$x\\in[-1,1], y\\in[-1,1]$上随机取一个点，我们知道这个点落在圆内的概率为圆与正方形面积之比：\n$$ p = \\frac{\\pi}{4} $$\n假设我们随机选取了n个点，我们可以使用下面的公式判断哪些点位于圆内，假设有m个。\n$$ x^2 + y^2 \u0026lt; 1 $$\n显然，我们知道m的数学期望是\n$$ E[M] = \\frac{\\pi n}{4} $$\n当n很大时, m近似于数学期望，因此有\n $$ \\begin{aligned} m \\approx \\frac{\\pi n}{4} \\\\ \\pi \\approx \\frac{4m}{n} \\end{aligned} $$  近似求定积分 #  蒙特卡洛求积分的本质是利用随机模拟估计一个随机变量的期望。\n假设我们想求定积分:\n$$ \\int_{a}^{b} f(x) \\mathrm{d} x $$\n我们可以把它转换成某个随机变量的数学期望，具体如下：\n设随机变量$X \\sim U[a, b]$，即服从均匀分布，X具有概率密度$p(x)=\\frac{1}{b-a}$，那么就有:\n$$ E[f(X)] =\\int_{a}^{b} p(x)f(x) \\mathrm{d} x = \\frac{1}{b-a}\\int_{a}^{b} f(x) \\mathrm{d} x $$\n在统计学中，我们知道期望是可以估计的，我们随机取N个$f(x)$的样本，这些样本的平均值可以作为所求期望的近似，即：\n$$ E[f(x)] = \\frac{\\sum_1^Nf(x_i)}{N} = \\frac{1}{b-a}\\int_{a}^{b} f(x) \\mathrm{d} x $$\n那么就有：\n$$ \\int_{a}^{b} f(x) \\mathrm{d} x = (b-a)\\frac{\\sum_1^Nf(x_i)}{N} $$\n参考资料 #    https://blog.csdn.net/weixin_41503009/article/details/107853383  https://github.com/wangshusen/DRL "},{"id":7,"href":"/blog/posts/ai/conv-fft/","title":"Convolution with Image Filter \u0026\u0026 Convolution with fft/ifft","section":"Posts","content":"我们可以使用傅立叶变换实现卷积，具体做法大概就是先对数据和卷积核进行傅立叶变换将数据变换到频域，然后卷积就是频域上的乘积, 最后做逆傅立叶变换转化回原来的空域。\nr = ifft(fft(x).*fft(h)) 但是需要注意的是傅立叶变换做的卷积对应的是Circular Convolution。\ng = conv2(f,h,\u0026#39;same\u0026#39;); g_fft2 = ifft2(fft2(circshift(f,[-1,-1])).*fft2(rot90(h,2),2,4)); 使用傅立叶变换如果要获得和Circular Convolution一模一样的结果，需要对数据做一个shift，shift的大小和卷积核大小有关，二维情况就是[-w/2, -h/2]；然后卷积核也要反转一下，因为conv2卷积是倒着的。\n"},{"id":8,"href":"/blog/posts/ai/vae-more/","title":"More about Variational Autoencoder","section":"Posts","content":"一些关于VAE的扩展知识。\n不断更新中\u0026hellip;\nTraining Tips #   spectral regularization1 有利于稳定VAE的训练2 训练的时候可以使用KL cost annealing3的训练策略，KL项的权重一开始为0，只关注重建，等重建能力差不多了，再逐渐增加KL项的权重到1。 限制KL项中logvar.exp()，防止其值过大。4  强制clip weight初始化要小, 或使用$log(\\sigma^2)$5    Related Models \u0026amp;\u0026amp; Papers #  Beta-VAE #   beta-vae6就是给KLD加了一个权重beta。\n $$ \\mathcal{F}(\\theta, \\phi, \\beta ; \\mathbf{x}, \\mathbf{z}) \\geq \\mathcal{L}(\\theta, \\phi ; \\mathbf{x}, \\mathbf{z}, \\beta)=\\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\beta D_{K L}\\left(q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right) $$  原文还设计了一个评测指标用于评测latent representation disentangle好不好的方法，简单来说是这样的：\n 前提是：假设数据x是有一系列的disentangle factor y得到的，而且我们有一个ground truth simulator可以根据这些factor合成出数据x。\n 具体方法：\n 随机选一个factor y。 (Choose a factor y ∼ Unif[1\u0026hellip;K]) 对一个batch里的L个样本（一个batch中每个sample的factor都一样）：  sample 两个latent representation（人工设计的），这两个样本，在刚刚选的那个factor上相同，其他随机。 用simulator合成两张图像，用之前训练好的vae-encoder预测一个latent representaiton（网络学到的） 计算两张图像的latent representation的差值。   使用L个sample差值的平均数作为一个线性分类器的输入，预测刚刚选的factor是哪个。用预测的准确性作为评测指标。  Intuition：\n 分类器会被强行设计的只有线性分类能力。 如果vae学习到的latent representation够好，那理论上它应该有一维就是代表factor y， 在这个维度，每个样本的两张图像的值应该是很接近的，也就是说它们的差值会很接近0，那么分类器只需要找到很接近0的那一项就可以预测出刚刚选的factor是哪个了。  Extended beta VAE #   在Understanding disentangling in β -VAE7中，作者进一步扩展了beta VAE， 方法如下：\n $$ \\mathcal{L}(\\theta, \\phi ; \\mathbf{x}, \\mathbf{z}, C)=\\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\gamma\\left|D_{K L}\\left(q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right)-C\\right| $$  其中C是一个逐渐增大的常数。\nIntuition是，当C逐渐增大时，KLD的作用逐渐变小，重构误差逐渐占主导地位，因此我们可以学习到更好的重构图像。这点与KLD annealing正好是反过来的。\nSpectral Regularization #   简单来说：spectral regularization1用于降低 sensitivity to perturbation（测试数据微小的变动会引起结果很大的变化），方法是使用一个正则化，约束网络权重矩阵的谱范数，不让其过大。\n 原文：To reduce the sensitivity to perturbation, we propose a simple and effective regularization method, referred to as spectral norm regularization, which penalizes the high spectral norm of weight matrices in neural networks.\n 设$f$是我们要学习的函数，我们的目标是要让perturbation，即 $f(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})$ 尽量小。通常$f$ 都是一个线性函数加一个非线性激活函数。考虑到深度学习常用ReLU等piecewise linear function[?]作为激活函数。当$\\boldsymbol{\\xi}$很小的，我们可以把$f$看出一个线性函数。因此我们有： $$ \\frac{\\left|f_{\\Theta}(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})\\right|_{2}}{|\\boldsymbol{\\xi}|_{2}}=\\frac{\\left|\\left(W_{\\Theta, \\boldsymbol{x}}(\\boldsymbol{x}+\\boldsymbol{\\xi})+\\boldsymbol{b}_{\\Theta, \\boldsymbol{x}}\\right)-\\left(W_{\\Theta, \\boldsymbol{x}} \\boldsymbol{x}+\\boldsymbol{b}_{\\Theta, \\boldsymbol{x}}\\right)\\right|_{2}}{|\\boldsymbol{\\xi}|_{2}}=\\frac{\\left|W_{\\Theta, \\boldsymbol{x}} \\boldsymbol{\\xi}\\right|_{2}}{|\\boldsymbol{\\xi}|_{2}} \\leq \\sigma\\left(W_{\\Theta, \\boldsymbol{x}}\\right), $$ 其中$\\boldsymbol{\\xi}$是个常数，因此，如果我们要让$f(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})$ 尽量小，我们应该让上界$\\sigma\\left(W_{\\Theta, \\boldsymbol{x}}\\right)$尽量小。\n即：我们应当约束网络权重矩阵的谱范数，不让其过大。\nLadder Variational Autoencoders #   Ladder Variational Autoencoders8\n 提出了一个类似Ladder Network的Ladder Variational Autoencoders。  具体结构：TODO\n指出Batch Normal和KL Warm Up对训练很重要。  Re-balancing Variational Autoencoder Loss #   这篇文章9分析了RNN-based VAE中posterior collapse问题出现的原因，并提出了一个loss减轻这个问题。\nposterior collapse：任务是在RNN-VAE中，因为decoder训练的时候有ground truth作为teaching force，因此当latent representation很差的时候，decoder会忽视掉latent representation，导致latent representation和先验很接近，KLD这项很小，虽然loss可能不高，reconstruct因为有teaching force，loss不会太高，但是总体效果是很差的。\n分析原因，就是reconstruct loss被低估了，因此需要用系数调整，思路和beta-vae是一样的。\n $$ \\begin{aligned} \\mathcal{L}(x ; \\theta, \\phi)=\u0026 \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\\\ \u0026-D_{\\mathrm{KL}}\\left(q_{\\phi}(z \\mid x) \\| p(z)\\right), \\alpha1 \\end{aligned} $$  也可以改成beta-vae的形式，不过这里beta是在0-1区间，而beta-vae是\u0026gt;1，因为目标不一样。这里是向增大reconstruction的占比 - 等价于缩小KLD。\n $$ \\begin{aligned} \\mathcal{L}(x ; \\theta, \\phi)=\u0026 \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\\\ \u0026-\\beta D_{\\mathrm{KL}}\\left(q_{\\phi}(z \\mid x) \\| p(z)\\right), 0 \\leq \\betaCVAE #   Learning Structured Output Representation using Deep Conditional Generative Models10这篇论文提出了CVAE。\n传统VAE估计的是边缘概率$P(X)$， 而CVAE估计的是条件概率$P(X|Y)$。例如在图像中，X就是图像，Y是图像的类别，CVAE不仅可以生成图像，还可以指定生成哪种类型的图像。\n传统VAE的ELBO是： $$ \\log P(X)-D_{K L}[Q(z \\mid X) | P(z \\mid X)]=E[\\log P(X \\mid z)]-D_{K L}[Q(z \\mid X) | P(z)] $$ 加入条件概率，直观地，相当于我们让encoder同时依赖于X和Y - $Q(z|x,y)$，让decoder依赖于Z和Y - $P(x|z,y)$，那我们的ELBO就变成了: $$ \\log P(X \\mid c)-D_{K L}[Q(z \\mid X, c) | P(z \\mid X, c)]=E[\\log P(X \\mid z, c)]-D_{K L}[Q(z \\mid X, c) | P(z \\mid c)] $$ 详细的推导：\n TODO  Conditional Variational Image Deraining #   这篇文章11是CVAE的一个应用，它的任务是把下雨时候照的照片中的雨给去掉。\n要理解它的做法，我们需要重新理解下CVAE究竟在做什么。\n 传统的VAE都是输入一张图像X，然后我们用encoder学它的latent representation，然后我们再sample，经过decoder生成图像。如果我们想要生成指定类别的图像，这种方法是做不到的。\n那么一个很直接的思路是，我们可不可以输入一个类别，让encoder学习输出这个类别的latent representation，然后我们再通过同样的过程生成特定类别的图像？答案是很难。\n CVAE的做法是什么呢？我们给encoder一个额外信息，我们告诉它某个类别的图像是长什么样的，这样它在学习的时候会更有效。在这种角度下，CVAE其实是对传统VAE的一个小优化。\n在这篇去雨论文中，它的思路是类似的：\n 我们把下雨照片当作“类别”，每一张下雨照片都是一个类别 我们希望学习每个类别的latent representation，即每张下雨照片，干净版本的latent representation。 然后我们通过多次sample，decode出多张干净照片，取个平均作为最终结果。 只输入下雨照片不好训练encoder，因此我们加入一个condition，训练的时候我们把干净照片也加进去，帮助encoder进行训练。 但是问题是预测的时候，我们没有干净照片作为encoder的额外输入，这时候encoder就没法用了，为此作者引入了一个额外的prior network去模仿encoder的encode过程，但是只用rainy image作为输入。   预测的时候使用的是prior network。\n NVAE #   英伟达2020的一个工作2，主要说两点：\n 提出了一个精心设计的VAE网络，并指出VAE可以在网络设计上多下点功夫。 提出了多个稳定KLD的方法：  Residual Normal Distributions: Spectral Regularization (SR) More Expressive Approximate Posteriors with Normalizing Flows    具体还没细看。\nResources #    Denoising Criterion for Variational Autoencoding Framework  Some useful tricks in training variational autoencoder  https://github.com/loliverhennigh/Variational-autoencoder-tricks-and-tips/blob/master/README.md  Loss设计 #     Balancing Reconstruction vs KL Loss Variational Autoencoder\n   Why don\u0026rsquo;t we use MSE as a reconstruction loss for VAE ? #399\n   how to weight KLD loss vs reconstruction loss in variational auto-encoder\n  Github #  这里是一些Github上使用VAE的项目代码：\n  AntixK/PyTorch-VAE  NVlabs/NVAE  Reference #    Yoshida 和 Miyato - 2017 -Spectral Norm Regularization for Improving the Generalizability of Deep Learning\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Vahdat 和 Kautz - 2020 - NVAE A Deep Hierarchical Variational Autoencoder\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Bowman 等。 - 2016 - Generating Sentences from a Continuous Space\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  https://discuss.pytorch.org/t/kld-loss-goes-nan-during-vae-training/42305\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  https://github.com/y0ast/VAE-Torch/issues/3\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Higgins 等。 - 2017 - β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Burgess 等。 - 2018 - Understanding disentangling in $\\beta$-VAE\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Sønderby 等。 - 2016 - Ladder Variational Autoencoders\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Yan 等。 - 2020 - Re-balancing Variational Autoencoder Loss for Molecule Sequence Generation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Sohn 等。 - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Du 等。 - 2020 - Conditional Variational Image Deraining\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  "},{"id":9,"href":"/blog/posts/misc/hugo-book/","title":"Hugo-Book Shortcodes Usages","section":"Posts","content":"Hugo-Book Shortcodes Usages\n 如果公式里出现矩阵，需要用div将$$包括起来，否则无法正常显。见 Link 如果公式需要换行，需要用六个斜杠\\\\\\，见 Link   Buttons #  {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}}{{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Get Home  Contribute  Columns #  Columns help organize shorter pieces of content horizontally for readability.\n{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example #  Left Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.  Mid Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!  Right Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.   Details #  Details shortcode is a helper for details html5 element. It is going to replace expand shortcode.\nExample #  {{\u0026lt; details \u0026#34;Title\u0026#34; [open] \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}}{{\u0026lt; details title=\u0026#34;Title\u0026#34; open=true \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}}Title Markdown content #  Lorem markdownum insigne\u0026hellip;   Expand #  Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nExample #  Default #  {{\u0026lt; expand \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}  展开 ↕  Markdown content Lorem markdownum insigne\u0026hellip;    With Custom Label #  {{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}}## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}  Custom Label ...  Markdown content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.    Hints #  Hint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{\u0026lt; hint [info|warning|danger] \u0026gt;}}**Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}}Example #  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Mermaid Chart #   Mermaid is library for generating svg charts and diagrams from text.\nExample #  {{\u0026lt; mermaid [class=\u0026#34;text-center\u0026#34;]\u0026gt;}}sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}}   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) sequenceDiagram Alice-Bob: Hello Bob, how are you? alt is sick Bob-Alice: Not so good :( else is well Bob-Alice: Feeling fresh like a daisy end opt Extra response Bob-Alice: Thanks for asking end   Tabs #  Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}}{{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}}# MacOS Content {{\u0026lt; /tab \u0026gt;}}{{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}}# Linux Content {{\u0026lt; /tab \u0026gt;}}{{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}}# Windows Content {{\u0026lt; /tab \u0026gt;}}{{\u0026lt; /tabs \u0026gt;}}Example #  MacOS MacOS #  This is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux Linux #  This is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows Windows #  This is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":10,"href":"/blog/posts/algorithms/AC%E8%87%AA%E5%8A%A8%E6%9C%BA/","title":"AC自动机","section":"Posts","content":"AC自动机用于解决下述问题及其同类问题:\n 给定一系列模式串和一个文本串，判断有多少模式串出现在文本串中，给出数目和对应模式串的出现位置。\n 主要思想 #  AC自动机主要用到了 Trie树和 KMP算法的思想。\n构造 #  我们首先将所有模式串构建成一个Trie树，构建方法与普通Trie树的构建方法没有区别。\nAC自动机的树结构主要是比Trie树多一个fail指针（类似于KMP算法的next数组）。\nclass AcNode { public: AcNode* m_childs[26] = { nullptr }; bool m_is_end; // 是否是单词边界  char m_c; // 新增内容  AcNode* fail = nullptr; }; 该fail指针是AC自动机的核心，它表示当前树节点匹配失败，下一个要尝试匹配的树节点。实际上，fail指针所指的树节点所构成的字符串是当前树节点构成字符串的后缀。\nFail指针的求法\nroot的孩子的fail指针是root, 对于其他节点，其fail指针是父亲的fail指针所指的树节点的孩子中和当前节点字母相同的节点。如果不存在这样的节点，则尝试寻找fail指针所指树节点的fail指针所指的节点有无这样的节点，直到root节点。\n比较绕口，举个例子：\n{% include image.html url=\u0026quot;/assets/img/ac_automaton1.png\u0026quot; description=\u0026ldquo;图1. Fail指针求解\u0026rdquo; size=\u0026ldquo;30%\u0026rdquo; %}\n如图1所示，h2的fail指针是h1。求解过程是h2 -\u0026gt; s2 -\u0026gt; s2.fail = s1 -\u0026gt; h1。\n匹配 #  匹配的时候也很简单，对于文本串中的每个字母，从AC自动机的root开始尝试匹配。\n 匹配成功，则跳至其fail指针，继续尝试匹配，直到跳回root指针 - 处理类似she同时包含she和he的情况。 匹配失败，也要跳到其fail指针继续尝试匹配。  根据需要记录匹配成功的数目，位置以及对应的模式串。\nC++实现 #  具体实现参见 ac_automaton.cpp，这里解释一下求fail指针的build函数。\nvoid build() { queue\u0026lt;AcNode*\u0026gt; q; for(int i=0; i\u0026lt;26; i++) { if(root-\u0026gt;m_childs[i] != NULL) { root-\u0026gt;m_childs[i]-\u0026gt;fail = root; q.push(root-\u0026gt;m_childs[i]); } else { root-\u0026gt;m_childs[i] = root; } } AcNode* current; while(!q.empty()) { current = q.front(); q.pop(); for(int i=0; i\u0026lt;26;i ++) { if(current-\u0026gt;m_childs[i] != NULL) { current-\u0026gt;m_childs[i]-\u0026gt;fail = current-\u0026gt;fail-\u0026gt;m_childs[i]; q.push(current-\u0026gt;m_childs[i]); } else { current-\u0026gt;m_childs[i] = current-\u0026gt;fail-\u0026gt;m_childs[i]; // 路径压缩  } } } } 要点:\n 使用BFS逐层的求解fail指针。 不存父亲指针，遍历到当前节点时，计算子节点的fail指针。 注意把root指针的所有“空”孩子设置为root，否则容易在计算孩子的fail指针时发生segemntation fault，current-\u0026gt;fail-\u0026gt;m_childs[i]这个表达式fail可能是root。 不用递归寻找符合要求的fail指针, 使用类似并查集的思想进行路径压缩。  路径压缩\n核心为这行代码:\ncurrent-\u0026gt;m_childs[i] = current-\u0026gt;fail-\u0026gt;m_childs[i]; 当current-\u0026gt;fail-\u0026gt;m_childs[i] = NULL的时候，这行代码将m_childs[i]置为其fail-\u0026gt;m_childs[i]，由于是逐层求解fail的，因此当fail-\u0026gt;m_childs[i]也等于NULL的时候，其值会在之前被设置成其fail-\u0026gt;m_childs[i]。\n因此current-\u0026gt;fail-\u0026gt;m_childs[i];总是指向满足要求的节点或者root。\n 满足要求: fail指针所指的树节点所构成的字符串是当前树节点构成字符串的后缀。\n 参考资料 #   [洛谷日报第44期]强势图解AC自动机: Link "},{"id":11,"href":"/blog/posts/algorithms/trie%E6%A0%91/","title":"Trie树","section":"Posts","content":"Trie树可以:\n 压缩存储大量的字符串 快速找出具有相同前缀的字符串 快速按字典序对字符串进行排序 \u0026hellip;  主要操作 #  Trie树的主要操作包括:\n insert: 插入字符串。 search: 查询某字符串是否存在。 startsWith: 查询包含某前缀的字符串。  [str]: 返回所有包含该前缀的字符串。 bool: 返回是否存在。    实现 #  Trie树的实现和普通树的实现类似，可以用数组或链表实现。 这里的例子是链表实现的，如下是树节点的定义。\nclass TrieNode { public: TrieNode* m_childs[26] = { nullptr }; bool m_is_end; // 是否是单词边界  char m_c; }; Trie树的定义如下:\nclass Trie { public: Trie(); void insert(string word); bool search(string word); bool startsWith(string prefix); protected: TrieNode* root; }; "},{"id":12,"href":"/blog/posts/programming/semaphores/","title":"Semaphores(信号量)","section":"Posts","content":"Semaphores是一种同步机制（Concurrency Mechanisms），它用来协调各个进程访问公共资源。其基本思想如下所述：\n 两个或多个进程通过一个信号量进行协调，当一个进程需要某个资源时，它需要申请并等待一个信号，如果信号没有来临则等待。\n General Semaphores #  general semaphore的一种实现。\nstruct semahpore { int count; queueType queue; }; void semWait(semaphore a) { s.count--; if (s.count \u0026lt; 0) { /* place this process in s.queue. */ /* block this process. */ } } void semSignal(semaphore a) { s.count++; if(s.count \u0026lt;= 0) { /* remove a process from s.queue. */ /* place process p on ready list. */ } }   count的含义\n 大于0时：指示当前可以加进来的进程数。 小于等于0时：绝对值指示当前想加入还未加入的进程数。    队列里存储的就是等待加入的进程。\n  Binary Semaphore #  struct binary_semaphore { enum {zero, one} value; queueType queue; }; // B: Binary void semWaitB(binary_semaphore s) { if (s.value == one) s.value = zero; else { /* place this process in s.queue. */ /* block this process. */ } } void semSignalB(semaphore s) { if (s.queue is empty()) s.value = one; else { /* remove a process P from s.queue. */ /* place process P on ready list. */ } } 优缺点 #   比较难编程，当需要同时使用多个semaphore的时候，需要仔细安排semaphore调用顺序等，否则容易出现死锁。  例子：\n/* program producersonsumer */ semaphore n = 0, s = 1; void producer() { while (true) { produce(); semWait(s); append(); semSignal(s); semSignal(n); } } void consumer() { while(true) { semWait(n); semWait(s); take(); semSignal(s); consume(); } } void main() { parbegin (producer, consumer); }  这里 semaphore n 用于防止consumer消耗空buffer，s用于buffer的mutual exclusion。 如果buffer为空，且consumer的 semWait(n); 与 semWait(s); 互换，则第一个semWait能成功进入，但会在第二semWait被阻塞。 这时由于，buffer被consumer占用，producer无法生产新的product加入buffer， semWait(s);将被一直阻塞，产生死锁。 "},{"id":13,"href":"/blog/posts/algorithms/%E6%B5%B7%E6%98%8E%E7%A0%81/","title":"错误检测-海(汉)明码","section":"Posts","content":"Hamming code，海明码，汉明码都是一个东西。它是一种编码方式，通常用在网络信息传输中，通过这种编码方式编码出来的二进制数据具有检测一位错误位的能力。\n例如：\n 假设要传输的二进制数据为 1011001 。 通过海明码编码后（如何编码后面会说），得到 101 0100 1110 。 假设从右往左第6位从0变为了1。 通过某种方法我们可以从产生变化后的数据 101 0110 1110 得知第六位发生改变。  下面则将具体讲解这些过程。\n编码过程 #  第一步是确定冗余位(bit)的数目，这些位将为检错提供必要的信息。\n根据算法要求，冗余位的数目(r)必须满足： $$ 2^r \\geq m + r + 1 $$ 其中m为原始数据位的数目。\n第二步是确定冗余位的位置\n同样根据算法设计，冗余位将放在位置编号为2的幂次的位置上。\n如：1,2,4,8\u0026hellip;\n假设要传输的数据为 1011001 ，则编码后各个数据位的排布应如下图所示：\n 第三步是确定冗余位的值\n计算方法如下：\n 处于第$2^i$位的冗余位的值，将是所有位置编码(二进制表示)中第i位为1的那些位置的原始数据的偶校验。\n 如R1，它的值将是第1,3,5,7,9,11这些位置上的数据的偶校验，所以\n由于3，5，7，9，11中1的个数为偶数，故R1=0（偶校验～1的个数为偶数个）。\n 假设要传输的数据为 1011001 ，则编码后结果如下：\n 检错与纠错 #  同上，假设要传输的数据为 1011001 ，编码后为1010 1001 110 。\n假设实际传输过去的数据为 1010 1101 110 ，即从右往左第6位由0变为了1。\n这时候，如果我们再用计算冗余位值的方法重新计算一遍冗余位的值，我们会发现：\nR4R3R2R1 = 0110 即十进制下的6。\n这表明从右往左第6位发生错误，这时候我们将其反转即可纠正。\nIntuition #  实际上，我们的目标是得到一个数字，这个数字表示数据中出错那一位的位置。\n如果我们将其表示为二进制的形式，那么我们的任务即变为了确定各个bit位是0还是1。\n在前面我曾要求过这个表达式：\n$2^r \\geq m + r + 1$\n它的含义即在于，如果我们利用r个冗余位确定这个二进制数字（r个冗余位确定r个数字中的bit位），那么我们必须保证这个数字的范围能够表示编码数据串中的所有位置。\n可能有人会问，既然如此，那满足这个表达式$2^r \\geq m + r$ 不就可以了吗，为什么要+1呢？\n 因为位置编码必须从1开始，从0开始没法纠错。因此位置编码的最大值位m+r+1。\n  海明码的巧妙之处就在于它偶校验的分组上，每个冗余位都对应了一个bit位为1的那些位置。\n  一开始将数据编码成海明码的时候，所有组都是满足偶校验的，即1的个数为偶数。\n  如果某一位发生变化，不管是从0变为1还是从1变为0，它所在组对应的偶校验均会变为1。\n  而且它只属于-它位置编码为1的那几位对应的那些组，也只有这些组的偶校验会变成1。\n  这样，如果我们对所有组求一次偶校验，组成的二进制数表示的正好是发生变化那个位的位置。\n  换句话说，海明码将错误位置的确认分散到了各个组上，通过一次能偶校验，能够确定错误位置属不属于这个组，经过r次偶校验后，便能确定其具体位置。\n参考资料 #   https://www.geeksforgeeks.org/computer-network-hamming-code/\n"},{"id":14,"href":"/blog/posts/ai/Generalized-Linear-Model/","title":"How genralized linear model work?","section":"Posts","content":"本文将简单的讲述：GLM是如何工作的？\nOur goal 在讨论GLM之前，我们还是先要明确我们的目标是什么：\n 给定一些 feature X，我们需要预测一个y。 即我们需要构造一个 hpythoesis: $h_\\theta(x) = y$\n How GLM work?  假设 y 的取值服从某个分布 如果这个分布可以写成指数族的形式：$p(y; \\eta) = b(y) exp(\\eta^T T(y) − a(\\eta))$ 则有一个性质：T(y)是y的充分统计量 然后我们则用 y 在该分布下的数学期望去预测y，即我们让 $h_\\theta(x) = E(y)$ 除此之外还假设 $\\eta$ 与 X 线性相关，即： $\\eta = \\theta^TX$  Example  Logistic Regression   假设y的取值服从伯努利分布： $y|x; \\theta ∼ Bernoulli(\\phi)$ 伯努利分布在指数族中，将其改写成指数族的形式：  \\[ \\begin{align} p(y; \\phi) \u0026= \\phi^y (1-\\phi)^{1-y} \\\\ \u0026= exp(ylog(\\phi) + (1-y)log(1-\\phi))\\\\ \u0026= exp(ylog\\frac{\\phi}{1-\\phi} +log(1-\\phi)) \\end{align} \\]\n​ 由此可得： \\( T(y) = y \\\\ \\eta = log\\frac{\\phi}{1-\\phi} \\) ​ 反解$\\phi$ ： \\( \\phi = \\frac{e^{\\eta}}{1+e^{\\eta}}= \\frac{1}{1+e^{-\\eta}} \\)\n 利用y在伯努利分布下的数学期望预测y：  \\[ \\begin{align} y \u0026= E[y|x;\\theta] \\\\ \u0026=\\phi \\\\ \u0026= \\frac{1}{1+e^{-\\eta}} \\end{align} \\]\n 假设 $\\eta$ 与 X 线性相关，即： $\\eta = \\theta^TX$ 。可得：  \\[ y = \\frac{1}{1+e^{-\\theta^TX}} \\]\n"},{"id":15,"href":"/blog/posts/algorithms/%E5%85%8B%E9%B2%81%E6%96%AF%E5%8D%A1%E5%B0%94%E7%AE%97%E6%B3%95/","title":"克鲁斯卡尔算法","section":"Posts","content":"首先，克鲁斯卡尔算法是用来求最小生成树的。另一种求最小生成树的算法叫普林姆算法（Prim）。\n克鲁斯卡尔算法本质是贪心，每次选取 不与当前已有边构成环 权值最小 的边作为生成树的边。\n算法细节 #   判断是否构成环使用并查集  复杂度分析 #  使用并查集的克鲁斯卡尔算法时间复杂度为O(eloge)，其中loge为并查集判断环所需时间，每条边都要判断一次，因此为O(eloge)。\n"},{"id":16,"href":"/blog/posts/algorithms/%E6%99%AE%E6%9E%97%E5%A7%86%E7%AE%97%E6%B3%95/","title":"普林姆算法","section":"Posts","content":"普林姆算法也是用来求最小生成树的，与克鲁斯卡尔算法遍历边不同，普林姆遍历的是点。\n普林姆算法同样基于贪心，以任意点为初始点，每次选取与已选点相连的边中权值最小的边，并把与这条边相连的点加入已选点集合。\n算法实现 #   维护一个数组 minEdge[i]  含义为已选点集合中到第i个点 最小权值的边的终点，没有边则为无穷大。\n每次选minEdge中最小的点加入已选点集合，并更新minEdge数组。 重复操作，直到所有点加入已选点集合。  优化 #   使用优先队列维护minEdge  每往已选点集合加入点时，就把与该点相连的所有边都加入优先队列。而当取边时，需要判断一下边的终点是否在已选点集合中（可以使用一个标记数组）。\n时间复杂度分析 #   非优先队列法   在minEdge中找最小的时间复杂度为O(n)。 更新数组时间复杂度总和为O(e) —— 每个点都会更新与它相连的边，所有边加起来为e。 一共要进行n次在minEdge中找最小的操作。  故总的时间复杂度为O(n^2+e)。\n优先队列法  优先队列加边出边的时间复杂度都是O(loge)。\n 所有边都会进入优先队列至少一次，至多两次。时间复杂度为O(eloge)。 出边次数最少为n次，最多为e次，时间复杂度也是O(eloge)。  故总的时间复杂度为O(eloge)。\n分析： 当边数达到cn^2或以上时，用非优先队列法要好，反之可以使用优先队列法。\n当e = n^2 ,  O(eloge) = O(n^2logn) , O(n^2+e) = O(n^2) 。\n"},{"id":17,"href":"/blog/posts/algorithms/%E5%B9%B6%E6%9F%A5%E9%9B%86/","title":"并查集","section":"Posts","content":"使用并查集可以快速判断两个元素是否属于同一个集合。\n算法 #  基本思路 #  使用树来表示集合。\n 初始时，所有元素都分别是一棵树。 若两个元素属于同一个集合，则用一个元素作为另一个元素的孩子。  实现\n数据结构：\n fa[i] : 保存元素i的祖先  第一步：初始化\nfor(int i=0; i \u0026lt; n; ++i) fa[i] = i;\t// 初始时，所有元素都分别是一棵树(祖先是自己） 第二步：根据关系构造并查集\n// 查找树根 int find(int x) { if(fa[x] == x) return x; return find(fa[x]); } int temp_a, temp_b; for(int i=0; i \u0026lt; m; ++i){ cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, b属于同一集合  temp_a = find(a), temp_b = find(b); fa[a] = b; } Note: 先找到树根，不管相不相同，都让a是b的孩子。\n第三步：查询方法\nfor(int i=0; i \u0026lt; T; ++i){ cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; if(find(a) == find(b)) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } Note: 判断树根是否相同即可。\n优化 #   降低树的深度（路径压缩）  find 操作耗费时间取决于树的深度，因此，如果在 find  执行过程让树的深度降低，则可以降低后续操作所需的时间。\n方法： 让树根的后代们尽量都是树根的孩子。\n// 查找树根 int find(int x) { if(fa[x] == x) return x; //return find(fa[x]);  return fa[x] = find(fa[x]); } Note: fa[x] = find(fa[x]) 的值为左值 fa[x] ，等价于先fa[x] = find(fa[x]);  再 return fa[x]; 。\n让小树接在大树上  每次发生树的拼接之后的 find 操作都需要重新降低树的深度，而显然元素少的树接到元素多的树上，需要的 重组次数较少。\n例子：\n 灰色圆圈即为要重组的元素，显然小接大要合算。\n实现方法：\n r[i] : 表示以元素i为根的树的相对大小  // 封装一下 void Union(int a, int b) { int temp_a = find(a), temp_b = find(b); if(r[temp_a] \u0026gt;= r[temp_b]) { fa[b] = a; //小树的父亲等于大树  if(r[temp_a] == r[temp_b]) ++r[a]; // b为a的孩子-\u0026gt;树变大了  } else fa[a] = b; } for(int i=0; i \u0026lt; n; ++i) { fa[i] = i;\tr[i] = 0;\t// 记得初始化 } for(int i=0; i \u0026lt; m; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, b属于同一集合 \tUnion(a,b); } 实验 #  输入：\n 10 8 1 2 1 3 1 4 1 5 2 6 5 7 5 8 0 9 5 1 6 1 3 5 7 0 9 1 0 输出：\nYes Yes Yes Yes No 完整代码：\n// 并查集 #include \u0026lt;iostream\u0026gt;using namespace std; const int MAXN = 100; int fa[MAXN], r[MAXN], n, m; // n为元素数,m为关系数  // 查找树根 int find(int x) { if(fa[x] == x) return x; return find(fa[x]); } void Union(int a, int b) { int temp_a = find(a), temp_b = find(b); if(r[temp_a] \u0026gt;= r[temp_b]){ fa[b] = a; //小树的父亲等于大树  if(r[temp_a] == r[temp_b]) ++r[a]; // b为a的孩子-\u0026gt;树变大了  } else fa[a] = b; } int main() { cin \u0026gt;\u0026gt; n; // 初始化  for(int i=0; i \u0026lt; n; ++i) { fa[i] = i;\t// 初始时，所有元素都分别是一棵树(祖先是自己）  r[i] = 0; } cin \u0026gt;\u0026gt; m; // 构造并查集  int a, b; for(int i=0; i \u0026lt; m; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, b属于同一集合  Union(a,b); } // 查询  int T; cin \u0026gt;\u0026gt; T; for(int i=0; i \u0026lt; T; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; if(find(a) == find(b)) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } return 0; } 复杂度分析 #   优化前的算法  算法复杂度主要取决于：1. 构造并查集 2. 查询操作\n 构造并查集复杂度分析：  查询find操作递归次数不会超过树的深度，树的深度不会大于n，因此，find操作复杂度为O(logn)。\n构造时时间花费主要在find操作上，且要执行2m次find操作。因此构造并查集的时间复杂度为O(2mlogn), 忽略常数为O(mlogn)。\n 查询操作复杂度分析：  查询操作需要两次find操作，时间复杂度为O(logn)。\n优化后的算法  先说结论：O((n+m)log*n) ，其中log*为增长及其缓慢的 迭代函数，通常可以视为常数。\n证明为构造性证明，较为复杂，这里不做扩展。\n"},{"id":18,"href":"/blog/posts/ai/Principal-component-analysis/","title":"Principal component analysis","section":"Posts","content":"PCA (Principal component analysis) 是一种给数据降维的方法。\n利用PCA，能将一堆高维空间的数据映射到一个低维空间，并最大限度保持它们之间的可区分性。\n 可以看到，图中的数据点大致分布在一条直线上。\n因此，我们能够将点投影到直线上，用直线上的点到原点的距离代替原来二维向量。\n这样我们的数据就从 二维降到了一维。\n 推导过程 #   设输入数据为X ，X 为 (n * m) 的矩阵，每一行为一个sample。  如果我们要将 数据 转换到一个低维空间，我们应该对 X 做一次线性变换（即更换基底）。\n 设新的 基底 为 P ， P 为一个 (m * k) 的矩阵，每一列为一个基底。 经过变换后，设新的数据为 Y, 则 $Y=XP$ 。  X 的协方差矩阵为 $X^TX$，我们希望经过变换后的 Y 的协方差矩阵（$Y^TY$）\n 对角线上元素绝对值尽可能大 非对角线上元素绝对值绝对值尽可能小。  即我们希望，在新的基底下，各个feature的关联性很小，而在同一个feature上，能尽最大可能保持数据的variance。\nY的协方差矩阵的数学表达式:\n$$C_Y = Y^TY = (XP)^T(XP) = P^TX^TXP$$\n可以证明，为了使 $C_Y$ 满足上述条件，P 应为 $X^TX$ 的特征向量矩阵，其中 P 的每一列为一个特征向量。\n那么，由矩阵对角化的知识，可以得到\n$$C_Y = P^TX^TXP = D$$\n其中D为一对角矩阵，对角线上元素为特征向量对应的特征值。\n在这里，每个特征值还对应了新的feature的方差（variance），它的值的大小反映了新的feature用于区分数据的能力。方差小说明这个feature对大部分数据来说基本都一样（直观来说，既然大家都一样，我们就可以说这个feature是多余的）。\n因此，我们可以将特征向量根据特征值排序，根据需要将原始数据转换为低维数据，并尽最大可能保持数据的有效性。\n参考资料 #  Stackexhange\n  How would you explain covariance to someone who understands only the mean?  Making sense of principal component analysis, eigenvectors \u0026amp; eigenvalues  Why is the eigenvector of a covariance matrix equal to a principal component?  Intuition on the definition of the covariance  Does the magnitude of covariance have any real meaning?  Intuitive understanding covariance, cross-covariance, auto-/cross-correliation and power spectrum density  Wikipedia\n  Covariance  Cross product  Covariance matrix  Other\n A Tutorial on Principal Component Analysis\u0026rsquo;by Jonathon Shlens  Principal Components Analysis "},{"id":19,"href":"/blog/posts/misc/%E6%B4%9B%E4%BC%A6%E5%85%B9%E5%8F%98%E6%8D%A2/","title":"洛伦兹变换","section":"Posts","content":"洛伦兹变换是从光速不变原理推出的，不同坐标系坐标之间的转换关系。\n**光速不变原理：**对任何参考系，光速都为一个固定值C\n狭义相对性原理： 在所有惯性系中，物理定律都有相同的表达形式\n 洛伦兹变换推导 #   以二维直角坐标为例，只考虑x轴方向运动\n假设你相对于我向x轴正方向以速度 u 运动，我和你在某个时间相遇，这时候\n 我和你分别以自己为原点建立坐标系。 将我和你的时钟归零    以我看来，假设在 t 时刻， x 位置发生了一个事件。\n设该事件，在你看来，发生在 t‘ 时刻， x‘ 位置。\n接下来，有关我的量都不带 ' ,而有关你的都带 '。\n###相对论出现之前\n那么由于你相对我在运动，则在我看来，你运动了 ut 的距离，并且我认为 你认为事件发生的位置是 $$ x' = x-ut $$ 而反过来，在你看来，你认为你运动了 ut' 的距离，你认为 我认为事件发生的位置是 $$ x = x' + ut' $$\n###相对论出现之后\n如果现在，我们质疑这两个式子的正确性。\n比如，如果在你看来，你觉得你的 x' 与我认为 你认为的 x' 不相等，那么不如给式子乘上一个系数 $\\gamma$ ，通过别的条件计算，如果系数是1，那我就是对的，如果不是，那结果就是那样。 $$ x' = \\gamma (x-ut) $$ 同理，我觉得你的 x 与你认为的我的 x 不相等，同样的，乘一个系数 $\\gamma$ ， 这个系数与之前的一样 [1]。 $$ x = \\gamma (x'+ut') $$ 为了解出这个系数 $\\gamma$ ，我们不妨从一个具体事件出发（当然这可能导致推导不太严谨，但是可以证明，结果是正确的）。\n  假设这个事件以一束光触发，并且在我看来在 t 时间的时候发生\n 由光速不变原理，我们可以得到 [2]\n 在我看来，发生的距离 $x = ct$ 在你看来，距离 $x' = ct'$  由上面四个方程，则能推出洛伦兹变换的坐标变换式 [3]\n$$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx' = \\frac{x-ut}{\\sqrt{1-u^2/c^2}} \u0026amp; (1)\\\nx = \\frac{x'+ut'}{\\sqrt{1-u^2/c^2}} \u0026amp; (2) \\end{array}\n\\right.\n\\end{equation} $$ 方程（1）即由我的坐标转换为你的。\n方程（2）反过来。\n注意： u表示你相对我向正方向运动的速度。\n而利用 (1), (2)式还能够导出 洛伦兹变换的时间变换式 [4]。 $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nt' = \\frac{t-xu/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp; (3)\\\nt = \\frac{t'+x\u0026rsquo;u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp; (4) \\end{array}\n\\right.\n\\end{equation} $$ 仅仅通过这四个式子可能不太容易弄明白 洛伦兹变换究竟意味着什么。\n下面将展示一系列通过洛伦兹变换的到的结果。\n时间差与空间差 #  假设在我看来在(在我的坐标系中)， 在$(x_1,t_1)$ 和 $(x_2,t_2)$ 分别发生了两个事件。\n那么由洛伦兹变换，在你看来 $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx_1' = \\frac{x_1-ut_1}{\\sqrt{1-u^2/c^2}} \u0026amp; \\\nt_1' = \\frac{t_1-x_1u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp;\n\\end{array}\n\\right.\n\\end{equation} $$\n$$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx_2' = \\frac{x_2-ut_2}{\\sqrt{1-u^2/c^2}} \u0026amp; \\\nt_2' = \\frac{t_2-x_2u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp;\n\\end{array}\n\\right.\n\\end{equation} $$\n上下分别相减，则可以得到时间差和空间差的关系 $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\n\\Delta x' = \\frac{\\Delta x -u\\Delta t }{\\sqrt{1-u^2/c^2}}\u0026amp; (5)\\\n\\Delta t' = \\frac{\\Delta t - \\Delta x u/c^2}{\\sqrt{1-u^2/c^2}}\u0026amp; (6) \\end{array}\n\\right.\n\\end{equation} $$ (5) 意味着在我看来的距离 $\\Delta x$，在相对我运动的你看来，要长一些。\n(6) 意味着在我看来的时间差 $\\Delta t$ ，对于相对我运动的你来说，不再是相同的时间差。\n长度收缩 #  现在，假设我和你要测量一个物体的长度，这个物体以u的速度相对我向正方向运动，而你和这个物体一起运动。\n我将通过两个事件测量这个物体的长度。\n 在某一时刻，在物体两端发生了两个事件，分别记为 $(x_1,t),(x_2,t)$ 那么两个事件的空间差即为物体的长度  那么由前面的空间差的关系，你测量出的长度为 $\\Delta x'$ ，而我测量出的长度为 $\\Delta x$，由公式5可知，我测出的长度比你测出的要短一些。\n即相对物体运动的观察者 测出的物体长度 要比相对物体静止的观察者 测出的要短。\n时间延缓 #  前面提到，我和运动的你，对于两个事件的时间差的认识发生了偏差。\n假设这两个事件对应时钟\n 事件1: 走到这一秒 滴 事件2: 走到下一秒 嗒  那么，由前面的（6）式可以知道，在我的时空坐标中的 1s 与你的时空坐标中的 1s 不再相同。 如果我们忽略两个走针的距离，即$\\Delta x= 0$ (对应其他事件，即同一地点发生的两个事件），那么我们可以得到 $$ \\Delta t' = \\frac{\\Delta t }{\\sqrt{1-u^2/c^2}} $$ 在相对我运动的你的时空坐标中，你的1s变长了。也即你的时间延缓了。\n其实，这里的钟可以提高到更加普遍的范畴上，任何两个事件的时间差都会变长。比如我们的身体的化学反应的进程等等，都被延缓了。\n孪生子悖论\n如果我活了50岁，你可能才到20岁。\n 如果我现在登上飞船，以很高的速度进行星际旅行，那么等我回到地球时，会发现你比我年轻。\n而这在你看来，则相反，你会认为我比你年轻。这就是孪生子悖论。\n 事实上，地球上的你的看法才是对的，狭义相对论只对惯性系有同等意义，即只有在惯性系中上面的等式才成立。对于在飞船上的你，必定经历了一段加速过程（你的时空坐标是非惯性系），在这段时间里，不能利用上面的公式计算 我和你的时间差。\n而要正确计算我和你的年龄差，需要利用积分进行计算。\n参考： 维基百科：双生子佯谬\n同时性的相对性 #  由上面的公式（6）我们还可以得到 同时的概念是相对的 。 $$ \\Delta t' = \\frac{\\Delta t - \\Delta x u/c^2}{\\sqrt{1-u^2/c^2}} $$ 两个对我来说同时发生的时间，即 $\\Delta t = 0$ ，对于你来说\n 如果对我来说，它们不同地发生 [5] ($\\Delta x \\not = 0$) ，则 $\\Delta t' \\not= 0$ ，即对我同时发生的两个事件，对你来说不同时发生。 而如果对我来说是同时同地发生的事件，对你来说也是同时同地发生的。[6]  速度的关系 #  由洛伦兹变换还能推导出，在我的时空坐标中的速度与你的时空坐标中的速度之间的关系。 $$ v' = \\frac{\\Delta x'}{\\Delta t'} = \\frac{\\Delta x -u\\Delta t }{\\Delta t - \\Delta x u/c^2} = \\frac{\\Delta x / \\Delta t -u }{1 - \\Delta x/\\Delta t u/c^2} = \\frac{v -u }{1 - v u/c^2} $$\n附录 #  [1] 根据相对性原理，在所有惯性系中物理定律都具有相同的形式，故修正系数 $\\gamma$ 对任何人都应相同。\n[2]\n[3]\n[4]\n[5] 由公式5，对于你也一定不同地\n[6] 对于这个，我们设想一个事件两辆车相撞，它可以分解为两个事件，一个是一辆车在某个时刻到某个地点，另一个是另一辆车在同一时刻到同一地点。这样它们才能相撞。所以如果对我来说同时同地发生的这两个事件，对你来说不是同时同地的，那么对你来说车不会相撞，而车相撞是客观存在的。因此对我来说是同时同地发生的事件，对你来说也是同时同地发生的。\n四维空间的理解 #  在二维坐标中，一个点(x,y)，不论怎么做变换，旋转，平移等等，都只会涉及x,y。用两个坐标就能够描述这个二维世界。\n在三维世界，我们描述一个事件，需要用到（x,y,z,t），我们怎么\u0026hellip;\n待续.\n"},{"id":20,"href":"/blog/posts/algorithms/Fibonacci%E6%95%B0%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95/","title":"Fibonacci数的迭代算法","section":"Posts","content":"使用迭代算法求斐波那契数列，\n时间复杂度O(n)，空间复杂度O(1)。\nint f(int n) { int f1 = 1, f2= 0; while(--n){ f1 = f1+f2; // f(n) = f(n-1)+f(n-2)  f2 = f1-f2;\t// f(n-1) = f(n)-f(n-2)  } return f1; } "},{"id":21,"href":"/blog/posts/algorithms/%E5%BF%AB%E9%80%9F%E5%B9%82/","title":"快速幂算法","section":"Posts","content":"在c，c++语言中，并没有提供求幂的基本运算，通常我们需要自己写函数或者调用STL提供的函数。\n一般情况下，我们写的求幂函数基本上都是循环累乘，时间复杂度为O(n)。虽说是线性的时间复杂度，但求幂运算作为基础运算，往往调用频繁，这时候即使是线性的时间复杂度也将变得难也接受。\n利用快速幂可以快速计算底数的n次幂。其时间复杂度为 O(logn)\n原理 #  以求 $a^{11}$ 为例，将11写成二进制形式 1011。\n则 $a^{11} = a^{2^0+2^1+2^3}=a^{1+2+8} =a^1\\times a^2\\times a^8$\n使用一个累乘器，每次翻倍 base = base*base 逐步得到 $a^1,a^2,a^4,a^8$ ，根据11的二进制，如果为1则乘进结果里。\n实现 #  迭代算法\nint power(int a,int b) { int base = 1,ans = a; while(a \u0026gt; 0){ if(a \u0026amp; 1 == 1) ans *= base;//二进制位为1的才要乘  base *= base; a \u0026gt;\u0026gt;= 1; } return ans; } 递归算法\nint power(int a,int b) { if(b == 1) return a; int temp = power(a,b\u0026gt;\u0026gt;1) * power(a,b\u0026gt;\u0026gt;1); return (b\u0026amp;1 == 1 ? a:1) * temp * temp; // 若指数为偶数则分解成一半，若为奇数，则还要再乘a } 递归算法与迭代算法的思路略有不同。\n"},{"id":22,"href":"/blog/posts/algorithms/%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9A%84%E5%A6%99%E7%94%A8/","title":"位运算的妙用","section":"Posts","content":"对于一些特定问题，巧妙运用位运算能使解法异常简洁和高效，同时，适当运用位运算也能对程序进行优化。\n运用的时候，可能涉及多种位运算。\n计算机中位运算分为以下六种：\n 与 \u0026amp; 或 | 非 ～ 异或 ^ 左移 \u0026laquo; 右移 \u0026raquo;  异或 #  异或具有以下性质（可能还有，下同）：\n a^b = b^a a^a = 0 a^0 = a  实例 #  [leetcode 136 single number]\n Given an array of integers, every element appears twice except for one. Find that single one.\nNote: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?\n **题目大意：**每个元素都出现两次，但有一个只出现一次，找出出现一次的元素。\n 这题如果用标记数组做 时间复杂度，空间复杂度都是 O(n) 。但如果利用异或的前两个性质，则可以将空间复杂度压缩到O(1)。\nclass Solution { public: int singleNumber(vector\u0026lt;int\u0026gt;\u0026amp; nums) { if(nums.size() == 0) return 0; int result = nums[0]; for(int i=1;i\u0026lt;nums.size();i++) result = result ^ nums[i]; return result; } }; 与或 #  与运算可以快速取得一个变量某个 bit位 的数值。\n如： 0010 \u0026amp; 1110 = 0010 0010 \u0026amp; 1101 = 0000\n如果 0010 \u0026amp; b = 0010 则表明 b 的第二位是1，不等则为0。\n 与运算可以快速将某bit为快速置为0，而或运算可以快速将某bit位置为1。\n与运算的其他用法\n n \u0026amp; (n-1) 可以把n的最低位置0  左移右移 #  左移相当于乘2，右移相当于除2，并且它们的速度比乘除要快。\n所以当程序中出现大量✖️2，或➗2运算时，可以用左移和右移进行优化。\n例子 #   LeetCode Bit Manipulation\n"},{"id":23,"href":"/blog/posts/algorithms/%E7%AD%9B%E6%B3%95%E6%B1%82%E7%B4%A0%E6%95%B0/","title":"筛法求素数","section":"Posts","content":" 假设要求n以内的素数\n 筛法求素数是用一个大小为n的数组，作为标记数组，如果没被标记到则为素数。\n开始均为未标记。\n从2开始，2没被标记，将2存入一个存素数的地方，然后筛掉小于n的，2的所有倍数。然后是3，筛掉3的所有倍数，依此类推，直到n-1。\n优化 #  上面的做法，同一个数可能会被筛掉多次，比如6会被3和2各筛一次。\n为了提高效率，需要进行优化，使得每个数尽可能的被少筛，如果能一次最好。\n考虑到任何合数都可以分解成若干个素数的乘积。在筛掉合数的过程中，最好的是让每个合数只被它最小的因子筛掉。\n如24 18 都只被2筛掉\nC++实现 #  int countPrimes(int n) { vector\u0026lt;bool\u0026gt; vis(n,false); vector\u0026lt;int\u0026gt; prime; for(int i=2;i\u0026lt;n;i++){ if(!vis[i]) prime.push_back(i); for(int j=0;j\u0026lt;prime.size() \u0026amp;\u0026amp; i*prime[j]\u0026lt;=n;j++){ vis[i*prime[j]] = true; if(i%prime[j] == 0) break;\t//优化  } } return prime.size(); } 最外层循环每次循环，都能得到小于等于i的所有素数，当要求i+1内的素数时，只需判断i+1是否在之前被筛掉。\n与此同时，将当前所有素数的i+1倍筛掉。\n那后面出现的素数的i+1倍，设为m ，会怎么样呢？\n 如果i+1是素数，m会被 i+1 筛掉 如果i+1不是素数，则m 会被i+1的最小质因数（之前出现过的素数中的某一个）筛掉。  **优化点：**每个数都被它最小的因数筛掉\n具体操作：如果循环到某个素数 prime[j] 是 i+1的倍数时，后面的素数的i+1倍 prime[j+1] * (i+1) 就不用筛了。\n原因在于：后面素数的i+1倍一定会被 prime[j] （更小的一个数）筛掉，\n$prime[j+1] * (i+1) = prime[j+1] * prime[j] * k$\n$k = (i+1) / prime[j]$\n"},{"id":24,"href":"/blog/docs/notes/paper/arbrcan/","title":"ArbSR: 任意尺度超分 - 阅读笔记","section":"Paper","content":"Created: August, 12, 2021 论文: Learning A Single Network for Scale-Arbitrary Super-Resolution\nCode: https://github.com/LongguangWang/ArbSR\n "},{"id":25,"href":"/blog/docs/matrix/jordan/","title":"Jordan Form","section":"Matrix","content":"Created: Decemenber, 7, 2020 Jordan Form是矩阵对角化的一个推广的产物。\n我们知道矩阵$A$可以对角化成一个对角矩阵$\\Lambda$，而且这个矩阵的对角线元素是$A$的特征值，前提是矩阵$A$存在n个线性无关的特征向量。如果不满足，则不能对角化，即找不到一个S，使得：\n$$ \\Lambda = S^{-1}AS $$\nJordan则推广了这个对角化的过程，任何矩阵，即使它不存在n个线性无关的特征向量，它也可以相似“对角化”为一个Jordan标准型$J$。\n$$ J = P^{-1}AP $$\n但这时，J的对角线不在是一个单独的元素，而是一系列Jordan块（请想象一下分块矩阵）。\n$\\lambda$矩阵 #  如果一个矩阵每个元素都是变量$\\lambda$的多项式，则称这个矩阵为$\\lambda$矩阵，记做$A(\\lambda)$。\nSmtih标准形\n每一个$\\lambda$矩阵都可以化成对角形矩阵，这个对角形矩阵称为该$\\lambda$矩阵的Smith标准形。\n对角线上的元素按一定顺序排列：\n 每一个元素都能整除后一个元素 0在最后面（这其实是第一条规则的推论）  不变因子\nSmith标准形对角线上的元素称为$A(\\lambda)$的不变因子。\n行列式因子\nk阶行列式因子就是Smith标准形的前k各元素相乘。k最大只能取到矩阵秩，就是说行列式因子没有0。\n$$ D_k(\\lambda) = d_1(\\lambda)d_2(\\lambda)\u0026hellip;d_k(\\lambda) $$\n初等因子\n不是常数的不变因子的“因数”称为初等因子。\n例如，若$\\lambda$矩阵的不变因子是\n1, 1, $(\\lambda-2)^5(\\lambda-3)^3$, $(\\lambda-2)^5(\\lambda-3)^4(\\lambda+2)$\n则它的初等因子是，仔细看\n$(\\lambda-2)^5$, $(\\lambda-3)^3$, $(\\lambda-2)^5$, $(\\lambda-3)^4$, $(\\lambda+2)$\n"},{"id":26,"href":"/blog/posts/algorithms/kmp/","title":"KMP算法","section":"Posts","content":"问题描述:\n 给定一个文本串S, 和一个模式串P, 我们要找到P在S中的位置，即给出P的第一个字符在S中的位置。\n 朴素算法 #  枚举S中每个位置，判断是否是模式串P的起点。\n基于上述思想，我们可以很容易可以写出如下的枚举代码:\nint vanilla_find(const string\u0026amp; s, const string\u0026amp; p) { int pos = -1; bool ok = true; for(int i=0; i\u0026lt;s.size(); i++) { ok = true; for(int j=0; j\u0026lt;p.size(); j++) { if(s[i+j] != p[j]) { ok = false; break; } } if(ok) { pos = i; break; } } return pos; } 上述实现使用1个循环枚举配对的起点，再用1个循环判断从该起点开始是否有模式串P。\n为了方便理解KMP算法，我们不妨换一种思路实现这个朴素算法。\nint vanilla_find2(const string\u0026amp; s, const string\u0026amp; p) { int i=0, j=0; while(i \u0026lt; s.size() \u0026amp;\u0026amp; j \u0026lt; p.size()) { if(s[i] == p[j]) { i++; j++; } else { i=i-j+1; j=0; } } if(j == p.size()) return i-j; else return -1; } 上述实现使用两个指针, i, j，i指针代表当前S串匹配到的位置，j表示P串匹配到的位置，遇到不匹配，则将i指针回溯到上一次尝试的起始点的后一个位置，即i-j+1。j指针回溯到开头。\n虽然两种实现本质是一样的，但第二种实现会更容易理解KMP算法的优化。\n如图1所示，当我们匹配成功ABCDAB后，发现D不匹配，这时如果我们将i回溯到i-j+1的位置，匹配必然失败，因为我们前一步已经知道S[i-j+1]=B了。\n如果我们能够利用前一步的匹配结果，让i不必回溯，只回溯j的话，那么匹配效率会大幅提升。\n{% include double-image.html url1=\u0026quot;/assets/img/kmp1.png\u0026quot; caption1=\u0026ldquo;Step1\u0026rdquo; url2=\u0026quot;/assets/img/kmp2.png\u0026quot; caption2=\u0026ldquo;Step2\u0026rdquo; size=\u0026ldquo;70%\u0026rdquo; description=\u0026ldquo;图1\u0026rdquo; %}\nKMP算法 #  如果我们把之前的优化\u0008假设形式化一下，我们将得到如下的描述。\n当失配时，我们希望能够让i=i,j=next[j]，即i不用回溯，j根据当前位置决定\u0008模式串要跟S[i]匹配的位置。\n事实上，这个假设是成立的，考虑如图2的情况：\n{% include double-image.html url1=\u0026quot;/assets/img/kmp3.png\u0026quot; caption1=\u0026ldquo;Step1\u0026rdquo; url2=\u0026quot;/assets/img/kmp4.png\u0026quot; caption2=\u0026ldquo;Step2\u0026rdquo; size=\u0026ldquo;70%\u0026rdquo; description=\u0026ldquo;图2\u0026rdquo; %}\n当D失配时，我们知道模式串的D之前有相投的前缀后缀AB，因此我们只需要尝试匹配模式串前缀AB后的D以及文本串的 即可。\n换句话说，我们知道这时候next[j]=2。\n如此，现在的问题就变成如何求next数组了。\n求解next数组 #  让我们再次考虑图2的情况，事实上，我们已经能够发现求解next[j]的规律了。对于P串的第j个元素，其失配后需要回溯到的位置取决于 - P串前j个元素有多长相同的前缀后缀。如果有长度为k的相同前缀后缀，则next[j]=k。\n基于上述思想，我们其实已经可以编写一个暴力求next数组的程序了。\nvector\u0026lt;int\u0026gt; get_next_vanilla(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; for(int i=1; i\u0026lt;p.size(); i++) { int n_match = 0; // 枚举所有\u0008可能的相同前缀后缀  for(int k=1; k\u0026lt;i; k++) { bool ok = true; for(int j=0; j\u0026lt;k; j++) { if(p[j] != p[i-k+j]) ok = false; } if(ok == true) n_match = k; } next[i] = n_match; } return next; } 很显然，这种暴力求解的效率是很低的($$O(n^3)$$)。因此，我们需要对求解next的算法进行优化。\n递推求解优化 #  考虑下面这种情况\nABCDABDC 01234567 如果我们已经求得next[6]=2，即D之前有相同前缀后缀AB，那么我们在求next[7]的时候，只需要判断P[next[6]] == P[7-1] (C==D)是否成立即可。\n 若成立，显然next[7] = next[6] + 1。 若不成立，那么我们需要考虑D要和AB这个前缀中的哪个字母进行配对组成相同前缀后缀。（其实这是一个套娃问题:》）  换句话说，我们在求next[i]的时候我们实际上需要的是，P[:i-1]中最长的相同前缀后缀（可以通过next[i-1]得到），设长度为k, 然后再判断P[l] == P[k]。如果不等于的话，我们就找出P[:k-1]中第二长的相同前缀后缀，用同样方法再判断一次。\n实际上，第二长的相同前缀后缀可以通过next[next[k-1]]得到。\n 为什么？第一长的相同前缀后缀的相同前缀后缀实际上就是第二长的相同前缀后缀。例如 ABABCDABAB, 第一长是 ABAB, 第二长是 AB\n 基于上述思想，我们可以写出如下的递推求解的代码\nvector\u0026lt;int\u0026gt; get_next(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; for(int i=1;i\u0026lt;p.size();i++){ int k=i-1; while(k!=0 \u0026amp;\u0026amp; p[i-1] != p[next[k]]) k=next[k]; // 先找出\u0026#34;OK\u0026#34;的前缀后缀  next[i] = next[k] + 1; } return next; } 继续优化 #  优化1\n我们首先可以重构一下代码, 以下代码和get_next等价。\nvector\u0026lt;int\u0026gt; get_next2(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; int i=0, k=-1; while(i \u0026lt; p.size() - 1) { if(k == -1 || p[i] == p[k]) { i++; k++; next[i] = k; } else { k = next[k]; } } return next; } 优化2\n再者，考虑如下情况\nabab next: -1, 0, 0, 1 当b失配时，next=1，但是P[1]=b，我们没必要再比一次，因为这次一定失配，基于此我们可以再次优化。如下所示：\nvector\u0026lt;int\u0026gt; get_next2_opt(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; int i=0, k=-1; while(i \u0026lt; p.size() - 1) { if(k == -1 || p[i] == p[k]) { i++; k++; // 以下两行为优化内容  if(p[k] != p[i]) next[i] = k; else next[i] = next[k]; } else { k = next[k]; } } return next; } 原来的get_next当然也可以用同样的方法优化，但是要注意写法。 下面这种优化方式就不行。事实上，当你把下面的代码改对之后，你会发现你的代码就变得和get_next2_opt基本一样了。\nvector\u0026lt;int\u0026gt; get_next_opt(const string\u0026amp; p) { vector\u0026lt;int\u0026gt; next(p.size()); next[0] = -1; for(int i=1;i\u0026lt;p.size();i++){ int k=i-1; while(k!=0 \u0026amp;\u0026amp; p[i-1] != p[next[k]]) k=next[k]; // 先找出\u0026#34;OK\u0026#34;的前缀后缀  // 直接这样优化是不行的  if(p[i] != p[next[k]+1]) next[i] = next[k] + 1; else next[i] = next[next[k]+1]; } return next; } 参考资料 #   从头到尾彻底理解KMP: cnblogs "},{"id":27,"href":"/blog/docs/notes/concept/nflow/","title":"Normalizing Flow","section":"Concept","content":"简单介绍 #  首先，Normalizing Flow是一种生成模型（Generative Model），它的提出是为了解决现有的生成的模型的一些问题，如GAN训练不稳定，VAE边缘概率估计intractable的问题。\nNormalizing Flow是一种特殊的inveritable网络，每个模块都是可逆的，并且有tractable determinant of the Jacobian，且求逆也是tractable的。\n优点：\n 相比VAE，不必引入噪声， more powerful local variance models. 相比GAN，训练更稳定，更易收敛。  缺点\n 每个模块都必须可逆，限制了表达能力。 可逆要求latent space的维度很高。  详细介绍 #  参考资料 #   Github: normalizing-flows  Introduction to Normalizing Flows  Stanford CS236: Normalizing flow models  Difference between invertible NN and flow-based NN  "},{"id":28,"href":"/blog/docs/notes/paper/patchmatch/","title":"PatchMatch 导读","section":"Paper","content":"Created: Jan, 24, 2022 PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing\n Sample Impl ｜ 知乎解读\n图像匹配的典中典，基于匹配块周围的匹配也近似匹配的先验知识的随机化算法。\n 首先明确这篇论文解决的问题是图像匹配问题，即给定两张图像A，B，我们需要将A，B中相同的像素或图像块（Patch）进行匹配，得到一个偏移量f，使得对于A中每个点的坐标x，B中对应点的坐标为x+f(x)。\n这篇论文提出了一个Randomized Correspondence Algorithm来做这件事情。要理解这个算法，我们首先需要这个算法的核心insight：\nThe key insights driving the algorithm are thatsome good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas.  用中文来说就是，对于一个匹配好的点，我们知道其偏移量为f(x)，基于图像的连续性，那么这个点周围的点的偏移量应该也近似是f(x)，这样我们就可以快速把正确点的偏移量向其周围传播，使其周围点的估计偏移量越来越准确。\n图像的连续性：一个苹果在两张图像上放在不同位置，但是苹果上某两个点的相对位置关系近似不变。  算法 #  前提条件 假设我们有两张图像A，B，我们需要找到一个偏移量f，使得对于A中每个点的坐标x，B中对应点的坐标为x+f(x)。\nPatchMatch的算法主要分为三个步骤：\n Initialization： 随机初始化匹配关系，即A中每个点的偏移量。 Propagation： 根据匹配程度，将每个点的偏移量传播到周围的点。实际实现是，对于某个点(x,y)，使用其周围点的偏移量还是自己的偏移量得到匹配点，计算匹配程度，取匹配程度最好的偏移量作为当前点的偏移量。 Random search： 为了进一步优化匹配结果，我们在前一步得到的偏移量基础上，我们再随机的做一些变化，检查匹配点周围是否有更好的匹配点。   解读：\n 随机初始化在点很多的时候，我们有很大几率会得到不少还不错的匹配，这些正确匹配是后续Propagation的基础。 不加random search性能如何？TODO  应用 #   Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution PatchmatchNet: Learned Multi-View Patchmatch Stereo  "},{"id":29,"href":"/blog/docs/notes/paper/raft/","title":"Raft 阅读笔记","section":"Paper","content":"Created: August, 6, 2021 论文: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow\nCode: https://github.com/princeton-vl/RAFT\n 这篇文章做的是光流估计，简单来说，任务就是输入两张图像$I_1,I_2$，我们需要估计出第二张图像$I_1$每个像素点$(u,v)$相对于第一张图像$I_1$的偏移量$(f^1(u), f^2(v))$。\n$$ I_1 = I_2 + f $$\n这篇文章的思路是这样的：\n 首先使用一个feature encoder  求出flow feature：同时输入img1，img2，得到fmap1，fmap2 求出context feature：只输入img1，得到inp   求出fmap1和fmap2的相似度corr 使用一个RNN迭代的求出flow。  RNN的输入包括一个隐藏状态net，context feature inp，相似度corr，上一步的flow     这里的主要问题是每迭代一次，我们都得到了一个更接近img1的img2，这时候相似度需要重新计算。\n RAFT设计了一个查表的思路，只需要在最开始算一遍即可。它的方法是用新坐标周围的点的flow feature拉成一个向量作为新坐标点的flow特征。\n"},{"id":30,"href":"/blog/docs/matrix/types/","title":"各种矩阵","section":"Matrix","content":"Created: Decemenber, 1, 2020 对称矩阵 #    定义：满足$A^T=A$的矩阵。 显然，对称矩阵是方阵，否则转一下形状都变了。  对称矩阵的特征值/向量有着很好的性质：\n 实对称矩阵的特征值都是实数。 对称矩阵的特征向量是正交的。没验证，不过应该是不同特征值对应的特征向量一定是正交的，同一个特征值对应的特征向量可能需要Schmidt正交化。  证明 1. 实对称矩阵的特征值都是实数\n已知$Ax = \\lambda x$，左右两边同时取共轭，有\n$$ \\bar{A}\\bar{x}=\\bar{\\lambda}\\bar{x} $$\n因为A是实对称矩阵，$\\bar{A}=A$，因此有 $$ A\\bar{x}=\\bar{\\lambda}\\bar{x} $$\n左右两边同时取转置，有\n$$ \\bar{x}^TA=\\bar{x}^T\\bar{\\lambda} $$\n左右两边同时右乘x，得 $$ \\bar{x}^TAx=\\bar{x}^T\\bar{\\lambda}x $$\n对$Ax = \\lambda x$，左右两边同时左乘$\\bar{x}^T$，得 $$ \\bar{x}^TAx=\\bar{x}^T\\lambda x $$\n比较两个式子，可以得到\n$$ \\bar{\\lambda} = \\lambda $$\n一个数的共轭等于其本身，这个数是实数。\n2. 对称矩阵的特征向量是正交的\n  正定矩阵 #   正定矩阵是对称矩阵的一种特殊情况，因此它也是对称的。\n定义 #  正定矩阵有各种各样的定义，它们互相之间是等价的。下面是几种常见定义/判定方法：\n 所有特征值大于0。 所有顺序主子式都大于0。 所有Pivots都大于0。 $x^TAx\u0026gt;0$ ： $x=[x_1,x_2, \u0026hellip;, x_n]$, 对于所有的非零x恒大于0  类似的可以定义半正定(大于等于0)，负定（小于0），半负定（小于等于0）。\n上述四个定义相关性的不严格证明 考虑第四个定义，我们知道正定矩阵就是二次型的系数矩阵，当二次型要恒大于0，则它必定可以化成一系列平方的和，且每一项的系数都大于0（可以等于0吗？不行，平方项是可以等于0的，如果有一项系数是0，就多了一个自由变量，就可以构造出等于0的x），而化为平方和形式的过程，等价于对A做elimination，平方项的系数就是elimination之后的pivots，里面的系数是elimination过程的L矩阵。\n例：\n  特殊情况 #   $A^TA$必定是半正定矩阵，如果A(m*n矩阵)的秩为n，则必定是正定矩阵。  证明 直观的，这个相当于矩阵形式的平方。我们知道标量，$a^2\u0026gt;0$，对应的，我们可以想象$A^TA$是应该是正定的。\n回忆正定矩阵的四种定义：特征值不知道，pivots不知道，顺序主子式不知道，因此考虑最后一种形式。\n严格证明：\n考虑矩阵$A\\in R^{m\\times n}$\n$$ x^TA^TAx = (Ax)^TAx = ||Ax||^2 \\geq 0 $$\n上式，只有在Ax=0的时候才取0，如果我们希望$A^TA$是正定的，这意味着Ax=0不能有解，而无解的条件是A的列向量线性无关，即A的秩为n。\n  正交矩阵 #   正交矩阵（英语：orthogonal1 matrix）是一个方块矩阵Q，其元素为实数，而且行向量与列向量皆为正交的单位向量。\n 相同行/列向量内积为1 不同行/列向量内积为0  显然，每个行/列向量都不能由其他行/列向量线性表出，因此正交矩阵必定是满秩矩阵。\n行向量正交能否推出列向量正交? 可以，推导如下：\n行向量正交，因此有$AA^T = I$, 对于方阵，有$AB = I \\simeq BA=I$ ( 证明)， 则有$A^TA=I$，则有列向量正交。\n 重要性质 #   正交矩阵的转置矩阵等于其逆矩阵: $A^T = A^{-1}$ 行列式值必定为+1或-1 $$ 1=\\operatorname{det}(I)=\\operatorname{det}\\left(Q^{T} Q\\right)=\\operatorname{det}\\left(Q^{T}\\right) \\operatorname{det}(Q)=(\\operatorname{det}(Q))^{2} \\Rightarrow \\operatorname{det}(Q)=\\pm 1 $$    虽然更完整的说法是orthonormal matrix(标准正交矩阵)，但我们说正交矩阵的时候一般都是指标准正交。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":31,"href":"/blog/docs/matrix/svd/","title":"奇异值分解","section":"Matrix","content":"Created: Decemenber, 1, 2020 所有矩阵都可以分解成：正交矩阵 * 对角矩阵 * 正交矩阵，这个分解叫奇异值分解。\n$$ A = U\\Sigma V^T $$\n其中A是一个m*n的矩阵，V是一个n*n的矩阵，U是一个m*m的矩阵，$\\Sigma$是一个m*n的矩阵。\n在一些特殊情况下，如A是对称矩阵，它的特征向量是正交的，奇异值分解退化为矩阵对角化，设Q是是A的特征向量（列向量）构成的矩阵。则有\n$$ A = U\\Sigma V^T = Q \\Lambda Q^T $$\nIntuition - 几何含义 #   当我们在做奇异值分解的时候，我们实际上是在做什么？\n 在MIT Linear Algebra 第29讲，Strang教授给了我们一个很生动的解释。  详细解释 Revision needed\n考虑两个线性空间$R^n$和$R^m$, 其中$V=[v_1,v_2,\u0026hellip;,v_n]$是$R^n$的一组标准正交基，我们希望对这个基底做一个线性变换得到$R^m$空间的一组基底，这个基底也是正交的。\n这个新基底是$R^m$空间下某个标准正交基乘上某个系数矩阵，用矩阵的语言来说，我们实际有这个等式：\n$$ AV = U\\Sigma $$\n其中A表示线性变换，V是$R^n$的一组标准正交基，U是$R^m$的一组标准正交基，$\\Sigma$是系数对角阵。\n换句话说，对每一个m*n的矩阵，它其实都是其行空间$R^n$和列空间$R^m$某两个正交基底的变换矩阵，奇异值分解其实就要找到这两个基底。\n  求解 #   简单来说：\n V是$A^TA$的特征向量构成的矩阵。 U是$AA^T$的特征向量构成的矩阵。 $\\Sigma$是$A^TA$和$AA^T$的特征值（二者特征值一致）。  详细推导 直接求解下式，并不好求，因为它同时包含很多个未知量。\n$$ A = U\\Sigma V^T $$\n我们知道$A^TA$具有很好的性质，考虑\n$$ A^TA = V\\Sigma^T U^TU\\Sigma V^T = V\\Sigma^T\\Sigma V^T $$\n我们知道$A^TA$是对称矩阵，而对称矩阵的特征向量是正交的，而我们正好要求V是一个正交矩阵！因此，我们知道$A^TA$的特征向量构成的矩阵就是（满足要求，但唯一性需要证明）我们要求的V。$\\Sigma^T\\Sigma$则是$A^TA$的特征值。\n类似的，考虑$AA^T$，我们有\n$$ AA^T = U\\Sigma^T V^TV\\Sigma U^T = U\\Sigma^T\\Sigma U^T $$\n即U是$AA^T$的特征向量构成的矩阵，$\\Sigma^T\\Sigma$则是$AA^T$的特征值。\n我们这时候其实得到了一个附属结论，那就是$AA^T$和$A^TA$的特征值是相同的。     对于复矩阵（酉矩阵），只需要将转置T改成共轭转置H即可。  需要注意的是，并不是任取n个$A^TA$的两两正交单位长的特征向量都可以作为V的列向量。$V_1$和$U_1$必须要满足下面的关系才行:\n$$ V_1 = A^HU_1\\Delta^{-1} $$\n其中$U_1$, $V_1$是非零特征值对应的特征向量组。\n"},{"id":32,"href":"/blog/docs/matrix/road/","title":"如何将各个知识点串起来？","section":"Matrix","content":"Created: Decemenber, 3, 2020 引入 特征值与特征向量之后，我们可以很容易的推导出 矩阵对角化的公式。\n矩阵对角化是针对一般方阵的，对于特殊方阵，如 对称矩阵，矩阵对角化可以进一步特化。\n对称矩阵有很多很好的性质，它是它的进一步特例， 正定矩阵有着更好的性质。\n#  "},{"id":33,"href":"/blog/docs/matrix/misc/","title":"杂七杂八","section":"Matrix","content":"Created: Decemenber, 1, 2020 Quick Look #   酉矩阵: $A^HA=E$ 正规矩阵: $A^HA=AA^H$, 包括对称矩阵，反对称矩阵，Hermite矩阵，反Hermite矩阵，正交矩阵，酉矩阵等。 酉对角化： $A=Q\\Lambda Q^H$, 普通对角化: $A=Q\\Lambda Q^{-1}$ 单纯矩阵：可以对角化的矩阵，即有n个线性无关的特征向量。  线性代数中常见的等价关系 #    singular matrix = 奇异矩阵 = 不可逆矩阵\n  A不可逆 \u0026lt;-\u0026gt;1 Ax = 0 有非零解 \u0026lt;-\u0026gt;2 特征值=0\n  Spectrum #  矩阵中有谱（spectrum）这个概念，例如谱分解，谱范数等等。\n这个谱其实说的就是特征值，特征向量，它是从光学里借过来的说法。光学中光是由各种pure light构成的，光谱就是各个成分的占比。而在矩阵中，矩阵由各个pure的component，特征值特征向量构成。\n进一步来说，我们看矩阵对角化，我们知道任何一个对称矩阵A都可以对角化成$Q\\Lambda Q^{T}$，其中Q为特征向量组成的矩阵。\n我们进一步把这个式子拆开，我们设A的特征向量为\u0008$q_i$, 则\n$$ A = Q\\Lambda Q^{T} = \\lambda_1 q_1 q_1^T + \\lambda_2 q_2 q_2^T + \u0026hellip; + \\lambda_n q_n q_n^T $$\n$q_i$是一个列向量，$q_1 q_1^T$是一个矩阵，因此A实际上就化成了n个矩阵的线性组合，第i个矩阵是$q_i q_i^T$，系数是特征值$\\lambda_i$。\n  A可逆的话，意味着简化阶梯形最后一行不全为0，为了满足Ax=0，x必须等于0。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n TODO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":34,"href":"/blog/docs/matrix/eigen/","title":"特征值\u0026\u0026特征向量","section":"Matrix","content":"Created: Decemenber, 1, 2020 首先明确一点，特征值和特征向量是针对方阵来说的，我们说某个矩阵A有什么特征值和特征向量时，我们实际上暗示了矩阵A是一个方阵。\n定义 #  任何满足下列条件的标量和向量分别为矩阵A的特征值和特征向量。1\n$$ Ax = \\lambda x $$\nIntuition #  将一个矩阵A与一个向量x相乘，我们实际上在对向量x做一个 线性变换，特征向量实际上就是经过这个变换与原向量仍然平行的那些向量，特征值就是前后两个向量长度的一个比值。\n 显然，零向量满足条件，那它是任何矩阵的特征向量吗？\n不是，我们显示将特征向量定义为非零向量。\n 求解 #  根据定义，有\n$$ (A-\\lambda I)x = 0 $$\n这个等式要有非零解，$(A-\\lambda I)$必须是singular（不可逆）的，特征值要等于0：\n$$ Det(A-\\lambda I) = 0 $$\n于是：\n 求行列式并解方程即可求出特征值。 对于特征向量，我们回代特征值，解$(A-\\lambda I)x=0$这个方程。  因为$(A-\\lambda I)$不可逆，所以解有无穷多个，即特征向量有无穷多个，这时候我们只需要给出一个就行了。    性质 #   $\\sum \\lambda = trace(A)$ : 矩阵A的所有特征值之和等于矩阵A的 trace（对角线元素之和）。 $\\prod \\lambda = det(A)$ : 矩阵A的所有特征值的乘积等于矩阵A的行列式。 三角矩阵的特征值就是对角线上的元素。 对称矩阵的特征向量是正交的。  证明：对称矩阵的特征向量是正交的 To be done     由特征向量的定义，显然A必须要个方阵，若不是，A与x相乘，x维数就变了。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":35,"href":"/blog/docs/matrix/similar/","title":"相似矩阵","section":"Matrix","content":"Created: Decemenber, 7, 2020 满足以下条件的两个矩阵，我们称它们是相似的。\n$$ B = M^{-1}AM $$\n换句话说，如果某个矩阵可以由另一个矩阵左乘一个矩阵再右乘这个矩阵的逆，则称它们是相似的。\nIntuition #  相似的矩阵具有很多相似的属性。\n我们可以把每一个矩阵和其相似的矩阵（们）都想成一个family，这个family里可能有一些矩阵很特殊，通过研究这些特殊的矩阵（通常更为容易），我们可以反推出原矩阵的一些属性。\n 矩阵对角化$\\Lambda = S^{-1}AS$的对角矩阵就是一种特殊的相似矩阵。   性质 #   相似矩阵的特征值是相同的（特征向量可能不同1,但相互独立的数量是相同的）  证明 证明要分两个方向，A的特征值是B的特征值，B的特征值也是A的，由这两者即可得到二者特征值相等。\n对于原矩阵A, 其特征值满足：\n$$ Ax = \\lambda x $$\n对于B：\n$$ (M^{-1}AM)M^{-1}x = M^{-1} \\lambda x = \\lambda M^{-1} x $$ $$ BM^{-1}x = \\lambda M^{-1} x $$\n于是，我们知道A的特征值也是B的特征值，且\u0008B的特征向量是A的特征向量乘以$M^{-1}$。\n反向很容易，把M乘到B那边得到$MBM^{-1} = A$，用类似的方法即可证明。\n     还是一定不同？TODO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  "},{"id":36,"href":"/blog/docs/matrix/diagonalization/","title":"矩阵对角化","section":"Matrix","content":"Created: Decemenber, 1, 2020  To be continue.  Keypoint: 一些矩阵可以对角化 $S^{-1}AS = \\Lambda$。其中S为特征向量(列向量)构成的矩阵，$\\Lambda$为特征值构成的对角矩阵。\n 什么样的矩阵可以？ 因为定义里S要可逆，所以A必须拥有n个线性无关的特征向量。   关于矩阵对角化，我们其实可以从两个角度理解：\n 一些矩阵可以对角化$S^{-1}AS = \\Lambda$。 或一些矩阵可以化成三个矩阵的乘积：$A = S\\Lambda S^{-1}$  推导 #   $$ \\begin{equation} \\begin{aligned} AS \u0026= A[x_1, x_2, ..., x_n] = [Ax_1, Ax_2, ..., Ax_n] = [\\lambda_1 x_1, \\lambda_1 x_2, ..., \\lambda_1 x_n] \\\\\\\\\\\\ \u0026= [x_1, x_2, ..., x_n] \\begin{bmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 \\lambda_n\\\\ \\end{bmatrix} = S\\Lambda \\end{aligned} \\end{equation} $$  左右两边同时乘S的逆，即可得到：\n$$ S^{-1}AS = \\Lambda $$\n特殊情况 #  当矩阵A是对称矩阵时，其特征向量是正交的1。因此，对角化可以写成\n$$ Q^{T}AQ = \\Lambda $$\n   证明\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":37,"href":"/blog/docs/matrix/space/","title":"线性空间和线性变换","section":"Matrix","content":"Created: Decemenber, 9, 2020 抽象来说，线性空间就是里面元素满足一定运算规律的一个空间。\n具体来说，空间的概念在数学上就是一个集合，空间里的元素就是集合里的元素，线性空间就是里面元素满足一定运算规律的一个集合。\n线性空间 #    符号说明：实数域R，复数域C，统称数域F。\n 线性空间是数域F上满足加法和数乘运算规律的一个集合。具体定义如下：\n TODO  核(零)空间 #   核空间，或者说零空间是针对一个矩阵来说的。对一个矩阵A，它的核空间就是Ax=0的解空间。\n常用 N(A) 来表示，N其实就是英文里Null Space的首字母。\n列空间/值域 #   同样是针对一个矩阵来说的，具体不解释。常用 R(A) 表示。\n向量 #   我们把线性空间里的元素称为向量。\n注意，这里的向量比线性代数里的n元向量含义更广。  概念：\n 线性组合/线性表示 线性相关/无关  基 #   一个线性空间的基，是可以线性表出这个线性空间中其它所有向量的n个线性无关的向量。\n$$ \\alpha = k_1a_1 + k_2a_2 + \u0026hellip; + k_na_n $$\n其中系数称为向量$\\alpha$在这个基下的坐标。\n基变换 #   不同基之间可以进行变换，其中P称为过渡矩阵。\n$$ B = AP $$\n$$ [b_1,b_2,\u0026hellip;,b_n] = [a_1,a_2,\u0026hellip;,a_n]P $$\n坐标变换 #   坐标变换是左乘$P^{-1}$。推导如下\n$$ B\\beta^T = A\\alpha^T $$\n$$ AP\\beta^T = A\\alpha^T $$\n$$ \\beta^T = P^{-1}\\alpha^T $$\n线性子空间 #   线性子空间是线性空间中仍满足线性空间要求的一个子空间。\n特殊的两个子空间：也称这两个子空间为平凡子空间\n 线性空间本身 零空间  生成子空间：写做 span{$a_1, a_2, \u0026hellip;, a_n$}\n交空间：两个子空间的交集，同时存在于两个子空间。\n $$ V_{1} \\cap V_{2}=\\left\\{\\boldsymbol{\\alpha} \\mid \\boldsymbol{\\alpha} \\in V_{1} \\text { 且 } \\boldsymbol{\\alpha} \\in V_{2}\\right\\} $$  和空间：这个空间的向量是两个空间向量的和。这两个子空间有点像是和空间的基底。\n $$ V_{1}+V_{2}=\\left\\{\\boldsymbol{\\alpha}=\\boldsymbol{\\alpha}_{1}+\\boldsymbol{\\alpha}_{2} \\mid \\boldsymbol{\\alpha}_{1} \\in V_{1} \\text { 且 } \\boldsymbol{\\alpha}_{2} \\in V_{2}\\right\\} $$  维数公式：\n$$ \\operatorname{dim} V_{1}+\\operatorname{dim} V_{2}=\\operatorname{dim}\\left(V_{1}+V_{2}\\right)+\\operatorname{dim}\\left(V_{1} \\cap V_{2}\\right) $$\n直和空间：若V1，V2两个子空间没有交集，则它们的和空间称为V1，V2的直和空间。\n补子空间：直和空间为全集的两个子空间互为自己代数补。\n线性映射 #    定义：满足加法和数乘的两个线性空间的映射关系称为线性映射。\n 线性映射前的空间叫原像，后的叫“像”。\n证明线性映射，只需要证明下面两个式子：\n $$ \\begin{array}{l} \\mathcal{A} \\left( \\alpha _{1}+ \\alpha _{2}\\right)= \\mathcal{A} \\left( \\alpha _{1}\\right)+ \\mathcal{A} \\left( \\alpha _{2}\\right) \\\\ \\mathcal{A} \\left(\\lambda \\alpha _{1}\\right)=\\lambda \\mathcal{A} \\left( \\alpha _{1}\\right) \\end{array} $$  性质 #   零元素经过线性映射还是零元素。 一组元素经过线性映射，如果原来线性相关，则之后也线性相关。但是，如果原来线性无关，则之后不一定线性无关。  矩阵表示 #  线性映射可以用矩阵表示，但这之前我们需要将两个线性空间的元素用各自的基底进行表示。\n选取不同的基底，同一个线性映射的矩阵表示就不同。\n注意区分两个概念：\n 基坐标变换：$\\alpha=\\beta A$ 向量坐标变换：$y=Ax$  给定基的情况下给个矩阵都表示一个线性映射。\n不同基底下矩阵表示之间的关系：\n设有两个空间$X,Y$, 两组基底$x_1,y_1$和$x_2,y_2$，\n $Q$是$y_1$到$y_2$的过渡矩阵， $P$是$x_1$到$x_2$之间的过渡矩阵，  则\u0008在$x_1,y_1$下的$A$与在$x_2,y_2$的$B$有如下关系。\n$$ B = Q^{-1}AP $$\n值域\u0026amp;\u0026amp;核 #  值域\n核子空间：线性映射的“零空间“，映射之后为0的元素构成的空间。\n对于一个从n维线性空间v1，映射到m维线性空间v2的线性映射。\n 核子空间核值域的维数之和为n  线性变换 #    同一个空间的线性映射称之为线性变换。\n 矩阵相似, 存在矩阵P，满足：\n$$ B=P^{-1}AP $$\n则称A，B相似。\n两个矩阵相似，意味着它们是不同基底下同一个线性变换的不同矩阵表示。  线性变换本身也可以定义运算, 这些运算和其矩阵表示有着对应关系:\n 乘法: AB 加法: A+B 数乘: kA  特征值\u0026amp;\u0026amp;特征向量 #  矩阵特征值的推广，求还是用线性变换的矩阵表示来求。\n 特征子空间：一个特征值对应的特征向量和零向量构成的一个子空间，称为这个特征值的特征子空间。 代数重复度：每个不同特征值的重复数目称为这个特征值的代数重复度。 几何重复度：一个特征值特征子空间的维数称为这个特征值的几何重复度。  相似对角化 #   n阶矩阵A可对角化的充要条件是A有n个线性无关的特征向量。  注意，并不是什么线性变换存在一个基，使得在这个基下的矩阵表示，呈现对角形。 "},{"id":38,"href":"/blog/posts/algorithms/%E8%B4%AA%E5%BF%83%E7%94%9F%E6%88%90%E6%9C%80%E4%BC%98%E7%BC%96%E7%A0%81%E7%9A%84%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/","title":"贪心生成最优编码的思路分析","section":"Posts","content":"贪心生成最优编码的思路分析 #  目标：求字符编码\n首先得先想到用二叉树表示编码，节点即为字符，边为编码。\n然后优化目标（目标函数）即为： f(x) = w(x)*l(x)\n w(x) 为 字符x的频率 l(x) 为 字符编码的长度  决策目标：巧妙安排二叉树的结构，使得目标函数值最小。\n最优编码树性质一：对于最优编码树，频率最小的两个节点深度最大，且为兄弟。\n证明：\n  深度最大显然，否则深度最大的频率更大，会使得目标函数值增大。\n  若最小的节点有兄弟，且其兄弟不是次小的，则可以让次小的节点与该节点交换，使得目标函数更小。\n  若最小的节点没有兄弟，则可以将该节点与其父亲交换，去掉“父亲”，可以使得目标函数更小。\n  有了这个性质，我们便可以先将两个频率最小的点连起来作兄弟。但为了进一步构造最优编码树，还需要第二个原则。\n最优编码树性质二： 对于最优编码树，删掉频率最小的节点（两个叶子节点），则其父亲变成了叶子结点，另其频率为两孩子频率之和，则新的编码树仍然是最优编码树。\n证明：f1为删之前，f2为删之后目标函数值，最小两个节点为x1,x2, 则有\nf1 = f2 + x1 + x2。\n已知f1最小，则f2也是最小的，否则调整新的编码树使其变为最优编码树，则f1更小，与已知f1最小矛盾。故新编码树也为最优编码树。\n有了这第二个性质，我们已知新的编码树为最优编码树，则根据性质一， 新编码树频率最小的节点为兄弟。递归这个过程，直到所有节点都纳入编码树（叶子节点）。\n例: 将两个频率最小的点连起来作兄弟，然后用这两个点的父亲代替这两个点，求剩余点的最优编码树。\n"}]