[{"id":0,"href":"/blog/docs/notes/concept/","title":"Concept","section":"Notes","content":"Concepts #  "},{"id":1,"href":"/blog/docs/notes/","title":"Notes","section":"Docs","content":"Notes #  这里是一些以PDF格式存放的一些笔记。\n 再看变分自动编码器VAE: [ Note], [ Slide] 推导高斯混合模型的EM算法：[ Note] 从最小化自由能的角度推导高斯混合模型的EM算法：[ Note] ADMM Tutorial: [ Note] ADMM各个任务推导: [ Note]  "},{"id":2,"href":"/blog/docs/notes/paper/","title":"Paper","section":"Notes","content":"Paper Notes #  一些论文阅读笔记\n [ PDF] Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems [ PDF] Effective Snapshot Compressive-spectral Imaging via Deep Denoising and Total Varia-tion Priors [ Draft] RAFT: Recurrent All-Pairs Field Transforms for Optical Flow  "},{"id":3,"href":"/blog/docs/matrix/","title":"Matrix","section":"Docs","content":"Matrix #  正在努力编写中✍️  矩阵分析速查表: [ PDF]\n  如何将各个知识点串起来？  杂七杂八: 一些无法分类的小知识  线性空间  各种矩阵\n  对称矩阵  正定矩阵  正交矩阵  相似矩阵 : ⚠️ 相似矩阵是指一系列互相相似的矩阵。  特征值/特征向量\n  基础知识  矩阵对角化 : 利用特征值，特征向量进行。  Jordan Form  矩阵分解\n  特征值分解(矩阵对角化) : 对角化也算是一种分解。  奇异值分解 : 特征值分解(eigen decomposition)的扩展  谱分解 : 特征值分解的另一种表达形式。  "},{"id":4,"href":"/blog/posts/machine-learning/monte_carlo/","title":"蒙特卡洛法","section":"Posts","content":"蒙特卡洛(MonteCarlo)是一大类随机算法(RandomizedAlgorithms)的总称，它们通过随机样本来估算真实值。\n估计圆周率 #  假设有一个半径为1，圆心在原点的圆，我们知道它的面积是$\\pi$。现在我们随机在$x\\in[-1,1], y\\in[-1,1]$上随机取一个点，我们知道这个点落在圆内的概率为圆与正方形面积之比：\n$$ p = \\frac{\\pi}{4} $$\n假设我们随机选取了n个点，我们可以使用下面的公式判断哪些点位于圆内，假设有m个。\n$$ x^2 + y^2 \u0026lt; 1 $$\n显然，我们知道m的数学期望是\n$$ E[M] = \\frac{\\pi n}{4} $$\n当n很大时, m近似于数学期望，因此有\n $$ \\begin{aligned} m \\approx \\frac{\\pi n}{4} \\\\ \\pi \\approx \\frac{4m}{n} \\end{aligned} $$  近似求定积分 #  蒙特卡洛求积分的本质是利用随机模拟估计一个随机变量的期望。\n假设我们想求定积分:\n$$ \\int_{a}^{b} f(x) \\mathrm{d} x $$\n我们可以把它转换成某个随机变量的数学期望，具体如下：\n设随机变量$X \\sim U[a, b]$，即服从均匀分布，X具有概率密度$p(x)=\\frac{1}{b-a}$，那么就有:\n$$ E[f(X)] =\\int_{a}^{b} p(x)f(x) \\mathrm{d} x = \\frac{1}{b-a}\\int_{a}^{b} f(x) \\mathrm{d} x $$\n在统计学中，我们知道期望是可以估计的，我们随机取N个$f(x)$的样本，这些样本的平均值可以作为所求期望的近似，即：\n$$ E[f(x)] = \\frac{\\sum_1^Nf(x_i)}{N} = \\frac{1}{b-a}\\int_{a}^{b} f(x) \\mathrm{d} x $$\n那么就有：\n$$ \\int_{a}^{b} f(x) \\mathrm{d} x = (b-a)\\frac{\\sum_1^Nf(x_i)}{N} $$\n参考资料 #   https://blog.csdn.net/weixin_41503009/article/details/107853383 https://github.com/wangshusen/DRL "},{"id":5,"href":"/blog/posts/deep-learning/vae-more/","title":"More about Variational Autoencoder","section":"Posts","content":"一些关于VAE的扩展知识。\n不断更新中\u0026hellip;\nTraining Tips #   spectral regularization1 有利于稳定VAE的训练2 训练的时候可以使用KL cost annealing3的训练策略，KL项的权重一开始为0，只关注重建，等重建能力差不多了，再逐渐增加KL项的权重到1。 限制KL项中logvar.exp()，防止其值过大。4  强制clip weight初始化要小, 或使用$log(\\sigma^2)$5    Related Models \u0026amp;\u0026amp; Papers #  Beta-VAE #   beta-vae6就是给KLD加了一个权重beta。\n $$ \\mathcal{F}(\\theta, \\phi, \\beta ; \\mathbf{x}, \\mathbf{z}) \\geq \\mathcal{L}(\\theta, \\phi ; \\mathbf{x}, \\mathbf{z}, \\beta)=\\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\beta D_{K L}\\left(q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right) $$  原文还设计了一个评测指标用于评测latent representation disentangle好不好的方法，简单来说是这样的：\n 前提是：假设数据x是有一系列的disentangle factor y得到的，而且我们有一个ground truth simulator可以根据这些factor合成出数据x。\n 具体方法：\n 随机选一个factor y。 (Choose a factor y ∼ Unif[1\u0026hellip;K]) 对一个batch里的L个样本（一个batch中每个sample的factor都一样）：  sample 两个latent representation（人工设计的），这两个样本，在刚刚选的那个factor上相同，其他随机。 用simulator合成两张图像，用之前训练好的vae-encoder预测一个latent representaiton（网络学到的） 计算两张图像的latent representation的差值。   使用L个sample差值的平均数作为一个线性分类器的输入，预测刚刚选的factor是哪个。用预测的准确性作为评测指标。  Intuition：\n 分类器会被强行设计的只有线性分类能力。 如果vae学习到的latent representation够好，那理论上它应该有一维就是代表factor y， 在这个维度，每个样本的两张图像的值应该是很接近的，也就是说它们的差值会很接近0，那么分类器只需要找到很接近0的那一项就可以预测出刚刚选的factor是哪个了。  Extended beta VAE #   在Understanding disentangling in β -VAE7中，作者进一步扩展了beta VAE， 方法如下：\n $$ \\mathcal{L}(\\theta, \\phi ; \\mathbf{x}, \\mathbf{z}, C)=\\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\gamma\\left|D_{K L}\\left(q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p(\\mathbf{z})\\right)-C\\right| $$  其中C是一个逐渐增大的常数。\nIntuition是，当C逐渐增大时，KLD的作用逐渐变小，重构误差逐渐占主导地位，因此我们可以学习到更好的重构图像。这点与KLD annealing正好是反过来的。\nSpectral Regularization #   简单来说：spectral regularization1用于降低 sensitivity to perturbation（测试数据微小的变动会引起结果很大的变化），方法是使用一个正则化，约束网络权重矩阵的谱范数，不让其过大。\n 原文：To reduce the sensitivity to perturbation, we propose a simple and effective regularization method, referred to as spectral norm regularization, which penalizes the high spectral norm of weight matrices in neural networks.\n 设$f$是我们要学习的函数，我们的目标是要让perturbation，即 $f(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})$ 尽量小。通常$f$ 都是一个线性函数加一个非线性激活函数。考虑到深度学习常用ReLU等piecewise linear function[?]作为激活函数。当$\\boldsymbol{\\xi}$很小的，我们可以把$f$看出一个线性函数。因此我们有： $$ \\frac{\\left|f_{\\Theta}(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})\\right|_{2}}{|\\boldsymbol{\\xi}|_{2}}=\\frac{\\left|\\left(W_{\\Theta, \\boldsymbol{x}}(\\boldsymbol{x}+\\boldsymbol{\\xi})+\\boldsymbol{b}_{\\Theta, \\boldsymbol{x}}\\right)-\\left(W_{\\Theta, \\boldsymbol{x}} \\boldsymbol{x}+\\boldsymbol{b}_{\\Theta, \\boldsymbol{x}}\\right)\\right|_{2}}{|\\boldsymbol{\\xi}|_{2}}=\\frac{\\left|W_{\\Theta, \\boldsymbol{x}} \\boldsymbol{\\xi}\\right|_{2}}{|\\boldsymbol{\\xi}|_{2}} \\leq \\sigma\\left(W_{\\Theta, \\boldsymbol{x}}\\right), $$ 其中$\\boldsymbol{\\xi}$是个常数，因此，如果我们要让$f(\\boldsymbol{x}+\\boldsymbol{\\xi})-f(\\boldsymbol{x})$ 尽量小，我们应该让上界$\\sigma\\left(W_{\\Theta, \\boldsymbol{x}}\\right)$尽量小。\n即：我们应当约束网络权重矩阵的谱范数，不让其过大。\nLadder Variational Autoencoders #   Ladder Variational Autoencoders8\n 提出了一个类似Ladder Network的Ladder Variational Autoencoders。  具体结构：TODO\n指出Batch Normal和KL Warm Up对训练很重要。  Re-balancing Variational Autoencoder Loss #   这篇文章9分析了RNN-based VAE中posterior collapse问题出现的原因，并提出了一个loss减轻这个问题。\nposterior collapse：任务是在RNN-VAE中，因为decoder训练的时候有ground truth作为teaching force，因此当latent representation很差的时候，decoder会忽视掉latent representation，导致latent representation和先验很接近，KLD这项很小，虽然loss可能不高，reconstruct因为有teaching force，loss不会太高，但是总体效果是很差的。\n分析原因，就是reconstruct loss被低估了，因此需要用系数调整，思路和beta-vae是一样的。\n $$ \\begin{aligned} \\mathcal{L}(x ; \\theta, \\phi)=\u0026 \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\\\ \u0026-D_{\\mathrm{KL}}\\left(q_{\\phi}(z \\mid x) \\| p(z)\\right), \\alpha1 \\end{aligned} $$  也可以改成beta-vae的形式，不过这里beta是在0-1区间，而beta-vae是\u0026gt;1，因为目标不一样。这里是向增大reconstruction的占比 - 等价于缩小KLD。\n $$ \\begin{aligned} \\mathcal{L}(x ; \\theta, \\phi)=\u0026 \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\\\ \u0026-\\beta D_{\\mathrm{KL}}\\left(q_{\\phi}(z \\mid x) \\| p(z)\\right), 0 \\leq \\betaCVAE #   Learning Structured Output Representation using Deep Conditional Generative Models10这篇论文提出了CVAE。\n传统VAE估计的是边缘概率$P(X)$， 而CVAE估计的是条件概率$P(X|Y)$。例如在图像中，X就是图像，Y是图像的类别，CVAE不仅可以生成图像，还可以指定生成哪种类型的图像。\n传统VAE的ELBO是： $$ \\log P(X)-D_{K L}[Q(z \\mid X) | P(z \\mid X)]=E[\\log P(X \\mid z)]-D_{K L}[Q(z \\mid X) | P(z)] $$ 加入条件概率，直观地，相当于我们让encoder同时依赖于X和Y - $Q(z|x,y)$，让decoder依赖于Z和Y - $P(x|z,y)$，那我们的ELBO就变成了: $$ \\log P(X \\mid c)-D_{K L}[Q(z \\mid X, c) | P(z \\mid X, c)]=E[\\log P(X \\mid z, c)]-D_{K L}[Q(z \\mid X, c) | P(z \\mid c)] $$ 详细的推导：\n TODO  Conditional Variational Image Deraining #   这篇文章11是CVAE的一个应用，它的任务是把下雨时候照的照片中的雨给去掉。\n要理解它的做法，我们需要重新理解下CVAE究竟在做什么。\n 传统的VAE都是输入一张图像X，然后我们用encoder学它的latent representation，然后我们再sample，经过decoder生成图像。如果我们想要生成指定类别的图像，这种方法是做不到的。\n那么一个很直接的思路是，我们可不可以输入一个类别，让encoder学习输出这个类别的latent representation，然后我们再通过同样的过程生成特定类别的图像？答案是很难。\n CVAE的做法是什么呢？我们给encoder一个额外信息，我们告诉它某个类别的图像是长什么样的，这样它在学习的时候会更有效。在这种角度下，CVAE其实是对传统VAE的一个小优化。\n在这篇去雨论文中，它的思路是类似的：\n 我们把下雨照片当作“类别”，每一张下雨照片都是一个类别 我们希望学习每个类别的latent representation，即每张下雨照片，干净版本的latent representation。 然后我们通过多次sample，decode出多张干净照片，取个平均作为最终结果。 只输入下雨照片不好训练encoder，因此我们加入一个condition，训练的时候我们把干净照片也加进去，帮助encoder进行训练。 但是问题是预测的时候，我们没有干净照片作为encoder的额外输入，这时候encoder就没法用了，为此作者引入了一个额外的prior network去模仿encoder的encode过程，但是只用rainy image作为输入。   预测的时候使用的是prior network。\n NVAE #   英伟达2020的一个工作2，主要说两点：\n 提出了一个精心设计的VAE网络，并指出VAE可以在网络设计上多下点功夫。 提出了多个稳定KLD的方法：  Residual Normal Distributions: Spectral Regularization (SR) More Expressive Approximate Posteriors with Normalizing Flows    具体还没细看。\nResources #    Denoising Criterion for Variational Autoencoding Framework  Some useful tricks in training variational autoencoder https://github.com/loliverhennigh/Variational-autoencoder-tricks-and-tips/blob/master/README.md  Loss设计 #     Balancing Reconstruction vs KL Loss Variational Autoencoder\n   Why don\u0026rsquo;t we use MSE as a reconstruction loss for VAE ? #399\n   how to weight KLD loss vs reconstruction loss in variational auto-encoder\n  Github #  这里是一些Github上使用VAE的项目代码：\n  AntixK/PyTorch-VAE  NVlabs/NVAE  Reference #    Yoshida 和 Miyato - 2017 -Spectral Norm Regularization for Improving the Generalizability of Deep Learning \u0026#x21a9;\u0026#xfe0e;\n Vahdat 和 Kautz - 2020 - NVAE A Deep Hierarchical Variational Autoencoder \u0026#x21a9;\u0026#xfe0e;\n Bowman 等。 - 2016 - Generating Sentences from a Continuous Space \u0026#x21a9;\u0026#xfe0e;\n https://discuss.pytorch.org/t/kld-loss-goes-nan-during-vae-training/42305 \u0026#x21a9;\u0026#xfe0e;\n https://github.com/y0ast/VAE-Torch/issues/3 \u0026#x21a9;\u0026#xfe0e;\n Higgins 等。 - 2017 - β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK \u0026#x21a9;\u0026#xfe0e;\n Burgess 等。 - 2018 - Understanding disentangling in $\\beta$-VAE \u0026#x21a9;\u0026#xfe0e;\n Sønderby 等。 - 2016 - Ladder Variational Autoencoders \u0026#x21a9;\u0026#xfe0e;\n Yan 等。 - 2020 - Re-balancing Variational Autoencoder Loss for Molecule Sequence Generation \u0026#x21a9;\u0026#xfe0e;\n Sohn 等。 - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models \u0026#x21a9;\u0026#xfe0e;\n Du 等。 - 2020 - Conditional Variational Image Deraining \u0026#x21a9;\u0026#xfe0e;\n  "},{"id":6,"href":"/blog/posts/misc/hugo-book/","title":"Hugo-Book Shortcodes Usages","section":"Posts","content":"Hugo-Book Shortcodes Usages\n 如果公式里出现矩阵，需要用div将$$包括起来，否则无法正常显。见 Link 如果公式需要换行，需要用六个斜杠\\\\\\，见 Link   Buttons #  {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}}  Get Home  Contribute  Columns #  Columns help organize shorter pieces of content horizontally for readability.\n{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example #  Left Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.  Mid Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!  Right Content #  Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.   Details #  Details shortcode is a helper for details html5 element. It is going to replace expand shortcode.\nExample #  {{\u0026lt; details \u0026#34;Title\u0026#34; [open] \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}} {{\u0026lt; details title=\u0026#34;Title\u0026#34; open=true \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}} Title Markdown content #  Lorem markdownum insigne\u0026hellip;   Expand #  Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nExample #  Default #  {{\u0026lt; expand \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}   展开 ↕  Markdown content Lorem markdownum insigne\u0026hellip;    With Custom Label #  {{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}   Custom Label ...  Markdown content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.    Hints #  Hint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{\u0026lt; hint [info|warning|danger] \u0026gt;}} **Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}} Example #  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Mermaid Chart #   Mermaid is library for generating svg charts and diagrams from text.\nExample #  {{\u0026lt; mermaid [class=\u0026#34;text-center\u0026#34;]\u0026gt;}} sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}}    mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) sequenceDiagram Alice-Bob: Hello Bob, how are you? alt is sick Bob-Alice: Not so good :( else is well Bob-Alice: Feeling fresh like a daisy end opt Extra response Bob-Alice: Thanks for asking end   Tabs #  Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}} # MacOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example #  MacOS MacOS #  This is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux Linux #  This is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows Windows #  This is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":7,"href":"/blog/posts/programming/semaphores/","title":"Semaphores(信号量)","section":"Posts","content":"Semaphores是一种同步机制（Concurrency Mechanisms），它用来协调各个进程访问公共资源。其基本思想如下所述：\n 两个或多个进程通过一个信号量进行协调，当一个进程需要某个资源时，它需要申请并等待一个信号，如果信号没有来临则等待。\n General Semaphores #  general semaphore的一种实现。\nstruct semahpore { int count; queueType queue; }; void semWait(semaphore a) { s.count--; if (s.count \u0026lt; 0) { /* place this process in s.queue. */ /* block this process. */ } } void semSignal(semaphore a) { s.count++; if(s.count \u0026lt;= 0) { /* remove a process from s.queue. */ /* place process p on ready list. */ } }   count的含义\n 大于0时：指示当前可以加进来的进程数。 小于等于0时：绝对值指示当前想加入还未加入的进程数。    队列里存储的就是等待加入的进程。\n  Binary Semaphore #  struct binary_semaphore { enum {zero, one} value; queueType queue; }; // B: Binary void semWaitB(binary_semaphore s) { if (s.value == one) s.value = zero; else { /* place this process in s.queue. */ /* block this process. */ } } void semSignalB(semaphore s) { if (s.queue is empty()) s.value = one; else { /* remove a process P from s.queue. */ /* place process P on ready list. */ } } 优缺点 #   比较难编程，当需要同时使用多个semaphore的时候，需要仔细安排semaphore调用顺序等，否则容易出现死锁。  例子：\n/* program producersonsumer */ semaphore n = 0, s = 1; void producer() { while (true) { produce(); semWait(s); append(); semSignal(s); semSignal(n); } } void consumer() { while(true) { semWait(n); semWait(s); take(); semSignal(s); consume(); } } void main() { parbegin (producer, consumer); }  这里 semaphore n 用于防止consumer消耗空buffer，s用于buffer的mutual exclusion。 如果buffer为空，且consumer的 semWait(n); 与 semWait(s); 互换，则第一个semWait能成功进入，但会在第二semWait被阻塞。 这时由于，buffer被consumer占用，producer无法生产新的product加入buffer， semWait(s);将被一直阻塞，产生死锁。 "},{"id":8,"href":"/blog/posts/algorithms/%E6%B5%B7%E6%98%8E%E7%A0%81/","title":"错误检测-海(汉)明码","section":"Posts","content":"Hamming code，海明码，汉明码都是一个东西。它是一种编码方式，通常用在网络信息传输中，通过这种编码方式编码出来的二进制数据具有检测一位错误位的能力。\n例如：\n 假设要传输的二进制数据为 1011001 。 通过海明码编码后（如何编码后面会说），得到 101 0100 1110 。 假设从右往左第6位从0变为了1。 通过某种方法我们可以从产生变化后的数据 101 0110 1110 得知第六位发生改变。  下面则将具体讲解这些过程。\n编码过程 #  第一步是确定冗余位(bit)的数目，这些位将为检错提供必要的信息。\n根据算法要求，冗余位的数目(r)必须满足： $$ 2^r \\geq m + r + 1 $$ 其中m为原始数据位的数目。\n第二步是确定冗余位的位置\n同样根据算法设计，冗余位将放在位置编号为2的幂次的位置上。\n如：1,2,4,8\u0026hellip;\n假设要传输的数据为 1011001 ，则编码后各个数据位的排布应如下图所示：\n 第三步是确定冗余位的值\n计算方法如下：\n 处于第$2^i$位的冗余位的值，将是所有位置编码(二进制表示)中第i位为1的那些位置的原始数据的偶校验。\n 如R1，它的值将是第1,3,5,7,9,11这些位置上的数据的偶校验，所以\n由于3，5，7，9，11中1的个数为偶数，故R1=0（偶校验～1的个数为偶数个）。\n 假设要传输的数据为 1011001 ，则编码后结果如下：\n 检错与纠错 #  同上，假设要传输的数据为 1011001 ，编码后为1010 1001 110 。\n假设实际传输过去的数据为 1010 1101 110 ，即从右往左第6位由0变为了1。\n这时候，如果我们再用计算冗余位值的方法重新计算一遍冗余位的值，我们会发现：\nR4R3R2R1 = 0110 即十进制下的6。\n这表明从右往左第6位发生错误，这时候我们将其反转即可纠正。\nIntuition #  实际上，我们的目标是得到一个数字，这个数字表示数据中出错那一位的位置。\n如果我们将其表示为二进制的形式，那么我们的任务即变为了确定各个bit位是0还是1。\n在前面我曾要求过这个表达式：\n$2^r \\geq m + r + 1$\n它的含义即在于，如果我们利用r个冗余位确定这个二进制数字（r个冗余位确定r个数字中的bit位），那么我们必须保证这个数字的范围能够表示编码数据串中的所有位置。\n可能有人会问，既然如此，那满足这个表达式$2^r \\geq m + r$ 不就可以了吗，为什么要+1呢？\n 因为位置编码必须从1开始，从0开始没法纠错。因此位置编码的最大值位m+r+1。\n  海明码的巧妙之处就在于它偶校验的分组上，每个冗余位都对应了一个bit位为1的那些位置。\n  一开始将数据编码成海明码的时候，所有组都是满足偶校验的，即1的个数为偶数。\n  如果某一位发生变化，不管是从0变为1还是从1变为0，它所在组对应的偶校验均会变为1。\n  而且它只属于-它位置编码为1的那几位对应的那些组，也只有这些组的偶校验会变成1。\n  这样，如果我们对所有组求一次偶校验，组成的二进制数表示的正好是发生变化那个位的位置。\n  换句话说，海明码将错误位置的确认分散到了各个组上，通过一次能偶校验，能够确定错误位置属不属于这个组，经过r次偶校验后，便能确定其具体位置。\n参考资料 #  https://www.geeksforgeeks.org/computer-network-hamming-code/\n"},{"id":9,"href":"/blog/posts/machine-learning/Generalized-Linear-Model/","title":"How genralized linear model work?","section":"Posts","content":"本文将简单的讲述：GLM是如何工作的？\nOur goal 在讨论GLM之前，我们还是先要明确我们的目标是什么：\n 给定一些 feature X，我们需要预测一个y。 即我们需要构造一个 hpythoesis: $h_\\theta(x) = y$\n How GLM work?  假设 y 的取值服从某个分布 如果这个分布可以写成指数族的形式：$p(y; \\eta) = b(y) exp(\\eta^T T(y) − a(\\eta))$ 则有一个性质：T(y)是y的充分统计量 然后我们则用 y 在该分布下的数学期望去预测y，即我们让 $h_\\theta(x) = E(y)$ 除此之外还假设 $\\eta$ 与 X 线性相关，即： $\\eta = \\theta^TX$  Example  Logistic Regression   假设y的取值服从伯努利分布： $y|x; \\theta ∼ Bernoulli(\\phi)$ 伯努利分布在指数族中，将其改写成指数族的形式：  \\[ \\begin{align} p(y; \\phi) \u0026= \\phi^y (1-\\phi)^{1-y} \\\\ \u0026= exp(ylog(\\phi) + (1-y)log(1-\\phi))\\\\ \u0026= exp(ylog\\frac{\\phi}{1-\\phi} +log(1-\\phi)) \\end{align} \\]\n​ 由此可得： \\( T(y) = y \\\\ \\eta = log\\frac{\\phi}{1-\\phi} \\) ​ 反解$\\phi$ ： \\( \\phi = \\frac{e^{\\eta}}{1+e^{\\eta}}= \\frac{1}{1+e^{-\\eta}} \\)\n 利用y在伯努利分布下的数学期望预测y：  \\[ \\begin{align} y \u0026= E[y|x;\\theta] \\\\ \u0026=\\phi \\\\ \u0026= \\frac{1}{1+e^{-\\eta}} \\end{align} \\]\n 假设 $\\eta$ 与 X 线性相关，即： $\\eta = \\theta^TX$ 。可得：  \\[ y = \\frac{1}{1+e^{-\\theta^TX}} \\]\n"},{"id":10,"href":"/blog/posts/algorithms/%E5%85%8B%E9%B2%81%E6%96%AF%E5%8D%A1%E5%B0%94%E7%AE%97%E6%B3%95/","title":"克鲁斯卡尔算法","section":"Posts","content":"首先，克鲁斯卡尔算法是用来求最小生成树的。另一种求最小生成树的算法叫普林姆算法（Prim）。\n克鲁斯卡尔算法本质是贪心，每次选取 不与当前已有边构成环 权值最小 的边作为生成树的边。\n算法细节 #   判断是否构成环使用并查集  复杂度分析 #  使用并查集的克鲁斯卡尔算法时间复杂度为O(eloge)，其中loge为并查集判断环所需时间，每条边都要判断一次，因此为O(eloge)。\n"},{"id":11,"href":"/blog/posts/algorithms/%E6%99%AE%E6%9E%97%E5%A7%86%E7%AE%97%E6%B3%95/","title":"普林姆算法","section":"Posts","content":"普林姆算法也是用来求最小生成树的，与克鲁斯卡尔算法遍历边不同，普林姆遍历的是点。\n普林姆算法同样基于贪心，以任意点为初始点，每次选取与已选点相连的边中权值最小的边，并把与这条边相连的点加入已选点集合。\n算法实现 #   维护一个数组 minEdge[i]  含义为已选点集合中到第i个点 最小权值的边的终点，没有边则为无穷大。\n每次选minEdge中最小的点加入已选点集合，并更新minEdge数组。 重复操作，直到所有点加入已选点集合。  优化 #   使用优先队列维护minEdge  每往已选点集合加入点时，就把与该点相连的所有边都加入优先队列。而当取边时，需要判断一下边的终点是否在已选点集合中（可以使用一个标记数组）。\n时间复杂度分析 #   非优先队列法   在minEdge中找最小的时间复杂度为O(n)。 更新数组时间复杂度总和为O(e) —— 每个点都会更新与它相连的边，所有边加起来为e。 一共要进行n次在minEdge中找最小的操作。  故总的时间复杂度为O(n^2+e)。\n优先队列法  优先队列加边出边的时间复杂度都是O(loge)。\n 所有边都会进入优先队列至少一次，至多两次。时间复杂度为O(eloge)。 出边次数最少为n次，最多为e次，时间复杂度也是O(eloge)。  故总的时间复杂度为O(eloge)。\n分析： 当边数达到cn^2或以上时，用非优先队列法要好，反之可以使用优先队列法。\n当e = n^2 ,  O(eloge) = O(n^2logn) , O(n^2+e) = O(n^2) 。\n"},{"id":12,"href":"/blog/posts/algorithms/%E5%B9%B6%E6%9F%A5%E9%9B%86/","title":"并查集","section":"Posts","content":"使用并查集可以快速判断两个元素是否属于同一个集合。\n算法 #  基本思路 #  使用树来表示集合。\n 初始时，所有元素都分别是一棵树。 若两个元素属于同一个集合，则用一个元素作为另一个元素的孩子。  实现\n数据结构：\n fa[i] : 保存元素i的祖先  第一步：初始化\nfor(int i=0; i \u0026lt; n; ++i) fa[i] = i;\t// 初始时，所有元素都分别是一棵树(祖先是自己） 第二步：根据关系构造并查集\n// 查找树根 int find(int x) { if(fa[x] == x) return x; return find(fa[x]); } int temp_a, temp_b; for(int i=0; i \u0026lt; m; ++i){ cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, b属于同一集合  temp_a = find(a), temp_b = find(b); fa[a] = b; } Note: 先找到树根，不管相不相同，都让a是b的孩子。\n第三步：查询方法\nfor(int i=0; i \u0026lt; T; ++i){ cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; if(find(a) == find(b)) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } Note: 判断树根是否相同即可。\n优化 #   降低树的深度（路径压缩）  find 操作耗费时间取决于树的深度，因此，如果在 find  执行过程让树的深度降低，则可以降低后续操作所需的时间。\n方法： 让树根的后代们尽量都是树根的孩子。\n// 查找树根 int find(int x) { if(fa[x] == x) return x; //return find(fa[x]);  return fa[x] = find(fa[x]); } Note: fa[x] = find(fa[x]) 的值为左值 fa[x] ，等价于先fa[x] = find(fa[x]);  再 return fa[x]; 。\n让小树接在大树上  每次发生树的拼接之后的 find 操作都需要重新降低树的深度，而显然元素少的树接到元素多的树上，需要的 重组次数较少。\n例子：\n 灰色圆圈即为要重组的元素，显然小接大要合算。\n实现方法：\n r[i] : 表示以元素i为根的树的相对大小  // 封装一下 void Union(int a, int b) { int temp_a = find(a), temp_b = find(b); if(r[temp_a] \u0026gt;= r[temp_b]) { fa[b] = a; //小树的父亲等于大树  if(r[temp_a] == r[temp_b]) ++r[a]; // b为a的孩子-\u0026gt;树变大了  } else fa[a] = b; } for(int i=0; i \u0026lt; n; ++i) { fa[i] = i;\tr[i] = 0;\t// 记得初始化 } for(int i=0; i \u0026lt; m; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, b属于同一集合 \tUnion(a,b); } 实验 #  输入：\n 10 8 1 2 1 3 1 4 1 5 2 6 5 7 5 8 0 9 5 1 6 1 3 5 7 0 9 1 0 输出：\nYes Yes Yes Yes No 完整代码：\n// 并查集 #include \u0026lt;iostream\u0026gt;using namespace std; const int MAXN = 100; int fa[MAXN], r[MAXN], n, m; // n为元素数,m为关系数  // 查找树根 int find(int x) { if(fa[x] == x) return x; return find(fa[x]); } void Union(int a, int b) { int temp_a = find(a), temp_b = find(b); if(r[temp_a] \u0026gt;= r[temp_b]){ fa[b] = a; //小树的父亲等于大树  if(r[temp_a] == r[temp_b]) ++r[a]; // b为a的孩子-\u0026gt;树变大了  } else fa[a] = b; } int main() { cin \u0026gt;\u0026gt; n; // 初始化  for(int i=0; i \u0026lt; n; ++i) { fa[i] = i;\t// 初始时，所有元素都分别是一棵树(祖先是自己）  r[i] = 0; } cin \u0026gt;\u0026gt; m; // 构造并查集  int a, b; for(int i=0; i \u0026lt; m; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; // a, b属于同一集合  Union(a,b); } // 查询  int T; cin \u0026gt;\u0026gt; T; for(int i=0; i \u0026lt; T; ++i) { cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; if(find(a) == find(b)) cout \u0026lt;\u0026lt; \u0026#34;Yes\u0026#34; \u0026lt;\u0026lt; endl; else cout \u0026lt;\u0026lt; \u0026#34;No\u0026#34; \u0026lt;\u0026lt; endl; } return 0; } 复杂度分析 #   优化前的算法  算法复杂度主要取决于：1. 构造并查集 2. 查询操作\n 构造并查集复杂度分析：  查询find操作递归次数不会超过树的深度，树的深度不会大于n，因此，find操作复杂度为O(logn)。\n构造时时间花费主要在find操作上，且要执行2m次find操作。因此构造并查集的时间复杂度为O(2mlogn), 忽略常数为O(mlogn)。\n 查询操作复杂度分析：  查询操作需要两次find操作，时间复杂度为O(logn)。\n优化后的算法  先说结论：O((n+m)log*n) ，其中log*为增长及其缓慢的 迭代函数，通常可以视为常数。\n证明为构造性证明，较为复杂，这里不做扩展。\n"},{"id":13,"href":"/blog/posts/machine-learning/Principal-component-analysis/","title":"Principal component analysis","section":"Posts","content":"PCA (Principal component analysis) 是一种给数据降维的方法。\n利用PCA，能将一堆高维空间的数据映射到一个低维空间，并最大限度保持它们之间的可区分性。\n 可以看到，图中的数据点大致分布在一条直线上。\n因此，我们能够将点投影到直线上，用直线上的点到原点的距离代替原来二维向量。\n这样我们的数据就从 二维降到了一维。\n 推导过程 #   设输入数据为X ，X 为 (n * m) 的矩阵，每一行为一个sample。  如果我们要将 数据 转换到一个低维空间，我们应该对 X 做一次线性变换（即更换基底）。\n 设新的 基底 为 P ， P 为一个 (m * k) 的矩阵，每一列为一个基底。 经过变换后，设新的数据为 Y, 则 $Y=XP$ 。  X 的协方差矩阵为 $X^TX$，我们希望经过变换后的 Y 的协方差矩阵（$Y^TY$）\n 对角线上元素绝对值尽可能大 非对角线上元素绝对值绝对值尽可能小。  即我们希望，在新的基底下，各个feature的关联性很小，而在同一个feature上，能尽最大可能保持数据的variance。\nY的协方差矩阵的数学表达式:\n$$C_Y = Y^TY = (XP)^T(XP) = P^TX^TXP$$\n可以证明，为了使 $C_Y$ 满足上述条件，P 应为 $X^TX$ 的特征向量矩阵，其中 P 的每一列为一个特征向量。\n那么，由矩阵对角化的知识，可以得到\n$$C_Y = P^TX^TXP = D$$\n其中D为一对角矩阵，对角线上元素为特征向量对应的特征值。\n在这里，每个特征值还对应了新的feature的方差（variance），它的值的大小反映了新的feature用于区分数据的能力。方差小说明这个feature对大部分数据来说基本都一样（直观来说，既然大家都一样，我们就可以说这个feature是多余的）。\n因此，我们可以将特征向量根据特征值排序，根据需要将原始数据转换为低维数据，并尽最大可能保持数据的有效性。\n参考资料 #  Stackexhange\n  How would you explain covariance to someone who understands only the mean?  Making sense of principal component analysis, eigenvectors \u0026amp; eigenvalues  Why is the eigenvector of a covariance matrix equal to a principal component?  Intuition on the definition of the covariance  Does the magnitude of covariance have any real meaning?  Intuitive understanding covariance, cross-covariance, auto-/cross-correliation and power spectrum density  Wikipedia\n  Covariance  Cross product  Covariance matrix  Other\n A Tutorial on Principal Component Analysis\u0026rsquo;by Jonathon Shlens  Principal Components Analysis "},{"id":14,"href":"/blog/posts/misc/%E6%B4%9B%E4%BC%A6%E5%85%B9%E5%8F%98%E6%8D%A2/","title":"洛伦兹变换","section":"Posts","content":"洛伦兹变换是从光速不变原理推出的，不同坐标系坐标之间的转换关系。\n**光速不变原理：**对任何参考系，光速都为一个固定值C\n狭义相对性原理： 在所有惯性系中，物理定律都有相同的表达形式\n 洛伦兹变换推导 #   以二维直角坐标为例，只考虑x轴方向运动\n假设你相对于我向x轴正方向以速度 u 运动，我和你在某个时间相遇，这时候\n 我和你分别以自己为原点建立坐标系。 将我和你的时钟归零    以我看来，假设在 t 时刻， x 位置发生了一个事件。\n设该事件，在你看来，发生在 t‘ 时刻， x‘ 位置。\n接下来，有关我的量都不带 ' ,而有关你的都带 '。\n###相对论出现之前\n那么由于你相对我在运动，则在我看来，你运动了 ut 的距离，并且我认为 你认为事件发生的位置是 $$ x' = x-ut $$ 而反过来，在你看来，你认为你运动了 ut' 的距离，你认为 我认为事件发生的位置是 $$ x = x' + ut' $$\n###相对论出现之后\n如果现在，我们质疑这两个式子的正确性。\n比如，如果在你看来，你觉得你的 x' 与我认为 你认为的 x' 不相等，那么不如给式子乘上一个系数 $\\gamma$ ，通过别的条件计算，如果系数是1，那我就是对的，如果不是，那结果就是那样。 $$ x' = \\gamma (x-ut) $$ 同理，我觉得你的 x 与你认为的我的 x 不相等，同样的，乘一个系数 $\\gamma$ ， 这个系数与之前的一样 [1]。 $$ x = \\gamma (x'+ut') $$ 为了解出这个系数 $\\gamma$ ，我们不妨从一个具体事件出发（当然这可能导致推导不太严谨，但是可以证明，结果是正确的）。\n  假设这个事件以一束光触发，并且在我看来在 t 时间的时候发生\n 由光速不变原理，我们可以得到 [2]\n 在我看来，发生的距离 $x = ct$ 在你看来，距离 $x' = ct'$  由上面四个方程，则能推出洛伦兹变换的坐标变换式 [3]\n$$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx' = \\frac{x-ut}{\\sqrt{1-u^2/c^2}} \u0026amp; (1)\\\nx = \\frac{x'+ut'}{\\sqrt{1-u^2/c^2}} \u0026amp; (2) \\end{array}\n\\right.\n\\end{equation} $$ 方程（1）即由我的坐标转换为你的。\n方程（2）反过来。\n注意： u表示你相对我向正方向运动的速度。\n而利用 (1), (2)式还能够导出 洛伦兹变换的时间变换式 [4]。 $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nt' = \\frac{t-xu/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp; (3)\\\nt = \\frac{t'+x\u0026rsquo;u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp; (4) \\end{array}\n\\right.\n\\end{equation} $$ 仅仅通过这四个式子可能不太容易弄明白 洛伦兹变换究竟意味着什么。\n下面将展示一系列通过洛伦兹变换的到的结果。\n时间差与空间差 #  假设在我看来在(在我的坐标系中)， 在$(x_1,t_1)$ 和 $(x_2,t_2)$ 分别发生了两个事件。\n那么由洛伦兹变换，在你看来 $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx_1' = \\frac{x_1-ut_1}{\\sqrt{1-u^2/c^2}} \u0026amp; \\\nt_1' = \\frac{t_1-x_1u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp;\n\\end{array}\n\\right.\n\\end{equation} $$\n$$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\nx_2' = \\frac{x_2-ut_2}{\\sqrt{1-u^2/c^2}} \u0026amp; \\\nt_2' = \\frac{t_2-x_2u/c^2}{\\sqrt{1-u^2/c^2}} \u0026amp;\n\\end{array}\n\\right.\n\\end{equation} $$\n上下分别相减，则可以得到时间差和空间差的关系 $$ \\begin{equation}\n\\left{\n\\begin{array}{lr}\n\\Delta x' = \\frac{\\Delta x -u\\Delta t }{\\sqrt{1-u^2/c^2}}\u0026amp; (5)\\\n\\Delta t' = \\frac{\\Delta t - \\Delta x u/c^2}{\\sqrt{1-u^2/c^2}}\u0026amp; (6) \\end{array}\n\\right.\n\\end{equation} $$ (5) 意味着在我看来的距离 $\\Delta x$，在相对我运动的你看来，要长一些。\n(6) 意味着在我看来的时间差 $\\Delta t$ ，对于相对我运动的你来说，不再是相同的时间差。\n长度收缩 #  现在，假设我和你要测量一个物体的长度，这个物体以u的速度相对我向正方向运动，而你和这个物体一起运动。\n我将通过两个事件测量这个物体的长度。\n 在某一时刻，在物体两端发生了两个事件，分别记为 $(x_1,t),(x_2,t)$ 那么两个事件的空间差即为物体的长度  那么由前面的空间差的关系，你测量出的长度为 $\\Delta x'$ ，而我测量出的长度为 $\\Delta x$，由公式5可知，我测出的长度比你测出的要短一些。\n即相对物体运动的观察者 测出的物体长度 要比相对物体静止的观察者 测出的要短。\n时间延缓 #  前面提到，我和运动的你，对于两个事件的时间差的认识发生了偏差。\n假设这两个事件对应时钟\n 事件1: 走到这一秒 滴 事件2: 走到下一秒 嗒  那么，由前面的（6）式可以知道，在我的时空坐标中的 1s 与你的时空坐标中的 1s 不再相同。 如果我们忽略两个走针的距离，即$\\Delta x= 0$ (对应其他事件，即同一地点发生的两个事件），那么我们可以得到 $$ \\Delta t' = \\frac{\\Delta t }{\\sqrt{1-u^2/c^2}} $$ 在相对我运动的你的时空坐标中，你的1s变长了。也即你的时间延缓了。\n其实，这里的钟可以提高到更加普遍的范畴上，任何两个事件的时间差都会变长。比如我们的身体的化学反应的进程等等，都被延缓了。\n孪生子悖论\n如果我活了50岁，你可能才到20岁。\n 如果我现在登上飞船，以很高的速度进行星际旅行，那么等我回到地球时，会发现你比我年轻。\n而这在你看来，则相反，你会认为我比你年轻。这就是孪生子悖论。\n 事实上，地球上的你的看法才是对的，狭义相对论只对惯性系有同等意义，即只有在惯性系中上面的等式才成立。对于在飞船上的你，必定经历了一段加速过程（你的时空坐标是非惯性系），在这段时间里，不能利用上面的公式计算 我和你的时间差。\n而要正确计算我和你的年龄差，需要利用积分进行计算。\n参考： 维基百科：双生子佯谬\n同时性的相对性 #  由上面的公式（6）我们还可以得到 同时的概念是相对的 。 $$ \\Delta t' = \\frac{\\Delta t - \\Delta x u/c^2}{\\sqrt{1-u^2/c^2}} $$ 两个对我来说同时发生的时间，即 $\\Delta t = 0$ ，对于你来说\n 如果对我来说，它们不同地发生 [5] ($\\Delta x \\not = 0$) ，则 $\\Delta t' \\not= 0$ ，即对我同时发生的两个事件，对你来说不同时发生。 而如果对我来说是同时同地发生的事件，对你来说也是同时同地发生的。[6]  速度的关系 #  由洛伦兹变换还能推导出，在我的时空坐标中的速度与你的时空坐标中的速度之间的关系。 $$ v' = \\frac{\\Delta x'}{\\Delta t'} = \\frac{\\Delta x -u\\Delta t }{\\Delta t - \\Delta x u/c^2} = \\frac{\\Delta x / \\Delta t -u }{1 - \\Delta x/\\Delta t u/c^2} = \\frac{v -u }{1 - v u/c^2} $$\n附录 #  [1] 根据相对性原理，在所有惯性系中物理定律都具有相同的形式，故修正系数 $\\gamma$ 对任何人都应相同。\n[2]\n[3]\n[4]\n[5] 由公式5，对于你也一定不同地\n[6] 对于这个，我们设想一个事件两辆车相撞，它可以分解为两个事件，一个是一辆车在某个时刻到某个地点，另一个是另一辆车在同一时刻到同一地点。这样它们才能相撞。所以如果对我来说同时同地发生的这两个事件，对你来说不是同时同地的，那么对你来说车不会相撞，而车相撞是客观存在的。因此对我来说是同时同地发生的事件，对你来说也是同时同地发生的。\n四维空间的理解 #  在二维坐标中，一个点(x,y)，不论怎么做变换，旋转，平移等等，都只会涉及x,y。用两个坐标就能够描述这个二维世界。\n在三维世界，我们描述一个事件，需要用到（x,y,z,t），我们怎么\u0026hellip;\n待续.\n"},{"id":15,"href":"/blog/posts/algorithms/Fibonacci%E6%95%B0%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95/","title":"Fibonacci数的迭代算法","section":"Posts","content":"使用迭代算法求斐波那契数列，\n时间复杂度O(n)，空间复杂度O(1)。\nint f(int n) { int f1 = 1, f2= 0; while(--n){ f1 = f1+f2; // f(n) = f(n-1)+f(n-2)  f2 = f1-f2;\t// f(n-1) = f(n)-f(n-2)  } return f1; } "},{"id":16,"href":"/blog/posts/algorithms/%E5%BF%AB%E9%80%9F%E5%B9%82/","title":"快速幂算法","section":"Posts","content":"在c，c++语言中，并没有提供求幂的基本运算，通常我们需要自己写函数或者调用STL提供的函数。\n一般情况下，我们写的求幂函数基本上都是循环累乘，时间复杂度为O(n)。虽说是线性的时间复杂度，但求幂运算作为基础运算，往往调用频繁，这时候即使是线性的时间复杂度也将变得难也接受。\n利用快速幂可以快速计算底数的n次幂。其时间复杂度为 O(logn)\n原理 #  以求 $a^{11}$ 为例，将11写成二进制形式 1011。\n则 $a^{11} = a^{2^0+2^1+2^3}=a^{1+2+8} =a^1\\times a^2\\times a^8$\n使用一个累乘器，每次翻倍 base = base*base 逐步得到 $a^1,a^2,a^4,a^8$ ，根据11的二进制，如果为1则乘进结果里。\n实现 #  迭代算法\nint power(int a,int b) { int base = 1,ans = a; while(a \u0026gt; 0){ if(a \u0026amp; 1 == 1) ans *= base;//二进制位为1的才要乘  base *= base; a \u0026gt;\u0026gt;= 1; } return ans; } 递归算法\nint power(int a,int b) { if(b == 1) return a; int temp = power(a,b\u0026gt;\u0026gt;1) * power(a,b\u0026gt;\u0026gt;1); return (b\u0026amp;1 == 1 ? a:1) * temp * temp; // 若指数为偶数则分解成一半，若为奇数，则还要再乘a } 递归算法与迭代算法的思路略有不同。\n"},{"id":17,"href":"/blog/posts/algorithms/%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9A%84%E5%A6%99%E7%94%A8/","title":"位运算的妙用","section":"Posts","content":"对于一些特定问题，巧妙运用位运算能使解法异常简洁和高效，同时，适当运用位运算也能对程序进行优化。\n运用的时候，可能涉及多种位运算。\n计算机中位运算分为以下六种：\n 与 \u0026amp; 或 | 非 ～ 异或 ^ 左移 \u0026laquo; 右移 \u0026raquo;  异或 #  异或具有以下性质（可能还有，下同）：\n a^b = b^a a^a = 0 a^0 = a  实例 #  [leetcode 136 single number]\n Given an array of integers, every element appears twice except for one. Find that single one.\nNote: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?\n **题目大意：**每个元素都出现两次，但有一个只出现一次，找出出现一次的元素。\n 这题如果用标记数组做 时间复杂度，空间复杂度都是 O(n) 。但如果利用异或的前两个性质，则可以将空间复杂度压缩到O(1)。\nclass Solution { public: int singleNumber(vector\u0026lt;int\u0026gt;\u0026amp; nums) { if(nums.size() == 0) return 0; int result = nums[0]; for(int i=1;i\u0026lt;nums.size();i++) result = result ^ nums[i]; return result; } }; 与或 #  与运算可以快速取得一个变量某个 bit位 的数值。\n如： 0010 \u0026amp; 1110 = 0010 0010 \u0026amp; 1101 = 0000\n如果 0010 \u0026amp; b = 0010 则表明 b 的第二位是1，不等则为0。\n 与运算可以快速将某bit为快速置为0，而或运算可以快速将某bit位置为1。\n与运算的其他用法\n n \u0026amp; (n-1) 可以把n的最低位置0  左移右移 #  左移相当于乘2，右移相当于除2，并且它们的速度比乘除要快。\n所以当程序中出现大量✖️2，或➗2运算时，可以用左移和右移进行优化。\n例子 #   LeetCode Bit Manipulation\n"},{"id":18,"href":"/blog/posts/algorithms/%E7%AD%9B%E6%B3%95%E6%B1%82%E7%B4%A0%E6%95%B0/","title":"筛法求素数","section":"Posts","content":" 假设要求n以内的素数\n 筛法求素数是用一个大小为n的数组，作为标记数组，如果没被标记到则为素数。\n开始均为未标记。\n从2开始，2没被标记，将2存入一个存素数的地方，然后筛掉小于n的，2的所有倍数。然后是3，筛掉3的所有倍数，依此类推，直到n-1。\n优化 #  上面的做法，同一个数可能会被筛掉多次，比如6会被3和2各筛一次。\n为了提高效率，需要进行优化，使得每个数尽可能的被少筛，如果能一次最好。\n考虑到任何合数都可以分解成若干个素数的乘积。在筛掉合数的过程中，最好的是让每个合数只被它最小的因子筛掉。\n如24 18 都只被2筛掉\nC++实现 #  int countPrimes(int n) { vector\u0026lt;bool\u0026gt; vis(n,false); vector\u0026lt;int\u0026gt; prime; for(int i=2;i\u0026lt;n;i++){ if(!vis[i]) prime.push_back(i); for(int j=0;j\u0026lt;prime.size() \u0026amp;\u0026amp; i*prime[j]\u0026lt;=n;j++){ vis[i*prime[j]] = true; if(i%prime[j] == 0) break;\t//优化  } } return prime.size(); } 最外层循环每次循环，都能得到小于等于i的所有素数，当要求i+1内的素数时，只需判断i+1是否在之前被筛掉。\n与此同时，将当前所有素数的i+1倍筛掉。\n那后面出现的素数的i+1倍，设为m ，会怎么样呢？\n 如果i+1是素数，m会被 i+1 筛掉 如果i+1不是素数，则m 会被i+1的最小质因数（之前出现过的素数中的某一个）筛掉。  **优化点：**每个数都被它最小的因数筛掉\n具体操作：如果循环到某个素数 prime[j] 是 i+1的倍数时，后面的素数的i+1倍 prime[j+1] * (i+1) 就不用筛了。\n原因在于：后面素数的i+1倍一定会被 prime[j] （更小的一个数）筛掉，\n$prime[j+1] * (i+1) = prime[j+1] * prime[j] * k$\n$k = (i+1) / prime[j]$\n"},{"id":19,"href":"/blog/docs/notes/paper/arbrcan/","title":"ArbSR: 任意尺度超分 - 阅读笔记","section":"Paper","content":"Created: August, 12, 2021 论文: Learning A Single Network for Scale-Arbitrary Super-Resolution\nCode: https://github.com/LongguangWang/ArbSR\n "},{"id":20,"href":"/blog/docs/matrix/jordan/","title":"Jordan Form","section":"Matrix","content":"Created: Decemenber, 7, 2020 Jordan Form是矩阵对角化的一个推广的产物。\n我们知道矩阵$A$可以对角化成一个对角矩阵$\\Lambda$，而且这个矩阵的对角线元素是$A$的特征值，前提是矩阵$A$存在n个线性无关的特征向量。如果不满足，则不能对角化，即找不到一个S，使得：\n$$ \\Lambda = S^{-1}AS $$\nJordan则推广了这个对角化的过程，任何矩阵，即使它不存在n个线性无关的特征向量，它也可以相似“对角化”为一个Jordan标准型$J$。\n$$ J = P^{-1}AP $$\n但这时，J的对角线不在是一个单独的元素，而是一系列Jordan块（请想象一下分块矩阵）。\n$\\lambda$矩阵 #  如果一个矩阵每个元素都是变量$\\lambda$的多项式，则称这个矩阵为$\\lambda$矩阵，记做$A(\\lambda)$。\nSmtih标准形\n每一个$\\lambda$矩阵都可以化成对角形矩阵，这个对角形矩阵称为该$\\lambda$矩阵的Smith标准形。\n对角线上的元素按一定顺序排列：\n 每一个元素都能整除后一个元素 0在最后面（这其实是第一条规则的推论）  不变因子\nSmith标准形对角线上的元素称为$A(\\lambda)$的不变因子。\n行列式因子\nk阶行列式因子就是Smith标准形的前k各元素相乘。k最大只能取到矩阵秩，就是说行列式因子没有0。\n$$ D_k(\\lambda) = d_1(\\lambda)d_2(\\lambda)\u0026hellip;d_k(\\lambda) $$\n初等因子\n不是常数的不变因子的“因数”称为初等因子。\n例如，若$\\lambda$矩阵的不变因子是\n1, 1, $(\\lambda-2)^5(\\lambda-3)^3$, $(\\lambda-2)^5(\\lambda-3)^4(\\lambda+2)$\n则它的初等因子是，仔细看\n$(\\lambda-2)^5$, $(\\lambda-3)^3$, $(\\lambda-2)^5$, $(\\lambda-3)^4$, $(\\lambda+2)$\n"},{"id":21,"href":"/blog/docs/notes/paper/raft/","title":"Raft 阅读笔记","section":"Paper","content":"Created: August, 6, 2021 论文: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow\nCode: https://github.com/princeton-vl/RAFT\n 这篇文章做的是光流估计，简单来说，任务就是输入两张图像$I_1,I_2$，我们需要估计出第二张图像$I_1$每个像素点$(u,v)$相对于第一张图像$I_1$的偏移量$(f^1(u), f^2(v))$。\n$$ I_1 = I_2 + f $$\n这篇文章的思路是这样的：\n 首先使用一个feature encoder  求出flow feature：同时输入img1，img2，得到fmap1，fmap2 求出context feature：只输入img1，得到inp   求出fmap1和fmap2的相似度corr 使用一个RNN迭代的求出flow。  RNN的输入包括一个隐藏状态net，context feature inp，相似度corr，上一步的flow     这里的主要问题是每迭代一次，我们都得到了一个更接近img1的img2，这时候相似度需要重新计算。\n RAFT设计了一个查表的思路，只需要在最开始算一遍即可。它的方法是用新坐标周围的点的flow feature拉成一个向量作为新坐标点的flow特征。\n"},{"id":22,"href":"/blog/docs/matrix/types/","title":"各种矩阵","section":"Matrix","content":"Created: Decemenber, 1, 2020 对称矩阵 #    定义：满足$A^T=A$的矩阵。 显然，对称矩阵是方阵，否则转一下形状都变了。  对称矩阵的特征值/向量有着很好的性质：\n 实对称矩阵的特征值都是实数。 对称矩阵的特征向量是正交的。没验证，不过应该是不同特征值对应的特征向量一定是正交的，同一个特征值对应的特征向量可能需要Schmidt正交化。  证明 1. 实对称矩阵的特征值都是实数\n已知$Ax = \\lambda x$，左右两边同时取共轭，有\n$$ \\bar{A}\\bar{x}=\\bar{\\lambda}\\bar{x} $$\n因为A是实对称矩阵，$\\bar{A}=A$，因此有 $$ A\\bar{x}=\\bar{\\lambda}\\bar{x} $$\n左右两边同时取转置，有\n$$ \\bar{x}^TA=\\bar{x}^T\\bar{\\lambda} $$\n左右两边同时右乘x，得 $$ \\bar{x}^TAx=\\bar{x}^T\\bar{\\lambda}x $$\n对$Ax = \\lambda x$，左右两边同时左乘$\\bar{x}^T$，得 $$ \\bar{x}^TAx=\\bar{x}^T\\lambda x $$\n比较两个式子，可以得到\n$$ \\bar{\\lambda} = \\lambda $$\n一个数的共轭等于其本身，这个数是实数。\n2. 对称矩阵的特征向量是正交的\n  正定矩阵 #   正定矩阵是对称矩阵的一种特殊情况，因此它也是对称的。\n定义 #  正定矩阵有各种各样的定义，它们互相之间是等价的。下面是几种常见定义/判定方法：\n 所有特征值大于0。 所有顺序主子式都大于0。 所有Pivots都大于0。 $x^TAx\u0026gt;0$ ： $x=[x_1,x_2, \u0026hellip;, x_n]$, 对于所有的非零x恒大于0  类似的可以定义半正定(大于等于0)，负定（小于0），半负定（小于等于0）。\n上述四个定义相关性的不严格证明 考虑第四个定义，我们知道正定矩阵就是二次型的系数矩阵，当二次型要恒大于0，则它必定可以化成一系列平方的和，且每一项的系数都大于0（可以等于0吗？不行，平方项是可以等于0的，如果有一项系数是0，就多了一个自由变量，就可以构造出等于0的x），而化为平方和形式的过程，等价于对A做elimination，平方项的系数就是elimination之后的pivots，里面的系数是elimination过程的L矩阵。\n例：\n  特殊情况 #   $A^TA$必定是半正定矩阵，如果A(m*n矩阵)的秩为n，则必定是正定矩阵。  证明 直观的，这个相当于矩阵形式的平方。我们知道标量，$a^2\u0026gt;0$，对应的，我们可以想象$A^TA$是应该是正定的。\n回忆正定矩阵的四种定义：特征值不知道，pivots不知道，顺序主子式不知道，因此考虑最后一种形式。\n严格证明：\n考虑矩阵$A\\in R^{m\\times n}$\n$$ x^TA^TAx = (Ax)^TAx = ||Ax||^2 \\geq 0 $$\n上式，只有在Ax=0的时候才取0，如果我们希望$A^TA$是正定的，这意味着Ax=0不能有解，而无解的条件是A的列向量线性无关，即A的秩为n。\n  正交矩阵 #   正交矩阵（英语：orthogonal1 matrix）是一个方块矩阵Q，其元素为实数，而且行向量与列向量皆为正交的单位向量。\n 相同行/列向量内积为1 不同行/列向量内积为0  显然，每个行/列向量都不能由其他行/列向量线性表出，因此正交矩阵必定是满秩矩阵。\n行向量正交能否推出列向量正交? 可以，推导如下：\n行向量正交，因此有$AA^T = I$, 对于方阵，有$AB = I \\simeq BA=I$ ( 证明)， 则有$A^TA=I$，则有列向量正交。\n 重要性质 #   正交矩阵的转置矩阵等于其逆矩阵: $A^T = A^{-1}$ 行列式值必定为+1或-1 $$ 1=\\operatorname{det}(I)=\\operatorname{det}\\left(Q^{T} Q\\right)=\\operatorname{det}\\left(Q^{T}\\right) \\operatorname{det}(Q)=(\\operatorname{det}(Q))^{2} \\Rightarrow \\operatorname{det}(Q)=\\pm 1 $$    虽然更完整的说法是orthonormal matrix(标准正交矩阵)，但我们说正交矩阵的时候一般都是指标准正交。 \u0026#x21a9;\u0026#xfe0e;\n   "},{"id":23,"href":"/blog/docs/matrix/svd/","title":"奇异值分解","section":"Matrix","content":"Created: Decemenber, 1, 2020 所有矩阵都可以分解成：正交矩阵 * 对角矩阵 * 正交矩阵，这个分解叫奇异值分解。\n$$ A = U\\Sigma V^T $$\n其中A是一个m*n的矩阵，V是一个n*n的矩阵，U是一个m*m的矩阵，$\\Sigma$是一个m*n的矩阵。\n在一些特殊情况下，如A是对称矩阵，它的特征向量是正交的，奇异值分解退化为矩阵对角化，设Q是是A的特征向量（列向量）构成的矩阵。则有\n$$ A = U\\Sigma V^T = Q \\Lambda Q^T $$\nIntuition - 几何含义 #   当我们在做奇异值分解的时候，我们实际上是在做什么？\n 在MIT Linear Algebra 第29讲，Strang教授给了我们一个很生动的解释。  详细解释 Revision needed\n考虑两个线性空间$R^n$和$R^m$, 其中$V=[v_1,v_2,\u0026hellip;,v_n]$是$R^n$的一组标准正交基，我们希望对这个基底做一个线性变换得到$R^m$空间的一组基底，这个基底也是正交的。\n这个新基底是$R^m$空间下某个标准正交基乘上某个系数矩阵，用矩阵的语言来说，我们实际有这个等式：\n$$ AV = U\\Sigma $$\n其中A表示线性变换，V是$R^n$的一组标准正交基，U是$R^m$的一组标准正交基，$\\Sigma$是系数对角阵。\n换句话说，对每一个m*n的矩阵，它其实都是其行空间$R^n$和列空间$R^m$某两个正交基底的变换矩阵，奇异值分解其实就要找到这两个基底。\n  求解 #   简单来说：\n V是$A^TA$的特征向量构成的矩阵。 U是$AA^T$的特征向量构成的矩阵。 $\\Sigma$是$A^TA$和$AA^T$的特征值（二者特征值一致）。  详细推导 直接求解下式，并不好求，因为它同时包含很多个未知量。\n$$ A = U\\Sigma V^T $$\n我们知道$A^TA$具有很好的性质，考虑\n$$ A^TA = V\\Sigma^T U^TU\\Sigma V^T = V\\Sigma^T\\Sigma V^T $$\n我们知道$A^TA$是对称矩阵，而对称矩阵的特征向量是正交的，而我们正好要求V是一个正交矩阵！因此，我们知道$A^TA$的特征向量构成的矩阵就是（满足要求，但唯一性需要证明）我们要求的V。$\\Sigma^T\\Sigma$则是$A^TA$的特征值。\n类似的，考虑$AA^T$，我们有\n$$ AA^T = U\\Sigma^T V^TV\\Sigma U^T = U\\Sigma^T\\Sigma U^T $$\n即U是$AA^T$的特征向量构成的矩阵，$\\Sigma^T\\Sigma$则是$AA^T$的特征值。\n我们这时候其实得到了一个附属结论，那就是$AA^T$和$A^TA$的特征值是相同的。     对于复矩阵（酉矩阵），只需要将转置T改成共轭转置H即可。  需要注意的是，并不是任取n个$A^TA$的两两正交单位长的特征向量都可以作为V的列向量。$V_1$和$U_1$必须要满足下面的关系才行:\n$$ V_1 = A^HU_1\\Delta^{-1} $$\n其中$U_1$, $V_1$是非零特征值对应的特征向量组。\n"},{"id":24,"href":"/blog/docs/matrix/road/","title":"如何将各个知识点串起来？","section":"Matrix","content":"Created: Decemenber, 3, 2020 引入 特征值与特征向量之后，我们可以很容易的推导出 矩阵对角化的公式。\n矩阵对角化是针对一般方阵的，对于特殊方阵，如 对称矩阵，矩阵对角化可以进一步特化。\n对称矩阵有很多很好的性质，它是它的进一步特例， 正定矩阵有着更好的性质。\n#  "},{"id":25,"href":"/blog/docs/matrix/misc/","title":"杂七杂八","section":"Matrix","content":"Created: Decemenber, 1, 2020 Quick Look #   酉矩阵: $A^HA=E$ 正规矩阵: $A^HA=AA^H$, 包括对称矩阵，反对称矩阵，Hermite矩阵，反Hermite矩阵，正交矩阵，酉矩阵等。 酉对角化： $A=Q\\Lambda Q^H$, 普通对角化: $A=Q\\Lambda Q^{-1}$ 单纯矩阵：可以对角化的矩阵，即有n个线性无关的特征向量。  线性代数中常见的等价关系 #    singular matrix = 奇异矩阵 = 不可逆矩阵\n  A不可逆 \u0026lt;-\u0026gt;1 Ax = 0 有非零解 \u0026lt;-\u0026gt;2 特征值=0\n  Spectrum #  矩阵中有谱（spectrum）这个概念，例如谱分解，谱范数等等。\n这个谱其实说的就是特征值，特征向量，它是从光学里借过来的说法。光学中光是由各种pure light构成的，光谱就是各个成分的占比。而在矩阵中，矩阵由各个pure的component，特征值特征向量构成。\n进一步来说，我们看矩阵对角化，我们知道任何一个对称矩阵A都可以对角化成$Q\\Lambda Q^{T}$，其中Q为特征向量组成的矩阵。\n我们进一步把这个式子拆开，我们设A的特征向量为\u0008$q_i$, 则\n$$ A = Q\\Lambda Q^{T} = \\lambda_1 q_1 q_1^T + \\lambda_2 q_2 q_2^T + \u0026hellip; + \\lambda_n q_n q_n^T $$\n$q_i$是一个列向量，$q_1 q_1^T$是一个矩阵，因此A实际上就化成了n个矩阵的线性组合，第i个矩阵是$q_i q_i^T$，系数是特征值$\\lambda_i$。\n  A可逆的话，意味着简化阶梯形最后一行不全为0，为了满足Ax=0，x必须等于0。 \u0026#x21a9;\u0026#xfe0e;\n TODO \u0026#x21a9;\u0026#xfe0e;\n   "},{"id":26,"href":"/blog/docs/matrix/eigen/","title":"特征值\u0026\u0026特征向量","section":"Matrix","content":"Created: Decemenber, 1, 2020 首先明确一点，特征值和特征向量是针对方阵来说的，我们说某个矩阵A有什么特征值和特征向量时，我们实际上暗示了矩阵A是一个方阵。\n定义 #  任何满足下列条件的标量和向量分别为矩阵A的特征值和特征向量。1\n$$ Ax = \\lambda x $$\nIntuition #  将一个矩阵A与一个向量x相乘，我们实际上在对向量x做一个 线性变换，特征向量实际上就是经过这个变换与原向量仍然平行的那些向量，特征值就是前后两个向量长度的一个比值。\n 显然，零向量满足条件，那它是任何矩阵的特征向量吗？\n不是，我们显示将特征向量定义为非零向量。\n 求解 #  根据定义，有\n$$ (A-\\lambda I)x = 0 $$\n这个等式要有非零解，$(A-\\lambda I)$必须是singular（不可逆）的，特征值要等于0：\n$$ Det(A-\\lambda I) = 0 $$\n于是：\n 求行列式并解方程即可求出特征值。 对于特征向量，我们回代特征值，解$(A-\\lambda I)x=0$这个方程。  因为$(A-\\lambda I)$不可逆，所以解有无穷多个，即特征向量有无穷多个，这时候我们只需要给出一个就行了。    性质 #   $\\sum \\lambda = trace(A)$ : 矩阵A的所有特征值之和等于矩阵A的 trace（对角线元素之和）。 $\\prod \\lambda = det(A)$ : 矩阵A的所有特征值的乘积等于矩阵A的行列式。 三角矩阵的特征值就是对角线上的元素。 对称矩阵的特征向量是正交的。  证明：对称矩阵的特征向量是正交的 To be done     由特征向量的定义，显然A必须要个方阵，若不是，A与x相乘，x维数就变了。 \u0026#x21a9;\u0026#xfe0e;\n   "},{"id":27,"href":"/blog/docs/matrix/similar/","title":"相似矩阵","section":"Matrix","content":"Created: Decemenber, 7, 2020 满足以下条件的两个矩阵，我们称它们是相似的。\n$$ B = M^{-1}AM $$\n换句话说，如果某个矩阵可以由另一个矩阵左乘一个矩阵再右乘这个矩阵的逆，则称它们是相似的。\nIntuition #  相似的矩阵具有很多相似的属性。\n我们可以把每一个矩阵和其相似的矩阵（们）都想成一个family，这个family里可能有一些矩阵很特殊，通过研究这些特殊的矩阵（通常更为容易），我们可以反推出原矩阵的一些属性。\n 矩阵对角化$\\Lambda = S^{-1}AS$的对角矩阵就是一种特殊的相似矩阵。   性质 #   相似矩阵的特征值是相同的（特征向量可能不同1,但相互独立的数量是相同的）  证明 证明要分两个方向，A的特征值是B的特征值，B的特征值也是A的，由这两者即可得到二者特征值相等。\n对于原矩阵A, 其特征值满足：\n$$ Ax = \\lambda x $$\n对于B：\n$$ (M^{-1}AM)M^{-1}x = M^{-1} \\lambda x = \\lambda M^{-1} x $$ $$ BM^{-1}x = \\lambda M^{-1} x $$\n于是，我们知道A的特征值也是B的特征值，且\u0008B的特征向量是A的特征向量乘以$M^{-1}$。\n反向很容易，把M乘到B那边得到$MBM^{-1} = A$，用类似的方法即可证明。\n     还是一定不同？TODO \u0026#x21a9;\u0026#xfe0e;\n  "},{"id":28,"href":"/blog/docs/matrix/diagonalization/","title":"矩阵对角化","section":"Matrix","content":"Created: Decemenber, 1, 2020  To be continue.  Keypoint: 一些矩阵可以对角化 $S^{-1}AS = \\Lambda$。其中S为特征向量(列向量)构成的矩阵，$\\Lambda$为特征值构成的对角矩阵。\n 什么样的矩阵可以？ 因为定义里S要可逆，所以A必须拥有n个线性无关的特征向量。   关于矩阵对角化，我们其实可以从两个角度理解：\n 一些矩阵可以对角化$S^{-1}AS = \\Lambda$。 或一些矩阵可以化成三个矩阵的乘积：$A = S\\Lambda S^{-1}$  推导 #   $$ \\begin{equation} \\begin{aligned} AS \u0026= A[x_1, x_2, ..., x_n] = [Ax_1, Ax_2, ..., Ax_n] = [\\lambda_1 x_1, \\lambda_1 x_2, ..., \\lambda_1 x_n] \\\\\\\\\\\\ \u0026= [x_1, x_2, ..., x_n] \\begin{bmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0\\\\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 \\lambda_n\\\\ \\end{bmatrix} = S\\Lambda \\end{aligned} \\end{equation} $$  左右两边同时乘S的逆，即可得到：\n$$ S^{-1}AS = \\Lambda $$\n特殊情况 #  当矩阵A是对称矩阵时，其特征向量是正交的1。因此，对角化可以写成\n$$ Q^{T}AQ = \\Lambda $$\n   证明 \u0026#x21a9;\u0026#xfe0e;\n   "},{"id":29,"href":"/blog/docs/matrix/space/","title":"线性空间和线性变换","section":"Matrix","content":"Created: Decemenber, 9, 2020 抽象来说，线性空间就是里面元素满足一定运算规律的一个空间。\n具体来说，空间的概念在数学上就是一个集合，空间里的元素就是集合里的元素，线性空间就是里面元素满足一定运算规律的一个集合。\n线性空间 #    符号说明：实数域R，复数域C，统称数域F。\n 线性空间是数域F上满足加法和数乘运算规律的一个集合。具体定义如下：\n TODO  核(零)空间 #   核空间，或者说零空间是针对一个矩阵来说的。对一个矩阵A，它的核空间就是Ax=0的解空间。\n常用 N(A) 来表示，N其实就是英文里Null Space的首字母。\n列空间/值域 #   同样是针对一个矩阵来说的，具体不解释。常用 R(A) 表示。\n向量 #   我们把线性空间里的元素称为向量。\n注意，这里的向量比线性代数里的n元向量含义更广。  概念：\n 线性组合/线性表示 线性相关/无关  基 #   一个线性空间的基，是可以线性表出这个线性空间中其它所有向量的n个线性无关的向量。\n$$ \\alpha = k_1a_1 + k_2a_2 + \u0026hellip; + k_na_n $$\n其中系数称为向量$\\alpha$在这个基下的坐标。\n基变换 #   不同基之间可以进行变换，其中P称为过渡矩阵。\n$$ B = AP $$\n$$ [b_1,b_2,\u0026hellip;,b_n] = [a_1,a_2,\u0026hellip;,a_n]P $$\n坐标变换 #   坐标变换是左乘$P^{-1}$。推导如下\n$$ B\\beta^T = A\\alpha^T $$\n$$ AP\\beta^T = A\\alpha^T $$\n$$ \\beta^T = P^{-1}\\alpha^T $$\n线性子空间 #   线性子空间是线性空间中仍满足线性空间要求的一个子空间。\n特殊的两个子空间：也称这两个子空间为平凡子空间\n 线性空间本身 零空间  生成子空间：写做 span{$a_1, a_2, \u0026hellip;, a_n$}\n交空间：两个子空间的交集，同时存在于两个子空间。\n $$ V_{1} \\cap V_{2}=\\left\\{\\boldsymbol{\\alpha} \\mid \\boldsymbol{\\alpha} \\in V_{1} \\text { 且 } \\boldsymbol{\\alpha} \\in V_{2}\\right\\} $$  和空间：这个空间的向量是两个空间向量的和。这两个子空间有点像是和空间的基底。\n $$ V_{1}+V_{2}=\\left\\{\\boldsymbol{\\alpha}=\\boldsymbol{\\alpha}_{1}+\\boldsymbol{\\alpha}_{2} \\mid \\boldsymbol{\\alpha}_{1} \\in V_{1} \\text { 且 } \\boldsymbol{\\alpha}_{2} \\in V_{2}\\right\\} $$  维数公式：\n$$ \\operatorname{dim} V_{1}+\\operatorname{dim} V_{2}=\\operatorname{dim}\\left(V_{1}+V_{2}\\right)+\\operatorname{dim}\\left(V_{1} \\cap V_{2}\\right) $$\n直和空间：若V1，V2两个子空间没有交集，则它们的和空间称为V1，V2的直和空间。\n补子空间：直和空间为全集的两个子空间互为自己代数补。\n线性映射 #    定义：满足加法和数乘的两个线性空间的映射关系称为线性映射。\n 线性映射前的空间叫原像，后的叫“像”。\n证明线性映射，只需要证明下面两个式子：\n $$ \\begin{array}{l} \\mathcal{A} \\left( \\alpha _{1}+ \\alpha _{2}\\right)= \\mathcal{A} \\left( \\alpha _{1}\\right)+ \\mathcal{A} \\left( \\alpha _{2}\\right) \\\\ \\mathcal{A} \\left(\\lambda \\alpha _{1}\\right)=\\lambda \\mathcal{A} \\left( \\alpha _{1}\\right) \\end{array} $$  性质 #   零元素经过线性映射还是零元素。 一组元素经过线性映射，如果原来线性相关，则之后也线性相关。但是，如果原来线性无关，则之后不一定线性无关。  矩阵表示 #  线性映射可以用矩阵表示，但这之前我们需要将两个线性空间的元素用各自的基底进行表示。\n选取不同的基底，同一个线性映射的矩阵表示就不同。\n注意区分两个概念：\n 基坐标变换：$\\alpha=\\beta A$ 向量坐标变换：$y=Ax$  给定基的情况下给个矩阵都表示一个线性映射。\n不同基底下矩阵表示之间的关系：\n设有两个空间$X,Y$, 两组基底$x_1,y_1$和$x_2,y_2$，\n $Q$是$y_1$到$y_2$的过渡矩阵， $P$是$x_1$到$x_2$之间的过渡矩阵，  则\u0008在$x_1,y_1$下的$A$与在$x_2,y_2$的$B$有如下关系。\n$$ B = Q^{-1}AP $$\n值域\u0026amp;\u0026amp;核 #  值域\n核子空间：线性映射的“零空间“，映射之后为0的元素构成的空间。\n对于一个从n维线性空间v1，映射到m维线性空间v2的线性映射。\n 核子空间核值域的维数之和为n  线性变换 #    同一个空间的线性映射称之为线性变换。\n 矩阵相似, 存在矩阵P，满足：\n$$ B=P^{-1}AP $$\n则称A，B相似。\n两个矩阵相似，意味着它们是不同基底下同一个线性变换的不同矩阵表示。  线性变换本身也可以定义运算, 这些运算和其矩阵表示有着对应关系:\n 乘法: AB 加法: A+B 数乘: kA  特征值\u0026amp;\u0026amp;特征向量 #  矩阵特征值的推广，求还是用线性变换的矩阵表示来求。\n 特征子空间：一个特征值对应的特征向量和零向量构成的一个子空间，称为这个特征值的特征子空间。 代数重复度：每个不同特征值的重复数目称为这个特征值的代数重复度。 几何重复度：一个特征值特征子空间的维数称为这个特征值的几何重复度。  相似对角化 #   n阶矩阵A可对角化的充要条件是A有n个线性无关的特征向量。  注意，并不是什么线性变换存在一个基，使得在这个基下的矩阵表示，呈现对角形。 "},{"id":30,"href":"/blog/posts/algorithms/%E8%B4%AA%E5%BF%83%E7%94%9F%E6%88%90%E6%9C%80%E4%BC%98%E7%BC%96%E7%A0%81%E7%9A%84%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/","title":"贪心生成最优编码的思路分析","section":"Posts","content":"贪心生成最优编码的思路分析 #  目标：求字符编码\n首先得先想到用二叉树表示编码，节点即为字符，边为编码。\n然后优化目标（目标函数）即为： f(x) = w(x)*l(x)\n w(x) 为 字符x的频率 l(x) 为 字符编码的长度  决策目标：巧妙安排二叉树的结构，使得目标函数值最小。\n最优编码树性质一：对于最优编码树，频率最小的两个节点深度最大，且为兄弟。\n证明：\n  深度最大显然，否则深度最大的频率更大，会使得目标函数值增大。\n  若最小的节点有兄弟，且其兄弟不是次小的，则可以让次小的节点与该节点交换，使得目标函数更小。\n  若最小的节点没有兄弟，则可以将该节点与其父亲交换，去掉“父亲”，可以使得目标函数更小。\n  有了这个性质，我们便可以先将两个频率最小的点连起来作兄弟。但为了进一步构造最优编码树，还需要第二个原则。\n最优编码树性质二： 对于最优编码树，删掉频率最小的节点（两个叶子节点），则其父亲变成了叶子结点，另其频率为两孩子频率之和，则新的编码树仍然是最优编码树。\n证明：f1为删之前，f2为删之后目标函数值，最小两个节点为x1,x2, 则有\nf1 = f2 + x1 + x2。\n已知f1最小，则f2也是最小的，否则调整新的编码树使其变为最优编码树，则f1更小，与已知f1最小矛盾。故新编码树也为最优编码树。\n有了这第二个性质，我们已知新的编码树为最优编码树，则根据性质一， 新编码树频率最小的节点为兄弟。递归这个过程，直到所有节点都纳入编码树（叶子节点）。\n例: 将两个频率最小的点连起来作兄弟，然后用这两个点的父亲代替这两个点，求剩余点的最优编码树。\n"}]